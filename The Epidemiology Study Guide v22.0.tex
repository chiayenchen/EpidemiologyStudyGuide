%% Do not edit unless you really know what you are doing.

\documentclass[12pt]{Epi_book}

\usepackage{amsfonts}
\usepackage{amsmath}   
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{arydshln}    
\usepackage{float}     
	\widowpenalty=1000
	\clubpenalty=1000
\usepackage{geometry} 
	\geometry{letterpaper}
	\geometry{left=1in,right=1in,top=1in,bottom=1in} 
	\usepackage{fancyhdr}
		\pagestyle{fancyplain}	
		\fancyhf{}
		\renewcommand{\headrulewidth}{0.4pt}
		\fancyhead[LE,RO]{\thepage}
		\fancyhead[CE]{\nouppercase{\slshape \leftmark}}
		\fancyhead[CO]{\nouppercase{\slshape \rightmark}}
		\renewcommand\contentsname{}  
		\headsep=25pt
		
		\fancypagestyle{plain}{
		\fancyhf{}
		\renewcommand{\headrulewidth}{0pt}
		\renewcommand{\footrulewidth}{0pt}}

\usepackage{graphicx}
\usepackage[figuresleft]{rotating}
\usepackage{chngcntr}
	\counterwithin{figure}{chapter}
	\counterwithin{table}{chapter}
\usepackage[pdfborder={0 0 0}]{hyperref}
	\usepackage[all]{hypcap}
\usepackage{multirow}
\usepackage[parfill]{parskip}
\usepackage{pmboxdraw}
\usepackage{subfigure}  

\makeatletter
\newcommand\@dotsep{4.5}
\def\@tocline#1#2#3#4#5#6#7{\relax
  \ifnum #1>\c@tocdepth % then omit
  \else
    \par \addpenalty\@secpenalty\addvspace{#2}%
    \begingroup \hyphenpenalty\@M
    \@ifempty{#4}{%
      \@tempdima\csname r@tocindent\number#1\endcsname\relax
    }{%
      \@tempdima#4\relax
    }%
    \parindent\z@ \leftskip#3\relax
    \advance\leftskip\@tempdima\relax
    \rightskip\@pnumwidth plus1em \parfillskip-\@pnumwidth
    #5\leavevmode\hskip-\@tempdima #6\relax
    \leaders\hbox{$\m@th
      \mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill
    \hbox to\@pnumwidth{\@tocpagenum{#7}}\par
    \nobreak
    \endgroup
  \fi}
\makeatother


\hypersetup{
pdftitle={The Epidemiology Study Guide 2010},
pdfauthor={Chia-Yen Chen, John Jackson, Chien-Chang Lee, Mitchell Machiela, Andreas Neophytou}, bookmarksopen=true, bookmarksopenlevel=0}
	
\begin{document}

\title{The Epidemiology Study Guide}
\author{Chia-Yen Chen\\John Jackson\\Chien-Chang Lee\\ Mitchell Machiela\\Andreas Neophytou}
\maketitle


\thanks{Thank you \LaTeX\ for making our dreams come true!\\\\Special thanks to all the faculty, friends, and fellow epidemiologists (past and present) whose thoughts and ideas fill the pages of this guide.\\\\\\\\\\
\today}

{\pagestyle{plain}
\chapter*{Table of Contents}
\tableofcontents
\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Measures \& Designs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Epidemioligical Concepts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sampling Variability \& Estimation}

\subsection{Random Error}
\begin{itemize}
\item fluctuations in a parameter estimate
\item we can never calculate the true average causal effect, we can only estimate it
	\begin{itemize}
	\item sampling variability: random error due to the fact we are studying a subset of the population
	\item nondeterministic (stochastic) counterfactuals: probability of a counterfactual event is not deterministic (0 or 1), but rather 
a probability between 0 
and 1 (quantum mechanics)
	\end{itemize}
\end{itemize}

\subsection{Estimator}
\begin{itemize} 
\item a function of observable sampled data used to approximate an unknown population parameter (the \em estimand\em)
	\begin{itemize} 
	\item \textbf{consistent estimator}: as the sample size approaches infinity, the estimated parameter approaches the truth
	\item \textbf{biased estimator}: the expected value of the estimator does equal the true value for the population parameter of interest
	\end{itemize}
\end{itemize}

\subsection{Efficiency}
\begin{itemize}
\item the precision of an estimation procedure
\item the more efficient an estimator, the smaller its estimate of variance
	\begin{itemize}
	\item \em variance\em : an estimation of uncertainty around a population parameter
	\item \em information\em : inverse of the variance ($\frac{1}{variance}$)
	\end{itemize}
\end{itemize}

\subsection{Bias}
\begin{itemize}
\item the difference between the true value and the estimated value of a population parameter
\item if the true value = expected value, there is no bias
\item if the true value $\neq$ expected value, there is bias
\item bias can arise from:
	\begin{enumerate}
	\item confounding
	\item selection bias
	\item information bias
	\end{enumerate}
\item often there is a trade-off between bias and efficiency (ex: stratifying more finely will remove more bias, but result in less efficiency)
\item for more information on bias, see Chapter \ref{chap:Bias}
\end{itemize}

\section{Probability}

\subsection{Variables}
\begin{itemize}
\item random variables: a variable whose values are realized from a probability distribution
	\begin{itemize}
	\item It may have different values for different individuals. 
	\item For notation, an uppercase letter $A$ (stands for action).
		\begin{itemize}
		\item continuous: may take any value
		\item discrete: can only be specified values (ex: integers)
			\begin{itemize}
			\item polytomous: can take more than two values
			\item dichotomous: can only take two values
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item particular value: the realized value of a random variable.  For notation, a lowercase letter $a$.
\end{itemize}

\subsection{Probability}
\begin{itemize}
\item marginal (unconditional) probability
	\begin{itemize}
	\item $Pr[Y=1]$ \em ``The probability that  $Y$ equals 1" \em
	\item estimated for the entire sample
	\end{itemize}
\item joint probability
	\begin{itemize}
	\item $Pr[Y=y, A=a] = Pr[Y=y \cap A=a]$
	\item \em ``The probability that $Y=y$ and $A=a$" \em
	\item equals $Pr[Y=y|A=a] \times Pr[A=a] = Pr[A=a|Y=y] \times Pr[Y=y]$ (Bayes theorem)
	\item estimated for the entire sample
	\end{itemize}
\item conditional probability
	\begin{itemize}
	\item $Pr[Y=y|A=a] = Pr[Y=y, A=a]/Pr[A=a]$ \em ``The probability that $Y=y$ given $A=a$" \em
	\item estimated in a subset of the sample (subjects with $A=a$)
		\begin{itemize}
		\item ex. $ Pr[Y=y|A=a, L=l] = Pr[Y=y, A=a, L=l]/Pr[A=a, L=l]$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Independence}
\begin{itemize}
\item denoted $\amalg$
\item $A \amalg Y = Y \amalg A, Pr[Y=1|A=1]=Pr[Y=1|A=0]=Pr[Y=1]$
\item information on one variable gives no information on another variable
\item no correlation or association between variables
	\begin{itemize} 
	\item mean independence
		\begin{itemize}
		\item for a continuous outcome, $ E[Y|A=1]=E[Y|A=0]=E[Y]$
		\item independence and mean independence are the same for a dichotomous outcome
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Association}
\begin{itemize}
\item if there is an association $\rightarrow Pr[Y=1|A=1] \neq Pr[Y=1|A=0]$
\item conditionally estimated (comparing the outcomes of two mutually exclusive subsets of the population under different scenarios)
\end{itemize}

\section{Causation}
\begin{itemize}
\item if there is a causal effect $\rightarrow Pr[Y^{a=1}] \neq Pr[Y^{a=0}]$
	\begin{itemize}
	\item $Y^a$ is defined as the outcome if everyone had received treatment $A=a$
	\end{itemize}
\item marginally estimated (comparing the outcomes of the whole population under different scenarios)
\end{itemize}

\begin{figure}
	\centering
		\includegraphics[scale=0.4]{Cause_Assoc.jpg}
		\caption{Causation is in the entire population, association is only in a subset of the population}
		\label{measures}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Measures of Frequency and Occurrence}
\label{chap:Measures of Frequency and Occurrence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Useful Concepts}
\subsection{Stationary Population (steady state assumption)}
\begin{itemize}
\item the number entering the population is balanced by the number exiting population in \em any \em period of time within levels of all determinants of risk. 
\item no \em net \em migration
\item allows calculation of the prevalence odds: $P/(1-P) = I \times D$, where $I$ is incidence and $D$ is disease duration
\item any time period would be suitable for estimating the incidence rate because it is assumed to be \em constant \em over time
\end{itemize}

\subsection{Person Time}
\begin{itemize}
\item is the amount of time an individual contributes to a group's observation, usually in a homogeneous state of risk 
\item has units of $person \times time$
\item 3 equivalent ways of calculating person-time
	\begin{itemize}
	 \item in a study of $n$ subjects $i$, total person-time ($PT$) of observation = 
	\begin{align*}
	\textbf{(a)} \; \displaystyle \sum_{i=1}^{n} [time]_{i} \qquad \qquad \textbf{(b)} \; E\big[(time)_i\big] \times \displaystyle \sum_{i=1}^{n} i 
	\end{align*}
	\begin{align*}
	\textbf{(c)} \  \sum time \times E\bigg[\sum_{i=1}^{n} i\bigg]
	\end{align*}
	\item[\textbf{(a)}] for each person, identify amount of person time contributed to group's observation, then sum the times of individual persons to get total person time for the group
	\item[\textbf{(b)}] multiply the number of persons under observation by the average duration of observation per person
	\item[\textbf{(c)}] multiply the length of the period of observation by the average number of persons under observation during the period
	\end{itemize}
\item classification of at-risk person-time by exposure status and other covariates assumes that the incidence rate of outcome is constant within these strata
\end{itemize}

\subsection{Risk-Set}

	\begin{itemize}
	\item group of cohort members who would have been observed to have event, had they done so at the moment at which the event occurs.
		\begin{itemize}
		\item unadjusted analysis: Kaplan-Mier
		\item adjusted: Mantel-Haenszel matched on time
		\item multivariate: Cox Proprotaional Hazards Regression
		\end{itemize}
	\item when\dots
		\begin{itemize}
		\item no event or censored $\Longrightarrow S(t) = 1$
		\item event occurs $\Longrightarrow S(t) = (n-1)/n$  and $F(t)=1/n$
		\end{itemize}
	\item assumption is that underlying hazard is unchanging over the interval of the risk set
	\end{itemize}
	
\section{Prevalence}
\subsection*{Definition}
\begin{itemize}
\item the total number of individuals who have an attribute or disease at a particular time (or period) divided by the total population 
under study
\item there are several types. Each corresponds to the proportion of individuals with disease or condition
	\begin{description}
	\item[- Point] at a particular point in time
	\item[- Period] during a specified period of time. Includes cases that (a) arise before but extend into the interval of study or (b) arise during the interval of study
	\item[- Lifetime] for at least part of their lives at \em any time \em during their life	
	\end{description}
\item time period and population must be specified
\end{itemize}

\subsection*{Characteristics}
\subsubsection*{General Properties}
\begin{itemize}
\item range $[0,1]$
\item a proportion (dimensionless)
\item binomial distribution used for inference
\item is a population parameter
\end{itemize}

\subsubsection*{Challenges for Causal Inference}
\begin{itemize}
\item is sensitive to causal and non-causal factors
\item $\uparrow$ occurrence = $\uparrow$ prevalence
\item $\uparrow$ duration (survival) = $\uparrow$ prevalence
\item sampling prevalent cases can select on consequence of outcome and introduce bias (\em see case-control \em and \em selection bias 
sections \em)
\end{itemize}

\subsubsection*{Challenges for Public Health Planning}
\begin{itemize}
\item measures extent of disease in population
\item short duration and survival rate may mask disease burden
\end{itemize}

\subsection*{Estimation}
\begin{itemize}
\item from cross-sectional or longitudinal data
	\begin{itemize} 
	\item prevalent case: existing case or new case arising during specified time interval (counting a person only once) 
	\item $\frac{\# \ prevalent \ cases}{total \ population}$ during a specified interval of time
	\item $Pr[Y=1]$ for a dichotomous $Y$ \{$1$ = prevalent  case,  $0$ = non-prevalent  case\}
	\end{itemize}
\item assuming steady state:
	\begin{itemize}
	\item let $N$ be population size, $P$ be prevalence pool (\# prevalent cases in population)
	\item let $I$ be incidence rate of disease, $I^{\prime}$ be incidence of exit from prevalence pool, $\bar{D}$ be average duration of 
disease
		\begin{align*}
		inflow \ to \ prevalence \ pool &= outflow \ from \ prevalence \ pool \\
            I\times (N-P) \times \Delta time &= I^{\prime} \times P \times \Delta time \\
            I\times (N-P) \times \Delta time &= (\frac{1}{\bar{D}}) \times P \times \Delta time \\
            \frac{P}{N-P} &= I \times \bar{D} \\
            \end{align*}
	\item $\frac{prevalence}{1-prevalence} = Incidence \ Rate \times Average \ Duration \Longrightarrow \frac{P}{1-P} = I \times \bar
{D}$
	\item applying rare disease assumption $ p/(1-p) \approx p $
		\begin{itemize}
		\item $prevalence \cong Incidence \ Rate \times Average \ Duration \Longrightarrow P \cong I \times \bar{D}$
		\end{itemize}
	\item note that ``steady state assumption" refers to constant prevalence over time period
	\item this method is not too useful in practice due to unrealistic constraints
	\item does not apply to age-specific prevalence unless assumptions hold for that particular age group
	\end{itemize}
\end{itemize}

\section{Risk}
\subsection*{Definition}
\begin{itemize}
\item has many synonyms and can refer individual or average risk:
	\begin{description}
	\item[- risk] an individual's probability of developing disease or attribute of interest \em during a specified time interval\em.  Can take values of either 0\% or 100\%.
	\item[- incidence proportion] the average risk in a group of individuals during a specified time interval. Can take values between 
0\% and 100\% (inclusive).
	      \begin{itemize}
		\item survival proportion is complement of incidence proportion $\Longrightarrow S=1-R$. The average probability of not 
having event during specified time interval. Can take values between 0\% and 100\%.
		\end{itemize}
	\item[- cumulative incidence] synonym for incidence proportion
	\end{description}
\item average risk (incidence proportion, cumulative incidence) is estimated from population data and is used to \em estimate \em the risk experienced by individuals. It is usually just referred to as ``risk".
\end{itemize}

\subsection*{Characteristics}
\subsubsection{General Properties}
\begin{itemize}
\item range $[0,1]$, proportion (dimensionless)
\item binomial distribution used for inference
\item makes no reference to when events occurred during follow up. Estimates of risk from different studies may relate to different patterns 
of disease accrual over time.
\item risk accumulates over follow up time
	\begin{itemize}
	\item towards 1 for inevitable outcome
	\end{itemize}
\item need to specify: 
	\begin{enumerate}
	\item time period of observation
	\item population at risk
	\end{enumerate}
\item risks are only comparable across studies if reference time is the same (ex: 1\% one-year risk vs. 1\% lifetime risk)
\end{itemize}

\subsubsection*{Challenges for Causal Inference}
\begin{itemize}
\item[] The incidence proportion is not well defined when there are:
\end{itemize}
\begin{description}
\item[$\bullet$ competing risks] death from another cause before occurrence of event. 
	\begin{itemize}
	\item the resulting risk estimate will be \em lower \em than the true risk. (if competing risk unrelated to exposure will bias \em 
risk ratio \em towards null).
	\item the magnitude of this problem increases with follow up length, so it can be ignored if time of follow up is short
	\item precludes use Kaplan-Meier, exponential formula, and many survival methods because identifiability conditions are harder to justify for competing risks. Unlike loss-to-follow up, the treatment to remove a competing risk is not well defined, making it harder to assume exchangeability between the censored and uncensored 
	\end{itemize} 
\item[$\bullet$ loss to follow up] no ascertainment of outcome in individuals who drop out or otherwise cannot be contacted
\item[$\bullet$ open cohort] open if new members are added to the cohort during follow up and/or some persons are right-censored (exit without occurrence of event). 
\end{description}

\subsubsection*{Challenges for Public Health Planning}
\begin{itemize}
\item can mask pattern of disease accrual over time if follow up is over a long time
\item since it's directly proportional to follow up time, it is interpretable only if time-frame specified
\end{itemize}

\subsection*{Estimation}
\begin{enumerate}
\item \underline{Closed Cohort}
	\begin{itemize}
	\item incident case: a case that arose during the period of follow-up
	\item $\frac{\# of \ incident \ cases \ between \ t_{0} \ and \ t_{1}}{\# \ of \ at \ risk \ subjects \ at \ t_{0} \ followed \ from 
\ t_{0} \ and \ t{1}}$
	\item probability of disease occurrence during follow up
		\begin{enumerate}
		\item marginal $\rightarrow (Pr[Y=1])$
		\item conditional $\rightarrow (Pr[Y=1|A=1])$
		\end{enumerate}
	\item if constant incidence rate: 
\begin{align*}
incidence \ proportion &= incidence \ rate \times length \ of \ follow \ up \\
R &= I \times \Delta T
\end{align*}
	\item logistic regression $PR[Y=1|X]=\frac{\exp(\beta X)}{1+\exp(\beta X)}$
	\end{itemize}
\item \underline{Open Cohort, Loss to Follow Up, \& Variable Incidence Rate}
	\begin{description}
	\item[$\bullet$ Kaplan-Meir or Product-Limit Method] $$R=1- \Bigg ( \displaystyle \prod_{k=1}^{K} (1-R_{k}) \Bigg )$$ \\
      Necessary assumptions: 
			\begin{enumerate}
			\item closed population (if open, censoring is random)
			\item event is inevitable (i.e. no competing risks)
			\item intervals $k$ have constant incidence rates
			\end{enumerate}
	Procedure:	
		\begin{enumerate}
		\item divide follow-up time into into $K$ intervals $1,2,...,k$ 
		\item From incidence rate and interval length, calculate incidence proportion for each interval \\ $Incidence \ Proportion_
{k} = Incidence \ Rate_{k} \times \Delta t_{k}$
		\item Convert to survival proportions for each interval \\ $Survival \ Proportion_{k} = 1-Incidence \ Proportion_{k}$
		\item Calculate average survival proportion over entire follow up (i.e. joint probability of survival during each interval) 
\\ $Survival = \displaystyle \prod_{k=1}^{K} Survival \ Proportion_{k}$
		\item Calculate average incidence proportion over entire follow up \\ $Incidence \ Proportion = 1-Survival \ Proportion$
		\end{enumerate}
	\item[$\bullet$ Exponential Formula] $$Incidence \ Proportion \approx 1-\exp \Bigg ( -\displaystyle \bigg[ \sum_{k=1}^{K} Incidence 
\ Rate_{k} \times \Delta t_{k} \bigg] \Bigg ) $$ \\ 
		Necessary assumptions:
		\begin{enumerate}
		\item closed population (if open, censoring is random)
		\item event is inevitable (i.e. no competing risks)
		\item $Risk = I\times\Delta t$ is small. Can be satisfied by stratifying follow up time into very fine intervals $k$ such 
that each event occurs at its own unique time 
		\end{enumerate}		
		Derivation and further forms:
			\begin{align*}
			\ln(1) &= 0 
			\end{align*}
		Let us stratify time so finely that $R_{k} = I_{k} \times \Delta t_{k}$ is very small
			\begin{align*}
		 	\ln(1-R_{k}) &\approx -R_{k} 	 		\\
		 	(1-R_{k}) &\approx \exp(-R_{k})
		 	\end{align*}
		Substituting into product limit gives exponential formula...
		 	\begin{align*}
			R_{\Delta t} &= 1- S_{\Delta t} \\
			 & =1- \bigg ( \prod_{k=1}^{K} (1-R_{k}) \bigg ) \\
		 	 &\approx  1-\exp \bigg ( -\sum_{k=1}^{K} R_{k} \bigg )	\\	
		 	 &\approx  1-\exp \bigg ( -\sum_{k=1}^{K} \big[ I_{k} \times \Delta t_{k} \big] \bigg )	\\		
 		 	\end{align*}	 		
		 	\begin{enumerate}
		\item If $I_{k}=I$ for all $k$ (constant incidence rate) then...
		 	\begin{align*}	 				
		 	R_{\Delta t} &\approx 1- \exp \big(I \times \Delta t \big)
			\end{align*}
		\item If $R_{\Delta t}<.10$ (i.e. $I$  or $\Delta t$ small) then... (Nelson-Aalan estimator)
			\begin{align*}
		 	R_{\Delta t} &\approx \sum_{k=1}^{K}  \big[I_{k} \times \Delta t_{k} \big]
			\end{align*}
		\item If $I_{k}=I$ for all $k$ \underline{and} $R_{\Delta t}<.10$ (i.e. $I$  or $\Delta t$ small) then... (cocktail party estimator)
			\begin{align*}			 	
			R_{\Delta t} &\approx I \times \Delta t
			\end{align*}
		\end{enumerate}
		Save this formula for the Annual Epidemiology Department Winter Cocktail Party and stick to Kaplan-Meir approach (product-limit method) 
	\end{description}
\item \underline{Competing Risks}	
	\begin{itemize}
	\item \em independent competing risks \em assumption: interval specific rates would not change if competing risks were removed
	\item necessary for product-limit method and exponential formula in the presence of competing risks
	\end{itemize}
\end{enumerate}	

\subsection*{Relation to other Frequency/Occurrence measures}
The hazard rate (or instantaneous incidence rate) is often referred to as \em instantaneous risk \em because it estimates the limit of the 
probability of event occurrence as $\Delta t \rightarrow 0$ 

\section{Odds}
\begin{itemize}
\item $[0,\infty]$, ratio (dimensionless) 
\item Odds = P/(1-P) 
\item Prob = Odds / (1+Odds)
\item $ \frac {Pr[Y=1|A=1]}{Pr[Y=0|A=1]} = \frac {Pr[Y=1|A=1]}{1 - Pr[Y=1|A=1]}$
\item ratio of probability of success to probability of failure
\item estimates risk when disease is rare (i.e. $Pr[Y=1]< 10\%$)
\item logistic regression: $\frac{P}{(1-P)}=\exp(\beta X)$
\end{itemize}

\section{Incidence Rate}
\subsection*{Definition}
\begin{itemize}
\item as with risk, there are instantaneous and average forms
	\begin{enumerate}
	\item \textbf{Average Incidence Rate:}
		\begin{itemize}
		\item the average rate of occurrence of disease in a population over a specified period of time
		\item number of incident cases per unit of person-time of follow up
		\end{itemize}
	\item \textbf{Incidence Density:} The rate of disease occurrence over an infinitely small period of time
		\begin{itemize}
		\item synonym: Instantaneous Incidence Rate
		\item synonym: Hazard
		\end{itemize}
	\end{enumerate}
\item proper interpretation of its value requires selection of a time unit. Unit usually chosen so at least one digit to left of decimal 
place.
\item is the limiting value of the incidence proportion as $\Delta t \rightarrow 0$. In this sense it is synonymous with \em instantaneous risk \em (i.e. risk per unit time, where units of time are infintesimally small)  
\end{itemize}

\subsection*{Characteristics}
\subsubsection{General Properties}
\begin{itemize}
\item $[0,\infty]$, rate (units of inverse time)
\item $1/incidence \ rate$ = average waiting time (time until disease)
\item Poisson distribution usually used for inference
\end{itemize}

\subsubsection*{Causal Inference \& Public Health Utility}
\begin{itemize}
\item is well defined even with
	\begin{itemize}
	\item an open cohort
	\item loss to follow up
	\item competing risks
	\end{itemize}
\item not as intuitive as the concept of risk
\end{itemize}

\subsection*{Estimation}
\begin{itemize}
\item $average \ incidence \ rate = \frac{\# \ incident \ cases \ between \ t_{0} \ and \ t_{1}}{total \ person-time \ at \ risk\ from \ t_
{0} \ to \ t_{1}}$
	\begin{itemize}
	\item only incident cases in which PT was contributed to denominator
	\end{itemize}
\item poissson regression: $\lambda=\exp(\beta X)$
\item $instantaneous \ incidence \ rate = \displaystyle \lim_{\Delta T \rightarrow 0}  (Pr[Y_{t+1}=1|Y_{t}=0])/\Delta T$
\end{itemize}

\subsection*{Relation to other frequency/occurrence measures}
\begin{itemize}
\item see \em prevalence \em
\item see \em risk \em
\end{itemize}

\pagebreak

\begin{sidewaystable}
\centering
\vspace{40em}
\begin{tabular}{l c c c c}
\hline\hline
& \multicolumn{4}{c}{\textbf{Measures of Disease Occurrence}}\\[0.5em]
\textbf{Measure} & \textbf{Prevalence} & \textbf{Cumulative Incidence} & \textbf{Survival} & \textbf{Incidence Rate}\\
\hline\\
Minimum Value & 0 & 0 & 0 & 0\\[0.5em]
Maximum Value & 1 & 1 & 1 & $\infty$ \\[0.5em]
Incorporation of time & None & Specified follow-up & Specified follow-up & Incremental\\[0.5em]
Dimension & Cases per population & Cases per population & Non-diseased per & Cases per population\\
& & after specified time & population after & per unit time\\
& & & specified time & \\[0.5em]
Parameter & Probability & Probability & Probability & Hazard\\[0.5em]
Probability Distribution & Binomial & Binomial & Binomial & Poisson\\[0.5em]
Variance & $\frac{Pr(1-Pr)}{N}$ & $\frac{CI(1-CI)}{N}$ & $\frac{S(1-S)}{N}$ & $\frac{IR}{P}$\\[0.5em]
\hline\hline\\
\end{tabular}
\caption{Measures of disease occurrence, adapted from Walker, AM. 1991 Observation and Inference.}
\label{measures}
\end{sidewaystable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Measures of Association}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{table}[h!]
	
	\centering
		\begin{tabular}{l c c}
			\hline\hline
			\textbf{Measures} & \textbf{Association measures} & \textbf{Causal effect measures} \\ 
			& (conditional) & (marginal) \\ 
			\hline\\
			Risk Difference & $Pr[Y=1 | A=1] - Pr[Y=1 | A=0]$ & $Pr[Y^{a=1}=1] - Pr[Y^{a=0}=1]$ \\[1em]
			Risk Ratio & $Pr[Y=1 | A=1] / Pr[Y=1 | A=0]$ & $Pr[Y^{a=1}=1] / Pr[Y^{a=0}=1]$ \\[1em]
			Odds Ratio & $\frac{Pr[Y=1 | A=1]/(1-Pr[Y=1 | A=1])}{Pr[Y=1 | A=0]/(1-Pr[Y=1 | A=0])}$ & 	
$\frac{Pr[Y^{a=1}=1]/(1-Pr[Y^{a=1}=1])}{Pr[Y^{a=0}=1]/(1-Pr[Y^{a=0}=1])}$ \\[1em]
			\hline\hline\\
		\end{tabular}
	\caption{Measures of association and causal effect}
	\label{tab:meausresOfAssociationAndCausalEffect}
\end{table}

\section*{Absolute Measures}
\begin{itemize}
\item absolute difference in risk, rate or odds
\item effect is on the additive scale (e.g. exposure adds to the baseline risk/rate)
\end{itemize}

\section{Risk Difference}\
\begin{itemize}
\item entire range $[-1,1]$ 
	\begin{itemize}
	\item $[-1,0]$ for preventative effect
	\item $[0,1]$ for causative effect
	\end{itemize}
\end{itemize}

\subsection{Number Needed to Treat}
The average number of individuals needed to receive treatment $a$ to reduce the number of cases by one
$$\frac{1}{|Preventative \ Risk \ Difference|}$$

\subsubsection{Number Needed to Harm} 
The average number of individuals needed to receive treatment $a$ to increase the number of cases by one
$$\frac{1}{|Causative \ Risk \ Difference|}$$

\section{Rate Difference}
\begin{itemize}
\item entire range $[-\infty,\infty]$
	\begin{itemize}
	\item $[-\infty,0]$ for preventative effect
	\item $[0,\infty]$ for causative effect
	\end{itemize}
\end{itemize}

\section{Attributable Measures}
\subsection{Types of exposed cases} 
\begin{itemize}
	\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{attributable_fraction.pdf}
	\caption{Definitions of types of cases for attributable fraction.  All individuals in the figure are exposed cases.}
	\label{fig:attributable_fraction}
	\end{figure}
\item $A_{0}$: number of exposed cases that the exposure had no impact on the cases' incidence time or disease outcome, \em non-attributable cases\em
\item $A_{1}$: number of exposed cases where the subject will be a case if exposed but the subject will not become a case if unexposed, \em excess cases\em
\item $A_{2}$: number of exposed cases that the exposure made the cases' incidence time earlier than it would have been in the absence of exposure; the subject will eventually become a case even without exposure, \em etiologic cases \em but not \em excess cases\em
\item $A_1 +A_2$: all \em etiologic cases\em
\item $M = A_{0}+A_{1}+A_{2}$: total number of \em exposed \em cases
\item $A_{exposed}=M=A_{0}+A_{1}+A_{2}= \sum_{i} \{Y_{i}^{a=1}=1|A=1\}$, cases that would have occurred if everyone exposed were actually exposed
\item $A_{unexposed}=A_{0}+A_{2}=\sum_{i} \{Y_{i}^{a=0}=1|A=1\}$, cases that would have occurred if everyone exposed had not been exposed
\end{itemize}


\subsection{Attributable Risk}
\begin{itemize}
\item a measure of the contribution of an exposure $A$ to the total incidence in the \em exposed \em
\item defined as contrast between observed risk in the exposed vs. the counterfactual risk in the exposed had exposure been withheld
\item alternate definitions
	\begin{itemize}
	\item the difference in risk comparing exposed to unexposed
	\item the risk associated with exposure among the exposed
	\item if causal, the change in average risk that would occur in the exposed had they not been exposed
	\end{itemize}
\item there are analogous definitions for the entire population and rate differences (later)
\item only includes excess cases $A_{1}$. Does not include non-excess etiologic cases $A_{2}$.
\end{itemize}

Formally speaking
\begin{itemize}
\item \textit{regular notation:} $$\frac{A_{exposed}}{N_{exposed}} - \frac{A_{unexposed}}{N_{unexposed}} = Risk_{exposed}-Risk_{unexposed}$$
\item the two are only equal if the identifiability conditions for causal inference are met
\item \textit{counterfactual notation:} \\
$Pr[Y^{a=1}|A=1]-Pr[Y^{a=0}|A=1] $\\
$\Longrightarrow Pr[Y=1|A=1]-Pr[Y^{a=0}|A=1] \mbox{(by consistency)}$\\
$\Longrightarrow Pr[Y=1|A=1]-Pr[Y=1|A=0] \mbox{(by exchangeability)}$
\end{itemize}

\subsection{Attributable Risk Fraction}
Due to the differences between case types, the attributable fraction includes two distinct concepts:
\begin{enumerate}
\item \textbf{ etiologic fraction:}
	\begin{itemize}
	\item regular notation: $\frac{A_{1}+A{2}}{M}$
	\item interpreted as proportion of cases whose disease originated from a biologic process in which treatment had an effect (i.e. 
change in survival time or disease status)
	\item the etiologic fraction is non-identifiable because it requires extremely strong exchangeability assumptions
	\end{itemize}
\item \textbf{excess fraction:} the fraction of disease in the exposed that would be eliminated if the exposed were unexposed
	\begin{itemize}
	\item \textit{regular notation:} $\frac{A_{1}}{M} =\frac{A_{0}+A_{1}+A_{2}}{M}-\frac{A_{0}+A_{2}}{M} =\frac{A_{exposed} - A_{unexposed}}{A_{exposed}}$
	\item For a closed cohort, given the sample size of the entire cohort ($N$); the exposed risk ($Risk_{exposed}$); the unexposed risk 
($Risk_{unexposed}$); the risk ratio ($RR=\frac{Risk_{exposed}}{Risk_{unexposed}}$), the excess fraction is given as:
$$\frac{A_{exposed} - A_{unexposed}}{A_{exposed}}  = \frac{A_{exposed}/N_{exposed} - A_{unexposed}/N_{unexposed}}{A_{exposed}/N_{exposed}} $$
$$ = \frac{A_{exposed}/N- A_{unexposed}/N}{A_{exposed}/N} = \frac{Risk_{exposed}-Risk_{unexposed}}{Risk_{exposed}}=\frac{RR - 1}{RR}$$
	
	\item \textit{counterfactual notation:} excess fraction in the treated
		\begin{align*}
		 \frac{Pr[Y^{a=1}=1|A=1]-Pr[Y^{a=0}=1|A=1]}{Pr[Y^{a=1}=1|A=1]} & \Longrightarrow \frac{Pr[Y=1|A=1]-Pr[Y^{a=0}=1|A=1]}{Pr
[Y=1|A=1]}\\
& \Longrightarrow \frac{Pr[Y=1|A=1]-Pr[Y=1|A=0]}{Pr
[Y=1|A=1]}\\
&\Longrightarrow \frac{Risk_{exposed}-Risk_{unexposed}}{Risk_{exposed}}\\
		\end{align*}
	\item thus, the attributable fraction only has a causal interpretation if $Risk_{unexposed}$, $Risk_{exposed}$, and $RR$ are causal
	\item can be estimated from case-control data if $OR \approx RR$
	\end{itemize}
\end{enumerate}	
			
\subsection{Extension to the entire population}
\begin{itemize}
\item a measure of the contribution of an exposure $A$ to the total incidence in the \em entire population \em
\item defined as contrast between observed risk in the entire population vs. the counterfactual risk in the entire population had exposure 
been withheld
\item as before, these definitions only have causal interpretations if their components are causal; note that $\big(Risk_{exposed}-Risk_
{unexposed} \big)$ and $\big( \frac{(Risk_{exposed}-Risk_{unexposed}}{Risk_{exposed}}\big)$ are respectively, the \em excess risk \em and 
\em excess fraction \em as defined above
\end{itemize}
\begin{enumerate}
\item \textbf{Population Attributable Risk:}
	\begin{itemize}
	\item \textit{regular notation:} 
	\begin{align*}
	& Risk_{entire \ population} - Risk_{unexposed}  \\
	&= \big(Risk_{exposed}-Risk_{unexposed} \big) \times Prev. \ of \ Exp \ in \ \underline{Population}\\
	&= excess \ risk \times Prev. \ of \ Exp \ in \ \underline{Population}
	\end{align*}
	\item \textit{counterfactual notation:} 
	\begin{align*}
	& Pr[Y=1]-Pr[Y^{a=0}=1] \\ 
& = Pr[Y=1]-Pr[Y=1|A=0] \mbox{(by consistency)}\\ 
&= \big(Pr[Y=1|A=1]\times Pr[A=1]+Pr[Y=1|A=0]\times Pr[A=0]\big)-Pr[Y=1|A=0] \\
&= \big(Pr[Y=1|A=1]\times Pr[A=1]+Pr[Y=1|A=0]\times (1-Pr[A=1])\big)-Pr[Y=1|A=0] \\
&= Pr[Y=1|A=1]\times Pr[A=1]+Pr[Y=1|A=0] -Pr[Y=1|A=0]\times Pr[A=1]-Pr[Y=1|A=0] \\
&= Pr[Y=1|A=1]\times Pr[A=1]-Pr[Y=1|A=0]\times Pr[A=1]\\
&= \big(Pr[Y=1|A=1]-Pr[Y=1|A=0]\big) \times Pr[A=1]\\
&= \big(Pr[Y^{a=1}=1|A=1]-Pr[Y^{a=0}=1|A=1]\big) \times Pr[A=1] \ \mbox{(by exchangeability)}\\ 
&= excess \ risk \times Prev. \ of \ Exp \ in \ \underline{Population}
	\end{align*}
	\item excess risk of disease in the population that could be eliminated if exposure were eliminated from the population
	\end{itemize}
\item \textbf{Population Attributable Risk Fraction:}
	\begin{itemize}
	\item \textit{regular notation:} 
	\begin{align*} &\frac{Risk_{entire \ population}-Risk_{unexposed}}{Risk_{entire \ population}} \\
	&=\bigg(\frac{Risk_{exposed}-Risk_{unexposed}}{Risk_{exposed}} \bigg) \times Prev. \ Exp \ in \ \underline{Cases}\\
	&=excess \ fraction \times Prev. \ Exp \ in \ \underline{Cases}
	\end{align*}
	\item \textit{counterfactual notation:}
	\begin{align*}
	\frac{Pr[Y=1]-Pr[Y^{a=0}=1]}{Pr[Y=1]} &=  \bigg(\frac{Pr[Y^{a=1}=1|A=1]-Pr[Y^{a=0}=1|A=1]}{Pr[Y^{a=1}=1|A=1]} \bigg) \times Pr
[A=1|Y=1] \\
	&\Rightarrow \bigg(\frac{Pr[Y=1|A=1]-Pr[Y^{a=0}=1|A=1]}{Pr[Y=1|A=1]} \bigg) \times Pr[A=1|Y=1]
	\end{align*}
	\item fraction of disease in the population that could be eliminated if exposure were eliminated from the population
	\end{itemize}
\end{enumerate}

\subsection{Extension to Rates}		
\begin{itemize}
\item Using risk to calculate excess fraction is fine because risk ratios can be translated into the ratio of numbers of cases
\item using rate to calculate excess fraction is not correct 
	\begin{itemize}
	\item rates cannot be translated readily into the number of cases
	\item use of rates results in the \em incidence-density fraction \em
	\end{itemize}
\end{itemize}

Analogous Attributable Measures for Rates
\begin{description}
\item[$\bullet$ Attributable Rate] the excess rate of disease that would be removed from the exposed if the exposure had been withheld
$$Rate_{exposed}-Rate_{unexposed}$$
\item[$\bullet$ Attributable Rate Percent] the fraction of disease associated with exposure among the exposed
$$\frac{Rate_{exposed}-Rate_{unexposed}}{Rate_{exposed}} = \frac{IRR-1}{IRR}$$
\item[$\bullet$ Population Attributable Rate] the excess rate of disease that would be removed from the entire population had exposure been 
withheld
\begin{align*}
& Rate_{entire \ pop.}-Rate_{unexposed} \\
&= \big(Rate_{exposed}-Rate_{unexposed} \big) \times Prev. \ of \ Exposed \ PT \ in \ \underline{Population}
\end{align*}
\item[$\bullet$ Population Attributable Rate Percent] the fraction of disease associated with exposure among the entire population
\begin{align*}
& \frac{Rate_{entire \ pop.}-Rate_{unexposed}}{Rate_{entire \ population}} 
 \\ &=\bigg(\frac{Rate_{exposed}-Rate_{unexposed}}{Rate_{exposed}}\bigg) \times Prev. \ of \ Exposure \ PT \ in \ 
\underline{Cases}
\end{align*}
\end{description}



\section{Preventable Measures}
\begin{itemize}
\item for when exposure is believed to cause disease \\
\item should be interpreted with caution as all or part of the apparent effect may be due to other factors associated with the apparent protective factor
\item \textbf{Population Preventable Fraction:}
	\begin{itemize}
	\item the proportion of disease (in the population) that would be prevented if the whole population were exposed to the factor
		\begin{align*}
		= \frac{Rate_{population}-Rate_{exposed}}{Rate_{population}}
		\end{align*}
	\end{itemize}	
\item \textbf{Population Prevented Fraction}
	\begin{itemize}
	\item the proportion of the hypothetical total load of disease in the population that has been prevented by exposure to the factor
		\begin{align*}
		= \frac{Rate_{unexposed}-Rate_{population}}{Rate_{unexposed}}
		\end{align*}
	\end{itemize}	
\end{itemize}



\section*{Relative Measures}
\begin{itemize}
\item based on the ratio of risks, rates, or odds
\item effect is on the multiplicative scale (e.g. exposure multiplies the baseline risk/rate)
\item[\-]``Relative Risk" is often used to refer to the following effect measures:
\end{itemize}			

\section{Risk Ratio}
	\begin{itemize}
	\item $[0,\infty]$, multiplicative scale (\textit{see figure~\ref{transformation}})
		\begin{itemize}
		\item preventative $[\infty,1]$
		\item causative $[1,\infty]$
		\end{itemize}
	\item ideal for causal inference, harder to estimate using models
	\end{itemize}
	
\section{Incidence Rate Ratio}
	\begin{itemize}
	\item $[0,\infty]$, multiplicative log scale
		\begin{itemize}
		\item preventative $[\infty,1]$
		\item causative $[1,\infty]$
		\end{itemize}
	\item not ideal for causal inference, easier to estimate using models
	\item average hazard ratio can mask protective or hazardous period specific hazard ratios
	\item period-specific hazard ratios have built in selection bias. Depletion of susceptibles among exposed can make exposure look 
protective towards the end of the study
	\item the hazard ratio is sensitive to length of follow up and studies of different length can report different findings
	\end{itemize}
	
\section{Odds Ratio}
	\begin{itemize}
	\item $[0,\infty]$, multiplicative log scale
		\begin{itemize}
		\item preventative $[\infty,1]$
		\item causative $[1,\infty]$
		\end{itemize}
	\item below risk of 0.1, odds approximates risk ($OR \cong RR$), rare disease assumption
	\item OR will always be further from null than RR
	\item not ideal for causal inference, easier to estimate using models
	\item less interpretable
	\item non-collapsible
		\begin{itemize}
		\item is only a weighted average of stratum specific effects when there is NO heterogeneity
		\end{itemize}
	\end{itemize}


\subsection{Interpretation of ORs from Case-Control Studies}
\begin{itemize}
\item in a case-control study, the OR can approximate various effect measures depending on the sampling scheme for controls and the underlying assumptions made (see Figure ~\ref{studytypes})
\end{itemize}

\begin{figure}
\centering
\includegraphics[scale=0.58]{studytypes.pdf}
\caption{Classification of estimates from different study designs and sampling schemes}
\label{studytypes}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[scale=0.4]{OR_RR_transformation.pdf}
		\caption{Transformation of odds ratio and risk ratio scales}
		\label{transformation}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Measures of Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h!]
\centering
\begin{tabular}{c c c c}

&\multicolumn{2}{c}{Truth}\\
	Test & $Y=1$ & $Y=0$ \\
	\cline{2-3} 
	\multicolumn{1}{r|}{$Y^*=1$} & 	\multicolumn{1}{c|}{$Tp$} & \multicolumn{1}{c|}{$Fp$} \\
	\cline{2-3}
	\multicolumn{1}{r|}{$Y^*=0$} & \multicolumn{1}{c|}{$Fn$} & \multicolumn{1}{c|}{$Tn$} \\
	\cline{2-3}
	& $P[Y]$ & $1-P[Y]$ & $N$\\\\

\end{tabular}
\caption{Confusion Matrix}
\label{confusion}
\end{table}

\section{Sensitivity \& Specificity}
\begin{align*}
\text{ Measurement} \ Y^{*}: \{0 = Negative, 1=Positive\} \\
\text{ Truth } \ Y: \{0 = Negative, 1=Positive\}
\end{align*}
\begin{enumerate}
\item \textbf{Sensitivity (Sn):} the probability of a positive test given that the truth is positive, also known as the \em true positive rate \em (TPR)
	\begin{itemize}
	\item[=] $Pr(Y^{*}=1|Y=1)$
	\item \textit{False Negative Rate (FNR):} the probability of a negative test given that the truth is positive $= Pr(Y^{*}=0|Y=1)=1-Sn$
	\item $TPR+FNR=1$
	\end{itemize}
\item \textbf{Specificity (Sp):} the probability of a negative test given that the truth is negative, also known as the \em true negative rate \em (TNR)
	\begin{itemize} 
	\item[=] $Pr(Y^{*}=0|Y=0)$ 
	\item \textit{False Positive Rate} (FPR): the probability of a positive test given that the truth is negative $= Pr(Y^{*}=1|Y=0)=1-Sp$
	\item $TNR+FPR=1$
	\end{itemize}
\item \textbf{Positive Predictive Value (PPV)}: The probability of the truth being positive given a positive test $= Pr(Y=1|Y^*=1)$ 
	\begin{itemize}
	\item[=] $\frac{Sn \times Pr[Y=1]}{\big(Sn\times Pr[Y=1] \big) + \left(Sp \times (1-Pr[Y=1])\right)}$ $\Longrightarrow$ \textit
{depends on the prevalence of Y}
	\end{itemize}
\item \textbf{Negative Predictive Value} (NPV): The probability of the truth being negative given a negative test $= Pr(Y=0|Y^*=0) $
	\begin{itemize}
	\item[=] $\frac{Sp \times (1-Pr[Y=1])}{\left(Sp\times (1-Pr[Y=1]) \right) + \big((1-Sn) \times Pr[Y=1]\big)}$ $\Longrightarrow$ 
\textit{depends on the prevalence of Y}
	\end{itemize}
\item Likelihood Ratio
	\begin{itemize}
	\item in terms of PPV
		\begin{align*}
		\underbrace{\frac{Pr(Y=1|Y^{*}=1)}{Pr(Y=0|Y^{*}=1)}}_\text{Posterior odds} 
		&= \underbrace{\frac{Pr(Y^{*}=1|Y=1)}{Pr(Y^{*}=1|Y=0)}}_\text{Likelihood Ratio} 
		\times \underbrace{\frac{Pr(Y=1)}{Pr(Y=0)}}_\text{Prior odds} 
		\\ \\
		\frac{PPV}{1-PPV} &= \frac{Sn}{1-Sp} \times \frac{Pr(Y=1)}{1-Pr(Y=1)}
		\end{align*}
	
		\begin{itemize}
		\item LR(+) tells us how much more likely a condition is, given a positive test result
		\end{itemize}
	
	\item in terms of NPV

		\begin{align*}
		\underbrace{\frac{Pr(Y=1|Y^{*}=0)}{Pr(Y=0|Y^{*}=0)}}_\text{Posterior odds} 
		&= \underbrace{\frac{Pr(Y^{*}=0|Y=1)}{Pr(Y^{*}=0|Y=0)}}_\text{Likelihood Ratio} 
		\times \underbrace{\frac{Pr(Y=1)}{Pr(Y=0)}}_\text{Prior odds} 
		\\ \\
		\frac{1-NPV}{NPV} &= \frac{1-Sn}{Sp} \times \frac{Pr(Y=1)}{1-Pr(Y=1)}
		\end{align*}
		
		\begin{itemize}
		\item LR(-) tells us how much more a likely a condition is, given a negative test result
		\end{itemize}
	
	\end{itemize}

\end{enumerate}

\section{Receiver Operating Characteristic Curves}
\begin{itemize}

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{ROC.png}
		\caption{ROC curve (solid line) with lower limit (dotted line), points indicate values for different cutoff points used in the prediction algorithm}
		\label{ROC}
\end{figure}

\item A Receiver Operating Characteristic Curve (ROC) is drawn by graphing different cutoff values of a prediction algorithm on a graph of 1-Specificity (FPR) on the x-axis versus Sensitivity (TPR) on the y-axis (Figure ~\ref{ROC})
\item the area under the ROC curve (AUC) can be estimated by the area underneath this curve
	\begin{itemize}
	\item AUC has the range [0.5, 1], and can be interpreted as the discriminative ability of a model
	\item simply put, the AUC is the probability that given a pair of individuals, one with disease and the other disease-free, the two individuals are correctly labeled by the prediction algorithm
	\item an AUC of 0.5 means the prediction algorithm has no more predictive ability than simply tossing a coin
	\item an AUC of 1.0 means the prediction algorithm has perfect discriminative ability 
	\item the closer a curve comes to the top left corner of the graph of 1-Sp vs Se, the higher its discriminative ability (AUC)
	\end{itemize}
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Study Designs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Study Population}
\subsection{Types of populations}
\begin{itemize}
\item[\-] populations: the focus on populations in epidemiology distinguishes it from other biomedical disciplines
\end{itemize}

There are four types of populations:
\begin{enumerate}
\item \textbf{study population}: a \em sample \em  of subjects for whom we make observations and collect data on, a subset 
of the source population
\item \textbf{source population}: a group of individuals who are designated by personal, geographic, or temporal 
characteristics as eligible for the study
	\begin{itemize}
	\item should be clearly defined by the investigator (ex: all 18-50 year old men living in Suffolk county between the years 2010-2011 at risk for prostate cancer)
	\item it is not necessary to identify every member of the source population
	\item also called the \em sampling frame\em
	\end{itemize}
\item  \textbf{base population}: group of individuals \em at risk \em of developing the outcome of interest, the population 
from which study cases arise
	\begin{itemize}
	\item the group of individuals we would like to make valid causal inferences on
	\item can be equivalent to the source population
	\item is the sum of all eligible person-time	
	\item \textbf{primary study base}: base population is identified first, then cases are ascertained
		\begin{itemize}
		\item clearly defined study population
		\item roster often exists (ex: nested case-control study, study of current HMO members)
		\item if the study base is known, baseline rates estimated for the population can be used in case-control studies
		\end{itemize}
	\item \textbf{secondary study base}: cases are ascertained first, then the base population is identified
		\begin{itemize}
		\item base often defined as the population that if they had developed the outcome of interest \em would have 
been \em a case in the study
		\item a case-defined study (ex: hospital-based case-control study)
		\item also known as the \em study base, population at risk, \em or \em source population \em (Rothman)
		\end{itemize}
	\end{itemize}
\item \textbf{target population}: population the investigator would like to generalize the study results to
	\begin{itemize}
	\item designated by the investigator according to their objectives
	\item typically not observed by the investigator
	\item investigator hopes the study results are externally valid to this population
	\end{itemize}
\end{enumerate}

\subsection{Case-Definition \& the Study Base}
\begin{itemize}
\item \textbf{Case}: is a person who developed the outcome and was \em diagnosed \em as a case
\item must be part of the study base when outcome occurs
\item if there are persons with no opportunity of diagnosis one must either
	\begin{enumerate}
	\item exclude them from the study-base
	\item include them by redefining the study base
		\begin{itemize}
		\item relaxing or removing diagnostic criteria for case-ness
		\end{itemize}
	\end{enumerate}
\item Case definition is implicitly intertwined with the availability of diagnosis
	\begin{enumerate}
	\item if valid diagnostic procedure rarely performed or highly subjective
		\begin{itemize}					
		\item must be careful that exposure is not a determinant of diagnosis, otherwise could induce a selection bias
		\end{itemize}
	\item if valid diagnostic procedure is widely available
		\begin{itemize}
		\item benefits of rigorous case definition most often outweights risk of bias from selective access / diagnosis
		\end{itemize}
	\end{enumerate}
\item Studies of benign conditions run a high risk for mistaking correlates of access to medical care for etiologic factors
\end{itemize}

\section{Randomized Experiments}
\begin{itemize}
\item randomized experiments: a study where the investigator uses a stochastic (random) mechanism to assign treatment
    \begin{itemize}
    \item randomization is \em expected \em to balance the distribution of risk factors for the  outcome between treatment groups (groups are comparable)
    \item intention-to-treat (ITT) analysis: estimates the effect of the instrument (treatment assignment) and not the treatment itself
    \item the study you would want to conduct (or are trying to mimic) to estimate causal effects
    \item not always practical or ethical to do a randomized experiment
    \item ideal randomized experiment: groups are marginally exchangeable
        \begin{itemize}
        \item treatment groups are exchangeable: allows estimation of average level causal effects
        \item near infinite sample size (no sampling variability)
        \item no loss to follow-up
        \item full adherence
        \item double-blind
        \end{itemize}
    \item crossover randomized experiment: individuals are observed during two or more periods
        \begin{itemize}
        \item individual is exchangeable: allows estimation of individual level causal effects  
        \item assumptions:
            \begin{itemize}
            \item exposure effect has short duration
            \item exposure has no carry-over effect
            \item outcome is an abrupt event that completely resolves by the next period (reversible event)
            \item all prognostic factors other than treatment remain constant across periods
            \end {itemize}
        \end{itemize}
    \end{itemize}
\item unconditional (marginal) randomization: treatment probability is independent of any covariates, $Pr[A] \amalg L$
\item conditional randomization: treatment probability depends on the level of a covariate or set of covariates, $Pr[A=a^ \prime|L=l^ 
\prime] \neq [A=a^{ \prime 
\prime} |L=l ^{\prime \prime}] $
\end{itemize}


\section{Observational Studies}
\begin{itemize}
\item a study set up by nature where the investigator observes exposures and outcome
\item prone to biases, especially confounding
\item include cohort studies and case-control studies
\end{itemize}

\section{Cohort Studies}
\begin{itemize}
\item Cohort: observational studies analogous to randomized experiments, but the investigator does not assign exposure
\item groups defined by exposure status
\item occurrence of disease compared in follow up time
\end{itemize}

\subsection{Closed Cohorts}
\begin{itemize}
\item membership defining event
	\begin{itemize}
	\item not necessarily at one point in time, depends on time scale (ex: birth cohort, military service, 50 yr olds)
	\end{itemize}
\item once a member always a member
\item fixed population: no new members can enter (static cohort)
\item measures of association 
	\begin{itemize}
	\item Cumulative Incidence Ratio/Difference: can be directly measured
	\item Odds Ratio
	\item Incidence Rate Ratio/Difference
	\end{itemize}
\item drawbacks
	\begin{itemize}
	\item loss to follow up
	\item competing risks
	\item changes in exposure status over time (except fixed cohorts)
	\item incidence rate chosen measure of association due to these drawbacks
	\item in the case of an inevitable outcome (e.g. death) the cumulative incidence will approach 1 as follow-up increases
	\end{itemize}
\end{itemize}

\subsection{Open Cohorts}
\begin{itemize}
\item membership defined by state that can change over time  (ex: 
MA residents, HMO members)
\item PT from changing roster of participants (dynamic population)
	\begin{itemize}
	\item individuals may contribute small or large amounts of person time
	\item individuals may come and go
	\item individuals may contribute person time to multiple exposure categories
	\end{itemize}
\item rates can be directly measured, but cumulative incidence cannot be directly measured
\item to get cumulative incidence we need $C=1-\exp[-\sum(I*t)]$ 
\end{itemize}

\subsection{Practical issues in Cohorts}
\begin{enumerate} 
\item are expensive for rare diseases with long induction periods. Four solutions:
	\begin{enumerate}
	\item use registry to reduce cost of monitoring
	\item use a historical or retrospective cohorts
	\item[\-] \#1 and \#2 often use records, thus may suffer from
		\begin{itemize}
		\item poor accuracy \& misclassification
		\item missing records that are related to exposure in some way (selection bias)
		\end{itemize}	
	\item use general population information (if exposure is rare)
		\begin{itemize}
		\item beware of ``healthy worker effect": 
			\begin{itemize}
			\item the exposure group are fundamentally different from the population
			\item exposure depends on meeting certain criteria that are not met in general population
			\end{itemize}
		\end{itemize}
	\item perform a nested case-control or case-cohort study
	\end{enumerate}
\item timing of outcome events hard to determine for chronic diseases with insidious onset
	\begin{itemize}
	\item measurement is at discrete intervals
	\item use of a minimum lag period is advisable when long latent periods are inevitable
		\begin{itemize}
		\item examples: undiagnosed disease or prodromal periods
		\end{itemize}
	\item can be viewed as a source of measurement error and source of bias
	\end{itemize}
\item tracing individuals must be feasible
	\begin{itemize}
	\item loss to follow up can damage validity
		\begin{itemize}
		\item if related to both exposure and disease will have bias
		\end{itemize}
	\item will add to cost
	\end{itemize}
\end{enumerate}



\section{Case-Control Sampling Schemes}
\begin{itemize}
\item Material for this section was drawn from:
	\begin{itemize}
	\item (Mittleman 2009; Rothman 2008; Walker 2009)
	\item (Miettenen 1976; Wacholder 1987; Langholz 1990; Wacholder 1991)
	\end{itemize}
\item The case-control paradigm is founded upon the odds ratio's symmetry
		\begin{align*}
		\frac{Pr[E|D]/Pr[\bar{E}|D]}{Pr[E|\bar{D}]/Pr[\bar{E}|\bar{D}]} = \frac{Pr[D|E]/Pr[\bar{D}|E]}{Pr[D|\bar{E}]/Pr[\bar{D}
|\bar{E}]} 
		\end{align*}
\end{itemize}



\subsection{Useful Concepts}
\subsubsection*{Definitions}
\begin{itemize}
\item \textit{Risk-Set:} the set of persons at risk when an event occurs
	\begin{itemize}
	\item implicitly matched on time
	\item may be constrained further to additional matching factors (e.g. age) 
	\item comparing a case to her risk set is very useful for dealing with time trends in exposure or other characteristics of an open cohort that are influenced by time
	\end{itemize}
\item \textit{Incident Case:} outcome event occurred during follow up. These are usually sampled from the case-series.
\item \textit{Prevalent Case:} outcome event occurred before the start of follow up. These are usually excluded from the case-series.
\item \textit{Sampling Person-Time:} sampling a control with probability proportional to at risk-person-time experience.
\item \textit{Sampling Fraction:} or selection frequency, is the probability of being selected into the study. 
	\begin{itemize}
	\item if either the selection frequency for cases $f$ or controls $g$ are related to \em exposure\em, our sample estimate 
$\widehat{OR}=\frac{ad}{bc}$ is a biased estimator the true population $OR=\frac{AD}{BC}$ because of selection bias. 
	\item by design cases are oversampled in a case-control study and thus $f \ > \ g$ (selection frequencies usually do differ by 
disease status)
	\end{itemize}
\end{itemize}

\subsubsection*{Sampling Fractions}
\begin{itemize}
\item Define sampling fractions:
	\begin{align*}
       f^{\prime} = \frac{a}{A} \qquad f^{\prime \prime} = \frac{b}{B} \qquad g^{\prime} = \frac{c}{C} \qquad g^{\prime \prime} = \frac{d}
{D}
	\end{align*}
\item[\-] $a/b$ is the distribution of exposure in sampled cases. It estimates $A/B$, the distribution of exposure in cases of the entire 
population
\item[\-] $c/d$ is the distribution of exposure in controls. It estimates $C/D$, the distribution of exposure in the \em person-time \em of 
the entire population
\item the incidence rate ratio estimate is
	\begin{align*}
	\widehat{IRR} = \widehat{OR} = \frac{(f^{\prime} \cdot A) \times (g^{\prime} \cdot D)}{(f^{\prime \prime} \cdot B) \times (g^{\prime 
\prime} \cdot C)} =  \frac{a \times d}{b \times c}
	\end{align*} 
\item we have bias if sampling fractions for cases or controls are related to exposure \\ 	($f^{\prime} \neq f^{\prime \prime} or \ g^
{\prime} \neq g^{\prime \prime}$)
\item note that due to sampling variability of $a,b,c,d$, $\rightarrow \hat{OR}$ provide an estimate of IRR with 
a margin of error. Thus we say it is a consistent estimator of the $IRR$
\end{itemize}

\subsubsection*{Case-Control Sampling}
$$\widehat{IRR}=\frac{I_1}{I_0}=\frac{\frac{A}{N_1}}{\frac{B}{N_0}}=\frac{\frac{A}{B}}{\frac{N_1}{N_0}} \times \frac{(f/f)}{(g/g)}=\frac{\frac{a}{b}}{\frac{c}{d}}=\frac{a \times d}{b \times c}=\widehat{OR}$$
\begin{itemize}
\item assuming $f=f^{\prime}=f^{\prime \prime}$ and $g=g^{\prime}=g^{\prime \prime}$
\item \textbf{cases}: give relative information about the numerator of the rates you would have calculated in a cohort study, $\frac{a}{b}$
	\begin{itemize}
	\item a sample of the cases may be used, but the cases must be sampled independent of exposure
	\end{itemize}
\item \textbf{controls}: give relative information about the denominator of the rates you would have calculated in a cohort study, $\frac
{N_1}{N_0}$
\item cases and controls must represent the same study base \em over the time period of the study \em, conditional upon all measured risk factors	
\item non-random samples can be used with complete case ascertainment only if...
	\begin{itemize}
	\item distribution of exposure is same in control series as in a random sample of the secondary base
	\end{itemize}
\item \textbf{stratified sampling fractions}: we can allow sampling fractions to vary by covariate leve. thus within strata that define the sapling fraction, the rate ratio is consistent.	
	\begin{itemize}
	\item this allows us to ensure adequate numbers of controls in each confounder level (i.e. we
can oversample some strata), for better control of confounding
	\item it's \em like \em the idea of a matched caco but it's not the same
	\end{itemize}
\end{itemize}

\subsubsection*{Non-random selection of controls}
\begin{itemize}
\item pseudo-random sampling: choose controls thrugh a process that is arguably random in nature (e.g. next life birth)
\item \textbf{homology argument}: employed when a random sample not possible
	\begin{enumerate}
	\item compare prevalence of risk factors for main outcome among exposed and unexposed, stratified by the diseases (or other factors) used to assemble control series 
		\begin{itemize}
		\item if see pattern probably reflects source population remove ones that are markedly different
		\end{itemize}
	\item look at idiosyncratic features (person, time, and place) and ask if, hypothetically, these characteristics would result in a distribution of risk factors that is different than the unknown distribution in the actual source population. 
		\begin{itemize}
		\item if you can't think of any, probably represents source population
		\end{itemize}
	\end{enumerate}
\end{itemize}	
	
\subsubsection*{Case-Definition}
\begin{itemize}
\item as mentioned before, a \em case \em implies diagnosis
	\begin{itemize}
	\item subjective or rare diagnostic procedures can spuriously relate case-definition to exposure
	\item for mild disorders that do not require medical attention should either...
		\begin{itemize}
		\item ascertain all cases
		\item restrict base to those that would use medical services if develop disease
		\item ensure that medical treatment seeking is not associated with exposure
		\end{itemize}
	\end{itemize}
\item feasibility can often be achieved by refining case definition (to better define a practical study base)
\item if redefine case eligibility definition be sure to redefine control eligibility as well
\end{itemize}

\subsubsection*{Control Sources}
\begin{enumerate}
\item population controls: random sampling of a population list
\item hospital/clinic controls: sample control disease that have similar referral patterns for case disease, \em but \em admission is unrelated to exposure of interest 	
	\begin{itemize}
	\item subjects with case disease would have been admitted for control disease (vice versa)
	\item ideally want to use several diseases to lessen the chance of an association with exposure
		\begin{itemize}
		\item for unrelated diseases, don't want their occurence, detection, or likelihood of hospitalization to be related to outcome or exposure.
		\end{itemize}
	\item \em case definition \em should be handled with care. Cases that occur...
		\begin{enumerate}
		\item \underline{after arrival} to the hospital: base is persons who \em while in the hospital \em, if they developed the outcome, they would have been included a case in the study, \em regardless of admission diagnosis \em
		\item \underline{before arrival} to the hospital: base is persons who if they developed the outcome would have been referred (or came on own accord) to hospital (i.e. may be a referring practice, HMO plan, or catchment area)
		\item[-] if both types of cases used you have a composite study base
		\end{enumerate}
	\end{itemize} 
\item random digit dialing: sample of households with telephones, several methodological issues
(secondary study base)
\item dead controls: comparability issues, not members of the base population
\item death registry for mortality studies: deaths from causes whose occurrence is unrelated to exposure
\item deterministic (non-random)
	\begin{itemize}
	\item divide base into non-overlapping strata $\rightarrow$ 100\% sample from case's strata
	\item ex. neighborhood controls: sample by residence
	\item ex. family/friend controls: case identifies a control, may lead to \em overmatching \em
	\end{itemize}
\end{enumerate}

\subsubsection*{Case-Control Paradigm}
\begin{itemize}
\item regardless of design (cohort or case-control), the comparison is always between the exposed and unexposed
\item the design employs sampling to ascertain exposure distribution in cases and the \em study base \em
\item is analogous to sampling from a full cohort, even if not possible to enumerate
	\begin{itemize}
	\item assumption that the unsampled persons' data is \em missing at random \em (MAR)
		\begin{enumerate}
		\item values of covariates for those who do (or do not) develop disease do not depend on whether the person is included as a case (or control) in the study
		\item met only if sampling fractions for cases and controls are independent of exposure
		\end{enumerate}
	\item absolute rates can be estimated, with complete case ascertainment, if we have either
		\begin{enumerate}
		\item known sampling fractions
		\item external information about the base (i.e. incidence rate for entire population)
		\end{enumerate}
	\end{itemize}
\item an advantage of case-control over cohortis by studying fewer people, can put more
effort/resources into ascertainent and measureent of confounders for better accracy	
\item fallacies
	\begin{enumerate}
	\item \em ``trohoc" fallacy\em: false belief that controls should be healthy and absent of disease
		\begin{itemize}
		\item distracts from the goal of creating a control group that efficiently samples from the base population
		\item exchangeability between cases and controls is \em not \em the same as comparability of cases and controls
			\begin{itemize}
			\item want comparability of cases and controls where they are both coming from the same base population
			\end{itemize}
		\item removing a causal intermediate from the control group is inappropriate
			\begin{itemize}
			\item to do so would change the distribution of exposure in the controls such that it is no longer representative of the study base that gave rise to the cases
			\end{itemize}
		\end{itemize}
	\item \em at risk for exposure\em : restricting the control series to persons (or periods) that are at risk for exposure is akin to overmatching, even in a case-crossover study
	\end{enumerate}
\end{itemize}



\subsection{Case-Cohort} \textit{Sampling at baseline}

\begin{figure}[H]
  \begin{center}
    \subfigure[Underlying Cohort]{\label{ccohort1}\includegraphics[scale=0.4]{ccohort1.png}} \hspace{2 cm}
    \subfigure[Data Structure]{\label{ccohort2}\includegraphics[scale=0.4]{ccohort2.png}} \hspace{2 cm}
  \end{center}
  \caption{Case-Cohort Sampling}
  \label{ccohort}
\end{figure}

\subsubsection*{Sampling Frame}
\begin{itemize}
\item controls sampled
	\begin{itemize}
	\item at baseline
	\item independently of exposure and outcome
	\item randomly from entire cohort
	\end{itemize}
\item forms sub-cohort from which risk-sets are formed during analysis	
\end{itemize}

\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item incident cases are observed for \underline{entire} cohort
\item date of entry, termination of follow up, and outcome ascertainment collected on \underline{entire} cohort
\item exposures, confounders, and effect modifiers measured only for \underline{sub}-cohort \em and \em incident cases
	\begin{itemize}
	\item identification and ascertainment of controls takes place \em before \em identification and ascertainment of cases
	\end{itemize}
\end{itemize}

\subsubsection*{Strengths (compared to Nested-Case-Control)}
\begin{itemize}
\item control ascertainment does not depend on case identification
	\begin{itemize}
	\item faster data collection - can identify controls immediately, before cases occur
	\item can collect expensive data ``in bulk" and save money
	\item controls suitable for multiple outcomes 
		\begin{itemize}
		\item however, 95\% CI require adjustment because risk sets are correlated
		\item in regression the data has a correlated structure
		\end{itemize}
	\item cases during extended follow up do not need new controls
	\item multiple time scales allowed $\rightarrow$ better control of confounding by time
	\item data can be re-analyzed using risk-set sampling (e.g. post-hoc studies)
	\end{itemize}
\item provides data for external comparisons (e.g. entire cohort/population)
\item more efficient when little late entry and little right-censoring	
\item can estimate baseline risk and risk ratio in full cohort
\end{itemize}

\subsubsection*{Weaknesses (compared to Nested-Case-Control)}
\begin{itemize}
\item power cannot be estimated until end of follow up because it depends on the size of the exposure overlap in the final risk-sets
\item cases and controls are assessed at different times, need to ensure comparable measurement accuracy and compliance
\item cases outside of sub-cohort have no baseline data, must be assessed at event
\item not well suited for time-varying exposures or confounding because must be measured in every control at every outcome event (more 
\$\$\$)
\item less efficient for staggered entry and substantial right-censoring
\item risk-sets are not randomly sampled and are therefore correlated $\rightarrow$ complicated analysis
\item forced to measure deteriorating lab samples at end of follow up, to avoid differential measurement error between sub-cohort cases, controls, and cases outside the sub-cohort
\end{itemize}



\subsection{Density-Sampling} \textit{Sampling PT over follow up}
\begin{figure}[h]
  \begin{center}
    \subfigure[Underlying Cohort]{\label{density1}\includegraphics[scale=0.4]{density1.png}} \hspace{2 cm}
    \subfigure[Data Structure]{\label{density2}\includegraphics[scale=0.4]{density2.png}} \hspace{2 cm}
  \end{center}
  \caption{Density Sampling}
  \label{density}
\end{figure}

\subsubsection*{Sampling Frame}
\begin{itemize}
\item the probability of sampling a control should be proportional to at risk person-time
\item[\-] this is satisfied one of three ways:
	\begin{enumerate}
	\item matched on time (see risk-set and nested-case-control sections)
	\item sampling controls at a constant rate over follow up
	\item STEP 1. randomly sample (with replacement) a date and a person from base\\STEP 2. if person under observation and at risk on that date, select as a control.
	\item[-] (\#3) requires that
		\begin{enumerate}
		\item controls are eligible for selection during the same period that they are eligible to become a case
		\end{enumerate}
	\item[-] (\#2) and (\#3) are unmatched on time
	\item[-] matching on time (\#1) is subsumed within sampling with a steady rate (\#2), which is subsumed with sampling in proportion 
to interval at risk (\#3)
	\end{enumerate}
\item steady state assumption is only required for \#2 and \#3 because risk-sets are not used in sampling or analysis. Specifically this means constant incidence 
and prevalence (see Chapter \ref{chap:Measures of Frequency and Occurrence}: Measures of Frequency and Occurrence for more detail)
\end{itemize}

\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item incident cases are observed for \underline{entire} cohort
\item date of entry, termination of follow up, and outcome ascertainment collected on \underline{entire} cohort
\item exposures, confounders, and effect modifiers measured for all controls \em and \em incident cases
\item[\-] If \underline{not} sampling at steady rate:
	\begin{itemize}
	\item control ascertainment cannot occur until end of follow up and the full period of risk can be specified
	\end{itemize}
\end{itemize}

\subsubsection*{Strengths \textit{(as compared to Cumulative-Incidence)}}
\begin{itemize}
\item better for chronic disease epidemiology 
	\begin{itemize}
	\item NO need for rare disease assumption to estimate incidence rate ratio
	\item long follow up makes prevalent controls (end of FU) more likely to induce selection bias
	\item incidence density sampling avoids this by sampling over time
	\end{itemize}
\end{itemize}

\subsubsection*{Weaknesses of Plain Density Sampling \textit{(as compared to Risk-Set)}}
\begin{itemize}
\item steady state assumption may not be reasonable
\item identification of controls is much slower
\end{itemize}

\subsubsection*{Weaknesses of Steady Rate Density Sampling \textit{(as compared to Risk-Set)}}
\begin{itemize}
\item more resource intensive
\end{itemize}



\subsection{Risk-Set Sampling} \textit{Sampling PT matched on time}
\begin{figure}[H]
  \begin{center}
    \subfigure[Underlying Cohort]{\label{riskset1}\includegraphics[scale=0.4]{riskset1.png}} \hspace{2 cm}
    \subfigure[Data Structure]{\label{riskset2}\includegraphics[scale=0.4]{riskset2.png}} \hspace{2 cm}
  \end{center}
  \caption{Risk-Set Sampling}
  \label{riskset}
\end{figure}

\subsubsection*{Sampling Frame}
\begin{itemize}
\item controls derived from a random sample of case's risk-set \em after \em case ascertainment
	\begin{itemize}
	\item cases and controls are implicitly \em matched on time \em
	\item risk-set can be further specified in terms of other covariates (age, gender, etc.)
		\begin{itemize}
		\item matching factors should be a characteristic that could be easily assessed for the entire population
		\item and should be something where you're not interested its effect
	\end{itemize}
\end{itemize}

\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item incident cases are sampled from the study base
\item date of entry, termination of follow up, and outcome ascertainment for cases and controls
\item exposure, covariates, and any matching factors assessed on case index day
\end{itemize}

\subsubsection*{Strengths (compared to unmatched density sampling)}
\begin{itemize}
\item no steady state assumption needed
	\begin{itemize}
	\item hazard ratios are estimated within risk-sets (e.g. at every failure time) and pooled using a weighted average
	\end{itemize}
\item matching on time provides
	\begin{itemize}
	\item comparable measurement between cases and controls
	\item allows assessment of exposures that vary over time (also confounding)
	\item lab samples are matched on ``deterioration" $\rightarrow$ comparability
	\end{itemize}
\item[\-] if there are time trends in exposure then a matched analysis is required (assuming no other matching factors)
\end{itemize}

\subsubsection*{Weaknesses}
\begin{itemize}
\item matching on time subjects the analysis to several constraints
		\begin{itemize}
		\item time scale is restricted to the one used to form risk-sets $\rightarrow$ potential residual confounding by time
		\end{itemize}
\item slow data collection - selection of controls depends on identification and \em confirmation \em of a case
\end{itemize}



\subsection{Nested-Case Control} \textit{Risk-set sampling in an enumerated cohort}
\subsubsection*{Sampling Frame}
\begin{itemize}
\item \textbf{technically} any sampling frame can be used
\item most often risk-set sampling is used to tightly control for confounding by time
\item here the study base is a fully enumerated cohort
\item what follows applies to the nested-case-control under risk-set sampling
\end{itemize}

\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item incident cases are observed for \underline{entire} cohort
\item date of entry, termination of follow up, and outcome ascertainment collected on \underline{entire} cohort
\item exposures, confounders, and effect modifiers measured only for the \underline{risk-sets} \em and \em incident cases (\em only for index dates \em)
	\begin{itemize}
	\item identification and ascertainment of controls takes place \em after \em identification and ascertainment of cases
	\end{itemize}
\end{itemize}

\subsubsection*{Strengths (compared to Case-Cohort)}
\begin{itemize}
\item power is virtually independent of the size of the entire cohort
\item matching on time provides
	\begin{itemize}
	\item comparable measurement between cases and controls
	\item allows assessment of exposures that vary over time (also confounding)
	\item data on time-varying exposures in cases and controls does not need to be collected beyond follow up of case (less money)
	\item lab samples are matched on ``deterioration" $\rightarrow$ comparability
	\end{itemize}
\item simpler analysis because risk-sets are randomly sampled (e.g. independent)
\item[\-] note that a matched analysis is required if time is related to exposure (assuming no other matching factors)
\end{itemize}

\subsubsection*{Weaknesses (compared to Case-Cohort)}
\begin{itemize}
\item matching on time subjects the analysis to several constraints
		\begin{itemize}
		\item time scale is restricted to the one used to form risk-sets $\rightarrow$ potential residual confounding by time
		\item not well suited for multiple outcomes
		\item difficult to make external comparisons because controls are not a random sample of entire cohort
		\item cannot be analyzed as case-cohort (i.e. post-hoc) because controls' selection is conditional upon being free of 
outcome at time of selection. Could introduce selection bias.
		\end{itemize}
\item slower data collection - selection of controls depends on identification and \em confirmation \em of a case
\end{itemize}



\subsection{Cumulative-Incidence} \textit{Sampling at end of follow up}
\begin{figure}[h]
  \begin{center}
    \subfigure[Underlying Cohort]{\label{cumin1}\includegraphics[scale=0.4]{cumin1.png}} \hspace{2 cm}
    \subfigure[Data Structure]{\label{cumin2}\includegraphics[scale=0.4]{cumin2.png}} \hspace{2 cm}
  \end{center}
  \caption{Cumulative Incidence Sampling}
  \label{cumin}
\end{figure}

\subsubsection*{Sampling Frame}
\begin{itemize}
\item requires a closed cohort
\item incident cases are selected over follow up
\item non-cases are sampled as controls at the end of follow up
\end{itemize}
\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item retrospective measurement of exposure
\end{itemize}
\subsubsection*{Strengths}
\begin{itemize}
\item FAST and resource efficient
\item short follow-up makes them suitable for acute-epidemic outbreaks of short duration
\end{itemize}

\subsubsection*{Weaknesses (as compared to all others)}
\begin{itemize}
\item \textbf{this is the only design that requires rare disease assumption} to interpret the incidence odds ratio as the incidence rate ratio
\item not ideal for chronic disease epidemiology when follow up periods are long
	\begin{itemize}
	\item use of prevalent controls likely to introduce bias because controls' selection is conditional upon survival to the end of 
follow up
	\item potential for recall bias since baseline exposure is measured retrospectively 
	\end{itemize}
\item potential for selection bias if cases and controls selected from same source
\end{itemize}



\subsection{Case-Crossover} \textit{Case become their own control}
\begin{figure}[h!]
  \begin{center}
    \subfigure[Underlying Cohort]{\label{ccross1}\includegraphics[scale=0.4]{ccross1.png}} \hspace{2 cm}
    \subfigure[Data Structure]{\label{ccross2}\includegraphics[scale=0.4]{ccross2.png}} \hspace{2 cm}
  \end{center}
  \caption{Case-Crossover Sampling}
  \label{ccross}
\end{figure}

\subsubsection*{Sampling Frame}
\begin{itemize}
\item requires transient exposure and acute effect on the outcome 
\item only cases are enrolled, use self as control
\item a short period immediately before the outcome would be defined as the case window
\item a matched length of period before the case period is defined as control window
\item Control window can be randomly selected outside the case window, can be prospective, retrospective, or bidirectional
\item the selection of control is based on the assumption of \textbf{conditional independence of the exposure} and \textbf{random distribution of the exposure over time}
\end{itemize}
\subsubsection*{Measurement \& Ascertainment}
\begin{itemize}
\item retrospective measurement of exposure
\end{itemize}
\subsubsection*{Strengths}
\begin{itemize}
\item suitable for transient exposure where control is hard to find
\item appropriate for environmental study where exposure (eg. air pollution) is common on all, no concurrent control is available
\item do not require controls to be recruited
\item immune to selection bias
\item effectively control for both \textbf{measured and unmeasured inter-individual confounding}
\end{itemize}

\subsubsection*{Weaknesses (as compared to all others)}
\begin{itemize}
\item It cannot control the \textbf{intra-individual confounding} (eg. someone always drink coffee after sex)
\item potential for recall bias since exposure is measured retrospectively
\item potential for exposure misclassification since induction period is usually unknown
\item susceptible to exposure trend bias, if the baseline exposures are associated with a longitudinal trend 
\item cannot be used for exposures that do not vary over time (at the individual level)
\end{itemize}

\subsubsection*{Analysis of case crossover study}
\begin{itemize}
\item Each case-control period pair can be viewed as stratum
\item unit of analysis is window of time: time just before acute event (case window) compared with other times (control windows) 
\item Mantel-Hansel stratified analysis can be used to compute summary rate ratio (person-time data)  \item The \textbf{rate ratio is identical to risk ratio}
	\begin{itemize}
	\item Because the follow-up periods for each control and case were fixed and identical and there was no loss to follow-up.  
	\end{itemize}
\item The RGB (Robins-Greenland-Breslow) variance can be used to compute confidence interval and hypothesis test. 
	$$\hat{Var}_{RGB} = \frac{A}{BC} = \frac{\frac{M_{1i}N_{1i}N_{0i}}{T_i^2}}{\frac{a_i N_{0i}}{T_i} \frac{b_i N_{1i}}{Ti}}$$

\item MH analysis in case-crossover study is algebraically identical to matched analysis 
\item The McNemar method or conditional logistic regression can also be used to obtain the effect estimate and confidence interval
\end{itemize}

\subsubsection*{Induction period analysis}
\begin{itemize}
\item The determination of the induction period is based on both biological plausibility and sensitivity analysis. 
\end{itemize}
\begin{figure}[H]
	\centering
		\includegraphics[scale=0.45]{ccross4.png}
		\caption{Induction period anlysis}
		\label{ccross4}
\end{figure}

\subsubsection*{Case-time-control correction of exposure trend bias}
\begin{itemize}
\item require to recruit control
\item control-crossover odds ratio would be computed 
\item control-crossover OR is used for adjustment of the case-crossover OR and obviate the potential exposure trend bias
\item The validity of such adjustment is based on the assumption that baseline exposure trend of case and control is exchangeable, which is usually not the case
\begin{figure}[H]
	\centering
		\includegraphics[scale=0.45]{ccross5.png}
		\caption{Case-time-control study}
		\label{ccross5}
\end{figure}
\end{itemize}



\subsection{Case-Only}

\section{Cross Sectional Studies}


\section{Ecological Studies}


\pagebreak

	\begin{sidewaystable}[h!]
	\vspace{40em}
	\centering
	\caption{Source population, Design, \& Estimand}
	\begin{tabular}{p{1.2 in} p{1.6 in} p{1.3 in} p{2 in} p{1.3 in}}
	\hline \hline
	\textbf{Observation Space} & \textbf{Design} & \textbf{Additional Sampling} & \textbf{Design} & \textbf{Estimate}
	\\
	\hline 
	& & & &
	\\
	%row------------------------------------
	\textsf{Persons} 		& \multirow{6}{1.5 in}{\textsf{Closed cohort with fixed follow up}}  
								&  \textsf{Non-cases} 	
								&  \textsf{Traditional Case-Control (cumulative incidence)} 
								&  \textsf{Odds Ratio}
								\\
							& 	& & & 
								\\
								\cline{3-5}
								\\
							& 	& & &
								\\
							&	&  \textsf{All persons} 
								&  \textsf{Case base} 	
								&  \textsf{Risk Ratio}
								\\
							&   & & & 
								\\
	\hline							
							&   & & &
								\\
	%row------------------------------------
	\multirow{14}{*}{\textsf{Person-Time}}
							
								& \textsf{Closed cohort with changing exposure or open cohort}
								& \textsf{Pool of Person Time (incidence density)}	
								& \textsf{Modern Case-control}	
								& \textsf{rate ratio}
								\\
								& & & & 
								\\
								\cline{2-5}
								& & & &
								\\
	%row------------------------------------
								& \multirow{6}{1.5 in}{\textsf{Cohort with changing baseline incidence}}
									& \textsf{Non-cases in risk-set}
									& \textsf{Matched on time case-control}
									& \textsf{Rate ratio matched on time}
									\\
								&	& & & 
									\\
									\cline{3-5}
									& & & 
									\\
								&	& \textsf{Sub-cohort reuse in risk sets}
									& \textsf{Case-Cohort}
									& \textsf{Rate ratio matched on time}
									\\
								&	& & & 
									\\
								\cline{2-5}
								&	& & & \\
	%row------------------------------------
								& \multirow{5}{1.5 in}{\textsf{Matched on person}}
									& \textsf{Cases only}
									& \textsf{Case-Crossover}
									& \textsf{Rate ratio}
									\\
								&	& & & 
									\\
									\cline{3-5}
								&	& & &
									\\
								&	& \textsf{Non-cases}
									& \textsf{Case-Time Control}
									& \textsf{Rate ratio}
									\\
								&	& & & 
									\\							
	\hline \hline
	\end{tabular}
	\end{sidewaystable}

\pagebreak

\begin{table}
\centering
\caption{Comparison between study designs}
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	\textbf{Randomized Trial} &\\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{random assignment of treatments guarantees that the distributions of measured and unmeasured confounders in treated group and in untreated group are identical (ideally)} 		
	\item \textsf{often considered as the gold standard}
	\end{itemize}		
	\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{can eliminate all potential confounding by design (ideally)}
	\item \textsf{prospective study $\Rightarrow$ no recall bias}
	\item \textsf{double-blind $\Rightarrow$ reduce measurement error of the outcome }
	\item \textsf{placebo controlled $\Rightarrow$ reduce behavior and life style change due to intervention}
	\end{itemize}
	\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{non-compliance $\Rightarrow$ intention-to-treat analysis}
	\item \textsf{limited follow-up often prevent observation of the outcome for chronic disease}
		\begin{itemize}
		\item \textsf{surrogate outcome (e.g. blood pressure instead of CHD)}
		\end{itemize}
	\item \textbf{lack of generalizability}
		\begin{itemize}
		\item \textsf{a trade-off between internal validity and external validity
		\item \textsf{\textbf{non-compliance}}
		\item[\-] \textsf{the pattern of non-compliance in the trial and in general population may be different}
		\item \textsf{\textbf{unrepresentative patients}}
		\item[\-] \textsf{to make the treatment effect easier to detect $\Rightarrow$ select patients with fewer }comorbidities, fewer medications, not pregnant, and at earlier stage of disease }
		\item \textsf{\textbf{unrepresentative treatments}}
		\item[\-] \textsf{the range of exposure in a trial is often narrower than the range of exposure in real life}
		\end{itemize}
	\item \textsf{expensive (especially when the treatment is expensive)}	
	\end{itemize}
	\\ \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{informed consent (applies to all kinds of study mentioned below)}
	\item \textsf{need to pre-specify sample size (statistical power)}
	\item \textsf{need to pre-specify stopping rule}
	\item \textsf{need to consider equipoise between treatment arms}
	\item \textsf{cluster randomized trial: for vaccination, infectious disease, and community intervention, cluster randomized trial may be a good choice instead of individually randomized trial}
		\begin{itemize}
		\item \textsf{the number of randomized units is usually small and balancing between confounders may not be achieved}
		\end{itemize}
	\item \textsf{\textbf{randomized crossover study}}
		\begin{itemize}
		\item \textsf{control confounding by using an individual as his own control}
		\item \textsf{need a washout period between two crossover periods}
		\end{itemize}
	\item \textsf{blocking in randomized trial: randomization within levels of factors (effect modifiers)}
		\begin{itemize}
		\item \textsf{e.g. randomization within males and within female to ensure that exactly half of the male and half of the female participants received the treatment}
		\end{itemize}
	\end{itemize}
	\\ \hline
	\end{tabular}
\end{table}
	\pagebreak
	
\begin{table}
\caption{Comparison between study designs (cont.)}
\centering
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	
	\textbf{Closed cohort study} & \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{components of a closed cohort study}
		\begin{enumerate}
		\item \textsf{a heterogeneous group}
		\item \textsf{a fixed follow-up}
		\item \textsf{uniformly observed outcome}
		\item \textsf{clear 'time zero'}
		\end{enumerate}
	\item \textsf{cdefined by ``membership defining event''}
	\end{itemize}
	\\ \hline
	\textbf{Strengths} & 
	\begin{itemize}
	\item \textsf{establish temporality}
	\item \textsf{can estimate risk if none of the limitations} occur
	\end{itemize}
	\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{limitations prevent estimation of risk}
		\begin{enumerate}
		\item \textsf{loss to follow up}
		\item \textsf{competing risks}
		\item \textsf{changes in exposure status over time }
		\end{enumerate}
	\item \textsf{expensive for rare diseases}
	\end{itemize}
	\\  \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{risk eventually reaches $1$ for inevitable outcomes (e.g. death)}
	\item \textsf{should put equal efforts on data collection for each exposure group}
	\end{itemize}
	\\  \hline
	
	\textbf{Open cohort study} & \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{components of a open cohort study}
		\begin{enumerate}
		\item \textsf{person-time at risk for each exposure category}
		\item \textsf{number of outcome events within each exposure category}
		\end{enumerate}
	\item \textsf{need to assume constant rate during the entire follow-up if want to use a single summary rate estimate}
	\item \textsf{defined by ``membership defining state''} 
	\end{itemize}
	\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{can handle non-differential loss to follow-up}
	\item \textsf{can handle time-varying exposure}
		\begin{itemize}
		\item \textsf{one individual can contribute person-time to more than one exposure group}
		\end{itemize}
	\end{itemize}
	\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{suitable for multiple outcome event}
	\item \textsf{differential loss to follow-up}
	\item \textsf{competing risks}
	\item \textsf{expensive for rare diseases}
	\item \textsf{timing of outcome events hard to determine}
	\item \textsf{prospectively need to wait for a long time for events to occur}
	\item \textsf{hard to control pre-baseline confounding (e.g. need pre-treatment covariates to get propensity score for the treatment at baseline)}
	\end{itemize}
	\\ \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{can estimate rate ratio, but cannot estimate risk ratio directly (can calculate risk ratio by exponential formula $Risk=1-e^{-\sum_{i=1}^{t} \mbox{incidence rate}\times \Delta t}$)}
	\item \textsf{use stratified analysis, Poisson regression, Cox PH model, pooled logistic regression to analyze person-time data}
	\end{itemize}
	\\ \hline
	\end{tabular}
\end{table}
	
			
\begin{table}
\centering
\caption{Comparison between study designs (cont.)}
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	\textbf{Case-control study} & \textbf{cumulative incidence sampling} \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{requires a closed cohort as study base}
		\begin{itemize}
		\item \textsf{incident cases are selected over follow-up}
		\item \textsf{non-cases are sampled as controls at the end of follow up}
		\end{itemize}
	\item \textsf{requires rare disease assumption for risk ratio interpretation}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{fast} 
	\item \textsf{efficient}
	\item \textsf{can estimate risk ratio directly}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{rely on rare disease assumption}
	\item \textsf{inherited limitations of closed cohort study}
		\begin{itemize}
		\item \textsf{subject to 1). loss to follow-up, 2). competing risk, and 3). time-varying exposure/confounder}
		\end{itemize}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{often suitable for outbreak of infectious disease}
	\item \textsf{not suitable for chronic disease (may be a lot of competing risks and loss to follow-up in the base cohort)}
	\end{itemize}
	\\ &\\ \hline
	
	\textbf{Case-control study} & \textbf{incidence density sampling} \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{controls are used to estimate the relative distribution of person-time (not persons) across the exposure categories}
	\item \textsf{assume constant rate over entire follow-up (steady state assumption)}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{can estimate rate ratio}  
	\item \textsf{high cost and time efficiency compared to open cohort study}	
	\item \textsf{do not rely on rare disease assumption}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{cannot estimate baseline rate unless know the sampling fractions, compared with open cohort study}
	\item \textsf{cannot directly estimate risk ratio, compared with cumulative incidence sampling}
	\item \textsf{not suitable for studies on exposure time trend in disease risk}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{can allocate more resources on collecting detailed information from each participant, compared with open cohort study}
	\end{itemize}
	\\ &\\ \hline
		\end{tabular}
\end{table}


\begin{table}
\centering
\caption{Comparison between study designs (cont.)}
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	\textbf{Nested Cs-Cn study} & \textbf{risk-set sampling} \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{components of nested case-control study}
		\begin{itemize}
		\item \textsf{date of entry, termination of follow-up, and outcome ascertainment for cases and controls for the entire cohort}
		\item \textsf{exposures, confounders, and effect modifiers measured only for cases and controls at the index day}
		\end{itemize}  
	\item \textsf{matching on time (index day) and other matching factors which define the risk set}
	\item \textsf{constant rate within a risk set (instantaneous incidence rate $\Rightarrow$ hazard)}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{no steady state assumption for entire follow-up needed, compared with incidence density sampling}
	\item \textsf{reduce confounding by matching}
	\item \textsf{can pre-specify the sample size, compared with case-cohort study}
	\item \textsf{less expensive to assess time-varying exposure and confounders, compared with case-cohort study}
	\item \textsf{can assess the exposure of cases and controls at the same time (index day), compared with case-cohort study (no differential sample degradation)}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{cannot assess effect of matching factors (can assess the effect modification by the matching factor)}
	\item \textsf{time scale is restricted to the one used to form risk-sets $\Rightarrow$ potential residual confounding by time}
	\item \textsf{not well suited for multiple outcomes}
	\item \textsf{difficult to make external comparison, compared with case-cohort study}
	\end{itemize}
	\\ &\\ \hline
	\end{tabular}
\end{table}
	
\begin{table}
\centering
\caption{Comparison between study designs (cont.)}
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	\textbf{Case-cohort study} & \textbf{sub-cohort sampling at baseline}\\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{components of case-cohort study}
		\begin{itemize}
		\item \textsf{date of entry, termination of follow-up, and incidence cases were observed for entire cohort}
		\item \textsf{exposures, confounders, and effect modifiers measured only for sub-cohort and incident cases}
		\end{itemize}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{control ascertainment does not depend on case identification, compared with nested case-control study}
	\item \textsf{sub-cohort can serve as control groups for multiple outcomes}
	\item \textsf{can estimate baseline risk and risk ratio in full cohort if sampling fraction is known}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{power cannot be estimated until end of follow-up}
	\item \textsf{the exposure (and confounders) of cases and controls are assessed at different times $\Rightarrow$ measurement error} 
		\begin{itemize}
		\item \textsf{at the end of follow-up otherwise $\Rightarrow$ sample degradation}
		\end{itemize}
	\item \textsf{high cost for time-varying exposures or confounding $\Rightarrow$  must assess every control at every time point with an outcome event}
	\item \textsf{need to account for correlation between observations from sub-cohort in the analysis (GEE and robust variance)}
	\item \textsf{more sensitive to censoring for the sub-cohort}
	\item \textsf{the exposure/confounder distribution may be different
at the end of follow-up and at baseline for the sub-cohort}
	\end{itemize}
	\\ &\\ \hline
	\end{tabular}
\end{table}
	

\begin{table}
\centering
\caption{Comparison between study designs (cont.)}
	\begin{tabular}{ | p{1.8in} | p{5in} | }
	\hline 
	\textbf{Case-crossover study} & \\ \hline
	\textsl{Assumptions} & 
	\begin{itemize}
	\item \textsf{requires transient exposure and acute effect on the outcome} 
	\item \textsf{components pf case-crossover study}
		\begin{itemize}
		\item \textsf{case window $\Rightarrow$ a short period immediately before the event}
		\item \textsf{a matched length of period other than the case window is defined as} control window
		\end{itemize}
	\item \textsf{the selection of control window is based on the assumption of \textbf{conditional independence of the exposure} and \textbf{random distribution of the exposure over time}}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Strengths} & 
	\begin{itemize}
	\item \textsf{only cases are needed}
	\item \textsf{suitable for transient exposure where control is hard to find}
	\item \textsf{appropriate for environmental study where exposure is common and no concurrent control is available}
	\item \textsf{immune to selection bias}
	\item \textsf{effectively control for both \textbf{measured and unmeasured inter-individual confounding}}
	\item no loss to follow-up.  
	\end{itemize}
	\\ &\\ \hline
	\textsl{Limitations} & 
	\begin{itemize}
	\item \textsf{cannot control the \textbf{intra-individual confounding}} 
	\item \textsf{potential for recall bias since exposure is measured retrospectively}
	\item \textsf{potential for exposure misclassification since induction period is usually unknown}
	\item \textsf{susceptible to exposure trend bias, if the baseline exposures are associated with a longitudinal trend}
	\item \textsf{cannot account for cumulative effect of exposure}
	\end{itemize}
	\\ &\\ \hline
	\textsl{Practical issues} & 
	\begin{itemize}
	\item \textsf{analyze as match case-control study}
	\item \textsf{The \textbf{rate ratio is identical to risk ratio}}
	\item \textsf{to adjust for time trend of exposure $\Rightarrow$ use bidirectional control window or do case-time control study}
	\end{itemize}
	\\ &\\ \hline	
	
	\end{tabular}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Causal Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Concepts \& Definitions}
\label{chap:Concepts Definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Phiolosophical Viewpoints of Causality}
\subsection{Popperian Philosophy} knowledge accumulates only by falsification
\begin{itemize}
\item scientific hypotheses have empirical content and are falsifiable
\item testing of an hypothesis occurs by attempting to falsify them
\item hypotheses that have been tested and not falsified remain reasonably good explanations of natural phenomenon
\end{itemize}

\subsection{Hill Criteria} proposed set of criteria for determining causation in biomedical research
\begin{enumerate}
\item \textit{Strength}: strong associations are particularly compelling
\item \textit{Consistency}: consistent findings across studies
\item \textit{Specificity}: a cause leads to a single effect, \em or \em an effect has one cause (not multiple causes)
\item \textit{Temporality}: necessity that the cause precede the effect
\item \textit{Biologic gradient}: presence of dose response or exposure related curve, not always true
\item \textit{Plausibility}: scientific plausibility of an association
\item \textit{Coherence}: cause-and-effect interpretation for an association does not conflict with what is known
\item \textit{Experimental evidence}: deducing or eliminating an exposure and investigating if disease frequency declines
\item \textit{Analogy}: hypotheses derived from insight
\end{enumerate}

\begin{itemize}
\item none of the criteria can bring indisputable evidence for or against causality
\item however, temporality is clearly required to establish causation, but not sufficient
\end{itemize}

\subsection{Sufficient and Component Cause Model}

\subsection{Rothman's Model} a general model of causation to conceptualize epidemiologic problems, (causal 
pies)

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{causal_pies.png}
		\caption{Rothman's Causal Pies where $I$, $II$, and $III$ are sufficient causes, $A$, $B$, $C$, $D$, $E$, $F$, $G$, $H$, 
$I$, and $J$ are component causes, and $A$ is a necessary cause}
		\label{pies}
\end{figure}

	\begin{itemize}
	\item cause: an event or condition that plays an essential role in producing an outcome
		\begin{itemize}
		\item can be a constellation of components that act in concert
		\item \textbf{sufficient cause}: set of minimal conditions that \em inevitably \em produce disease
		\item \textbf{component cause}: an event or characteristic required by a given sufficient cause
		\item \textbf{necessary cause}: a component present in every sufficient cause
		\end{itemize}
	\item strength of cause: predictive ability of a cause is determined by the relative prevalence of component causes
		\begin{itemize}
		\item ex: a rare factor is a strong cause if its complementary component causes are common (ex: PKU and Diet Coke$^{\tiny 
\textregistered}$)
		\end{itemize}
	\item interactions among causes
		\begin{itemize}
		\item component causes in a single sufficient cause are considered to have mutual biologic interaction (ex: $D$ and $E$ in 
Figure ~\ref{pies})
		\item component causes present in multiple sufficient causes are considered to have super-additive effects (ex: $B$ and $C$ 
in Figure ~\ref{pies})
		\end{itemize}
	\item \textbf{induction period}: period of time from causal action of a component cause to disease initiation, varies by component cause (ex: 
final component cause has an induction period of 0, Figure ~\ref{cause_time})
	\item \textbf{latent period}: amount of time between disease occurrence (sufficient cause) and clinical detection
	\end{itemize}

\begin{figure}
	\centering
		\includegraphics[scale=0.64]{cause_timeline.png}
		\caption{Timeline for a sufficient cause.  $A$, $B$, $C$, and $D$ are component causes, each with its own induction time.}
		\label{cause_time}
\end{figure}

\section{Randomized Experiment Paradigm}
\begin{itemize}
\item often, randomized trials are considered as gold standard of all epidemiological study designs
\item the analysis of observational studies should be mimicking the analysis of randomized trial
\end{itemize}


\section{Counterfactual Definition of Causal Effect}
\subsection{Possible Worlds}
	\begin{itemize}
	\item actual world: the way things actually are, what we observe
	\item possible world: the way things might be, for example, a \em counterfactual world \em
 	   \begin{itemize} 
 	   \item closest possible world: minimally different from actual world, only difference is the intervention of interest took place	
	    \item $Y^{a_{i}}$: the outcome of the subject in the possible world that is closest to the actual world and where the subject 
was treated with $a$
 	   \item the data generating model is the same in all worlds, only a different treatment or treatments have taken place
 	   \end{itemize}
	\end{itemize}

\subsection{Null Hypothesis}
\begin{itemize}
\item null hypothesis of no average causal effect: $E[Y^{a=1}] = E[Y^{a=0}]$
\item \textbf{sharp causal null hypothesis}: $Y^{a_i=1} = Y^{a_i=0}$ for all subjects $i$.
	\begin{itemize}
	\item absence of average causal effect does not imply absence of individual causal effect, \em BUT \em absence of individual causal effect does imply absence of average causal effect
	\item the presence of individual causal effect implies the presence of average causal effect
	\end{itemize}
\end{itemize}

\subsection{Individual Causal Effect}
$$Y^{a_i=1} \neq Y^{a_i=0}$$
\begin{itemize}
\item with the exception of crossover studies, we can never calculate or estimate an individual causal effect because of the missing data problem (counterfactuals are not observed)
\item causal contrast: a comparison between two counterfactual outcomes for two clearly defined treatment levels
\item interference: when one individual's conuterfactual is influenced by other individuals' treatments
	\begin{itemize}
	\item usually make the assumption of no interference for causal inference in most situations (exception: herd immunity from vaccination)
	\end{itemize}
\end{itemize}

\subsection{Average Causal Effect}
$$ E[Y^{a=1}] \neq E[Y^{a=0}] $$
$$ Pr[Y^{a=1}=1] \neq Pr[Y^{a=0}=1]$$
\begin{itemize}
\item we can never \em calculate \em the true average causal effect because of the missing data problem (counterfactuals are not observed)
\item we can, however, \em estimate \em the true average causal effect when certain identifiability assumptions hold (more on this later)
\item information needed: outcome, actions, counterfactual outcomes
	\begin{itemize}
	\item outcome: need an observed outcome ($Y$)
	\item actions ($A$)
		\begin{itemize}
		\item treatment must be able to be manipulated
		\item a well defined contrast is needed (ex: 150 mg aspirin a day versus no aspirin)
		\end{itemize}
	\item a clearly defined study population whose counterfactual(s) ($Y^{a}$) are to be compared
	\end{itemize}
\end{itemize}



\section{Conditions for Causal Inference}
\label{sec:CCI}
observational study: a study in which the investigator does not assign treatment but simply observes treatments assigned by nature
\begin{itemize}
\item general paradigm
    \begin{itemize}
    \item make observational study look as much as possible like non-randomized experiment
        \begin{itemize}
        \item non-randomized experiment: treatment is deterministically assigned based on measured covariates $L$
        \item \em non-blinded \em conditionally randomized experiment
        \end{itemize}
    \item assume that conditional on measured covariates, treatment was randomly assigned
        \begin{itemize}
        \item requires some sort of covariate adjustment
        \end{itemize}
    \item for each causal question need to carefully describe:
    \begin{enumerate}
    \item the randomized experiment we would like to do
    \item the observational study that mimics the randomized experiment
    \item meet the identifiability conditions of \em exchangeability, positivity \em and \em consistency \em
    \item we need data and identifiability conditions to be met (or assumed) for an observational study
    \end{enumerate}
    \end{itemize}
\item information from observational study data is insufficient for causal inference, we need to make assumptions
\end{itemize}

\subsection{Identifiability}
        \begin{itemize}
        \item an average causal effect is \em non-parametrically \em identifiable when the distribution of the observed data is consistent 
with a \em single \em value of the effect measure
        \item can get only one effect estimate from the observed data at hand (e.g. for a given observed dataset, the OR estimate can be 4 
or 10 $\rightarrow$ non-identifiable; the OR estimate is 3 $\rightarrow$ identifiable.)
        \item from observational studies, it requires that the data is \em supplemented \em with three assumptions/conditions:
            \begin{itemize}
            \item exchangeability
            \item positivity
            \item consistency
            \end{itemize}
       \end{itemize}

\subsection{Exchangeability}
\begin{itemize}
\item exchangeability (exogeneity): $ Y^{a} \amalg A $, that is treatment $A$ is independent of the counterfactual outcome $Y^a$
\item allows estimation of all causal effect measures
\item necessary causal assumption
\item for observational studies
    \begin{itemize}
    \item the question is whether \em all \em joint predictors of $A$ and $Y$ were measured and included in $L$
    \item this is an assumption that cannot be empirically verified.
    \item it will require us to know both $Y^{a=1} \amalg A $ and $ Y^{a=0} \amalg A $ to verify this assumption, which is unverifiable
    \item we can only rely on expert knowledge
    \end {itemize}
\item no unmeasured confounding
\item d-separation: no unblocked backdoor pathway
\item \em caution: \em $Y^{A=a} \amalg A$ is different from $Y \amalg A$
    \begin{itemize}
     \item full exchangeability: $Y^{A=a} \amalg A$ for all $a$
        \begin{itemize}
        \item stronger version of exchangeability
        \item \em all \em possible counterfactual outcomes (continuous, ie: two or more) are independent of treatment
        \item usually only a result of successful randomization
        \item assumes that all the individual counterfactual outcomes  $ Y^{a_i} $ that are not observed are missing completely at random 
(MCAR)
        \end{itemize}
     \item partial exchangrability: $Y^{A=a^ \prime} \amalg A$ only for  $A=a^ \prime$ (not for all $A=a$)
        \begin{itemize}
        \item only assumes counterfactual outcome for treatment level $a^ \prime$ is independent of treatment
        \item only allows for estimation of SMR (indirect standardization)
        \item assumes that only the individual counterfactual outcomes  $ Y^{a ^\prime_i} $ that are not observed are missing completely 
at random (MCAR)
        \end{itemize}
     \item Some notes on \em full \em and \em partial \em exchangeability. \\
     Again, say that A and Y are both binary variables. \\ \\ 
     We may calculate the causal risk ratio for the entire population with \em full exchangeability \em. ($ Y^{A} \amalg A, \ i.e. \ Y^{a=1} 
\amalg A \ and \ Y^{a=0} 
\amalg A $) \\
\begin{align*}
			Causal \; risk \; ratio &  \\
			= & \frac{Pr[Y^{a=1}=1]}{Pr[Y^{a=0}=1]}     \\
			= & \frac{Pr[Y^{a=1}=1|A=1]}{Pr[Y^{a=0}=1|A=0]}  \ \mbox{(full exchangeability)}  \\
			= & \frac{Pr[Y=1|A=1]}{Pr[Y=1|A=0]}  \ \mbox{(consistency)} \\
\end{align*}

However, with \em partial exchangeability \em (say, only $ Y^{a=0} \amalg A $ but not $ Y^{a=1} \amalg A $), \\
we can only estimate the causal risk ratio in a subset of the population (exposed group, A=1). \\
The causal risk ratio among the exposed is also known as SMR. \\ 
\begin{align*}
SMR & \\
 = &  \frac{Pr[Y^{a=1}=1|A=1]}{Pr[Y^{a=0}=1|A=1]}     \\
= &  \frac{Pr[Y^{a=1}=1|A=1]}{\sum Pr[Y^{a=0}=1|A=1,L=l]Pr[L=l|A=1]} \ \mbox{(introduce L)} \\
= &  \frac{Pr[Y^{a=1}=1|A=1]}{\sum Pr[Y^{a=0}=1|A=0,L=l]Pr[L=l|A=1]} \ \mbox{(partial exchangeability)} \\
= &  \frac{Pr[Y=1|A=1]}{\sum Pr[Y=1|A=0,L=l]Pr[L=l|A=1]} \ \mbox{(consistency)}\\
\end{align*}

With $ Y^{a=1} \amalg A $ alone, we can only estimate causal effect in the unexposed group. 
     
     \item mean exchangeability: average counterfactual outcome (not each individual's) is exchangeable
        \begin{itemize}
        \item $E[Y^a] \amalg A$ 
        \item does not imply $Y^{a_i} \amalg A$ for all $i$
        \item $Y^{a_i} \amalg A$ for all $i$ does imply $E[Y^a] \amalg A$
        \end{itemize}
     \item \textbf{marginal exchangeability}: $ Y^{a} \amalg A $
     \item \textbf{conditional exchangeability}: $ Y^{a} \amalg A|L $
     \begin{itemize}
     \item assumes treatment groups are exchangeable within subsets of $ L $
     \item assumes that all the individual counterfactual outcomes  $ Y^{a_i} $ that are not observed are missing at random (MAR)
     \end{itemize}
     \item if we cannot achieve conditional exchangeability with our measured covariates, then the unobserved $ Y^{a_i} $ are missing not 
at random (MNAR) and 
depend on unmeasured covariates
     \end{itemize}
\end{itemize}

\subsection{Positivity}
\begin{center}
$Pr[A=a] > 0$ for all $a$ involved in the causal contrast \\
$Pr[A=a|L=l] > 0$ for all $l$ with $Pr[L=l] \neq 0$
\end{center}
	\begin{itemize}
	\item positivity: probability of being assigned to each treatment level is greater than zero (i.e. experimental treatment assumption)
	\item it is guaranteed in randomized experiment
	\item can be violated in an observational study
	\item is violated when there are either no treated or no untreated subjects in all levels of $L$ you are interested in (i.e. the levels of $L$ \em in your data \em)
	\end{itemize}
	
	\subsubsection{structural violation}
		\begin{itemize}
		\item the probability of some combination of $L$ and $A$ is guaranteed to be zero
		\item example: A study investigating the impact of combat exposure in the past year ($A$) on post-traumatic stress disorder ($Y$) that is stratified by age ($L$) will not have any 5 year-olds exposed to combat ($Pr[A=1|L=5]=0$) because structurally in the US there are laws prohibiting children from military service
		\item solution is to restrict study to all subsets with $Pr[A|L]>0$ (i.e. only include participants $>18$ years old)
		\item can usually determined by subject matter knowledge
		\end{itemize}
	
	\subsubsection{random violation}
		\begin{itemize}
		\item strata are empty by chance (i.e. MCAR) due to sampling variability
		\item is more likely as stratification becomes finer
		\item must be assumed
		\end{itemize}	
		
	\subsubsection{Identifying Positivity Violations}		
	\item difficult to distinguish between structural vs. random violations of positivity
	\item positivity is only necessary for 
		\begin{itemize}
		\item the set of variables $L$ required for exchangeability
		\item the levels of $L$ where $Pr[L=l]>0$
		\end{itemize}
	\item if $L$ is composite (more than 1 variable)
		\begin{itemize}
		\item consider the probability of $A$ in the joint set of covariate values
		\item for example, persons may be able to (1) change treatment (2) be treated if not observed, but (3) not be able to change treatment if not observed
		\item for IPW, positivity depends on how the weights are specified (i.e. what is in the conditioning event)
		\end{itemize}
	\item positivity can sometimes be verified
	\item more likely to be violated when estimating the effect of extreme interventions          
	\item required for standardization and IPW
		\begin{itemize}
		\item workaround: restrict to strata of $L$ where $Pr[A=a|L=l] > 0$ for all $a$
		\item[\-] potential loss of generalizability
		\item workaround: combine nearby strata, but could have 
		\item[\-] residual confounding
		\end{itemize}           
	\end{itemize}

\subsection{Consistency}
\begin{itemize}
\item for every subject with $A_{i}=a_{i},\ Y^{a}_{i}=Y^{A_{i}}=Y_{i}$
\item requires that each level of $A$ is clearly defined (could be assigned in an RCT)
\item requires that the investigator be able to describe the actual mechanism that is operating in the population to assign treatment
\item $Y^{a=1}$ and $Y^{a=0}$ are precisely defined 
\item taken for granted in RCT, but not so obviously true in observational studies
    \begin{itemize}
    \item ex: obesity can be a result of exercise, diet, genes
    \item each of these can have a different effect on the outcome (e.g. mortality)
    \end{itemize}
\item for an ill-defined treatment
    \begin{itemize}
    \item Solution 1:
        \begin{enumerate}    
        \item the investigator can describe the assignment mechanism as changing the \em distribution of determinants \em of the ill-defined 
``treatment" (e.g. obesity)
        \item but the proposed assignment mechanism does not match any hypothetical intervention of interest and thus has little public 
health utility
        \end{enumerate}
    \item Solution 2:
        \begin{enumerate}
        \item change causal question to a specific determinant of the ill-defined treatment (e.g. exercise vs. ``obesity")
        \item but the new question is further away from the original research question
        \end{enumerate}
     \item regardless of how well refined a causal question is, it still has an inherent amount of vagueness
    \end{itemize}
\item well-defined interventions
    \begin{itemize}
    \item uncertainty of conditional exchangeability is exacerbated by ill-defined interventions
    \item $L$ for well-defined interventions may be easier to identify than for ill-defined interventions
    \item positivity may not hold (ex: strong obesity gene resulting in no non-obese carriers)
    \item ill-defined interventions may be unreasonable or extreme
    \item effect estimates are only as well-defined as the interventions that are being compared
    \end{itemize}
\end{itemize}



\section{Effect Modification}
\subsection{Effect Modification: The Big Picture}
\begin{itemize}
\item refers to the change in the magnitude of an effect measure according to the value of a third variable, a \em modifier \em
\item a modifier of the average causal effect may only be \em associated \em with the outcome 
     \begin{itemize}
     \item may not have a causal effect or else would be an \em interaction \em
     \item effect modifier is associated with a third variable that has a causal effect on outcome and \em interacts \em with treatment
     \end{itemize}
\item an intrinsic, biological phenomenon and cannot be eliminated from a study by clever design
     \begin{itemize}
     \item differs from confounding because confounding is a bias the investigator tries to avoid, whereas effect modification is an 
elaborated description of the effect itself
	    \begin{itemize}
	    \item confounder: must be an independent risk factor for disease and associated with exposure
	    \item modifier: may be an independent risk factor for disease, but may also be an independent risk factor for disease in the 
presence of another risk factor
		\begin{itemize}
	      \item that is, a factor may have properties of a modifier, a confounder, both, or neither 
	      \end{itemize}
	    \end{itemize}
    	\item the focus is on the identification of effect modification and not its particular mechanism (we are not able to identify such 
mechanisms)
	\end{itemize}
    \item the choice to study effect modification depends on inferential goals and the research question of interest
        \begin{itemize}
        \item example 1: fluorination of public drinking water $\rightarrow$ not interested in effect modification
        \item example 2: effect of pharmacological treatment $\rightarrow$ interested in effect modification (especially for those for whom 
treatment is contraindicated) 
        \end{itemize}
    \item there is no such thing as \em the \em average causal effect because of heterogeneity
        \begin{itemize}
        \item the average causal effect in a population depends on the characteristics of the population under study 
        \item each population may have a different frequency of effect modifiers, limiting the external validity of a study which is 
restricted to a particular sub-population
        \item the estimated effect should be reported for each level $m$ of modifier $M$
        \end{itemize}
    \item causal hypotheses and effect modification
        \begin{itemize}
        \item the null hypothesis of no average causal effect in the entire population does \em not \em imply that there is no average 
causal effect for particular subgroups of the population
        \item the sharp causal null hypothesis \em does \em imply that there is no average causal effect for any subgroup of the population
        \end{itemize}
   \item effect modification and adjustment methods
        \begin{itemize}
        \item discrepant effect measures result from different causal questions asked by each investigator rather than their choice of 
analytic approach
        \item effect modification can be a major source of heterogeneity in effect estimates produced by IPW, standardization, 
stratification, and matching methods because they ask different questions
        \end{itemize}
   \item it is not always obvious whether effect measure heterogeneity across levels of $L$ is due to effect modification, may also be due 
to:
	\begin{itemize}
	\item selection bias
	\item information bias
	\item confounding
	\item chance
	\end{itemize}
   \end{itemize}

\subsection{Effect Modification: The Specifics}
   \begin{itemize}
    \item $M$ is a modifier of the effect of $A$ on $Y$ when the average causal effect of $A$ on $Y$ varies across levels of $M$
   \item modifier: pre-treatment variables $M$, in the absence of other biases (above)
	\begin{itemize}
   	\item mediator: cannot be considered as an effect modifier
	\end{itemize}
   \item the presence and extent of effect modification depends on the scale on which the effect is assessed
	\begin{itemize}
   	\item \textbf{additive effect modification}: effect modification on the additive (absolute) scale
   $$ E[Y^{a=1}|M=1]-E[Y^{a=0}|M=1] \neq E[Y^{a=1}|M=0]-E[Y^{a=0}|M=0]$$
   	\item \textbf{multiplicative effect modification}: effect modification on the multiplicative scale
   $$ \frac{E[Y^{a=1}|M=1]}{E[Y^{a=0}|M=1]} \neq \frac{E[Y^{a=1}|M=0]}{E[Y^{a=0}|M=0]} $$
   	\item \textbf{qualitative effect modification}: causal effects across $M$ are in opposite directions
        		\begin{itemize}
        		\item only for qualitative modification will we see effect modification on both the additive and multiplicative 
scales
        		\end{itemize}
	\item effect modification on one scale does not imply effect modification on the other scale
    	\item \em effect-measure modification \em: synonym used to stress dependence of effect modification on a particular scale 
(measurement)
	\end{itemize}
    \item \textbf{heterogeneity} or ``heterogeneity of causal effects across strata of $M$"
	\begin{itemize}
	\item test for homogeneity of difference measures
	$$  \chi^{2}_{m-1}= \displaystyle \sum^{M}_{i=1} \frac{(\hat{RD_i}-\hat{RD})^2}{\hat{Var_i}(\hat{RD_i})} $$
		\begin{itemize}
		\item where $ \hat{RD}$ is the summary estimate of the $RD$ assuming homogeneity and $\hat{RD_i}$ is the stratum specific 
estimate for level $m$
		\item $\hat{Var_i}(\hat{RD_i})=\frac{a_i(N_{1i}-a_i)}{N_{1i}^3}+\frac{b_i(N_{0i}-b_i)}{N_{0i}^3}$ for risk differences
		\item $\hat{Var_i}(\hat{RD_i})=\frac{a_i}{N_{1i}^2}+\frac{b_i}{N_{0i}^2}$ for rate differences
		\end{itemize}
	\item test for homogeneity of ratio measures
	$$ \chi^{2}_{m-1}= \displaystyle \sum^{M}_{i=1} \frac{(log \ \hat{RR_i}-log \ \hat{RR})^2}{\hat{Var_i}(log \ \hat{RR_i})} $$
		\begin{itemize}
		\item where $ \hat{RR}$ is the summary estimate of the odds/risk/rate ratio assuming homogeneity ($\hat{RR_{MH}}$) and $\hat{RR_i}$ is the stratum specific estimate for level $m$
		\item $\hat{Var_i}(\log \hat{RR}_i)=\frac{c}{a_iN_{1i}}+\frac{d}{b_iN{0i}}$ for risk ratios
		\item $\hat{Var_i}(\log \hat{RR}_i)=\frac{1}{a_i}+\frac{1}{b_i}$ for rate ratios
		\item $\hat{Var_i}(\log \hat{OR}_i)=\frac{1}{a_i}+\frac{1}{b_i}+\frac{1}{c_i}+\frac{1}{d_i}$ for odds ratios
		\end{itemize}
	\item considerably more data is needed to assess heterogeneity (effect modification), thus a failure to reject the null hypothesis 
of no heterogeneity may be explained by a lack of power
	\end{itemize}

    \item \textbf{stratification}: the causal effect of $A$ on $Y$ is calculated in each stratum of $M$
      \begin{itemize}
      \item for conditional risks
           \begin{description}
           \item[1)] stratify by $M$
           \item[2)] standardization or IPW by $L$
           \end{description}
   \item How to calculate causal effect in M=0 group
      \begin{enumerate}
      \item Standardization \\
      \mbox{for risk difference:}
      \begin{align*}
      \hat{RD}=& \displaystyle \sum_{l} E[Y^{a=1}|A=a,L=l,M=0] \times Pr[L=l|M=0]  - \\
      & \displaystyle \sum_{l} E[Y^{a=0}|A=a,L=l,M=0] \times Pr[L=l|M=0]  \\
      \end{align*} 
      \mbox{for risk ratio:}
	  \item[\-] using weights specific to $M$ only make sense if there are no other effect modifiers in $L$
      $$\hat{RR}=\frac{\sum_{l} E[Y^{a=1}|A=a,L=l,M=0] \times Pr[L=l|M=0]}
                {\sum_{l} E[Y^{a=0}|A=a,L=l,M=0] \times Pr[L=l|M=0]}$$
      \item IP weight
      $$\frac{1}{Pr[A=a|L=l,M=0]}$$
      \end{enumerate}
 \end{itemize}
   \item \textbf{generalizability}: external validity or transportability, only applicable when:
       \begin{itemize}
       \item the distribution of effect modifier $M$ is the same in both populations
       \item there are no effect modifiers
       \end{itemize}
   \item reasons investigators are interested in effect modification
       \begin{enumerate}
       \item if $M$ modifies the effect of $A$ on $Y$, then the average causal effect in the population may not be generalizable to the 
other populations in which $M$ is more or less frequent
       \item evaluating the presence of effect modification is helpful to identify the groups of subjects that would benefit most from an 
intervention
           \begin{itemize}
           \item additive effect modification important for public health interventions
           \end{itemize}
       \item identification of effect modification may help understand the biological, social, or other mechanisms leading to the outcome
       \end{enumerate}
   \end{itemize}

\subsection{Assessing Effect Modification}
\begin{itemize}
\item we only want to assess effect modification after we have controlled for any confounding or selection bias
	\begin{itemize}
	\item that is, the stratum-specific effect estimates should be unbiased before we proceed to consider effect modification
	\end{itemize}
\item the use of stratification as an adjustment method to evaluate effect modification $always \ gets \ E[Y^{A=a}|M=m]$
\item the use of IPW and Standardization as adjustment methods does not require the evaluation of effect modification because exchangeability and effect modification are handled separately $can \ get \ E[Y^{A=a}] and E[Y^{A=a}|M=m]$
\end{itemize}

\section{Interaction}
$$Pr[Y^{A=a,E=1}=1] \neq Pr[Y^{A=a,E=0}=1]$$

\subsection*{Definition}
	\begin{itemize}
	\item the causal effect of a $A$ on $Y$ after a joint intervention that set $E$ to 1 differs from the causal effect of $A$ on $Y$ after a joint intervention that set $E$ to 0
	\item interaction involves at least two treatments (joint interventions)
		\begin{itemize}
		\item identifying conditions (exchangeability, positivity, and consistency) required for \em all \em treatments 
		\item must be able to block all non-causal paths for all treatments
		\item interaction is assessed marginally
		\item rather than viewing $A$ and $E$ as two distinct treatments with levels (0 or 1), they can be viewed as a combined treatment $AE$ with levels (00, 01, 10, or 11)
		\end{itemize}
	\item if no conditional exchangeability for $E$, one can still assess effect modification, however, cannot assess interaction without \em identifiability \em assumptions
	\item defining interaction by heterogeneity or a departure from expected joint effects are mathematically equivalent
	\item \textit{synergism}: the presence of $A$ increases the effect of $E$ (vice versa)
	\item \textit{antagonism}: the presence of $A$ decreases the effect of $E$ (vice versa)	
	\end{itemize}

\subsubsection{Interaction on the Additive Scale}
	\begin{itemize}
	\item \underline{Defined by terms of Heterogeneity of Effects}
	
		\begin{align*}
		\underbrace{Pr[Y^{a=1,e=1}=1]-Pr[Y^{a=1,e=0}=1]}_\text{effect of $E$ when A set to 1}
		\neq 
		\underbrace{Pr[Y^{a=0,e=1}=1]-Pr[Y^{a=0,e=0}=1]}_\text{effect of $E$ when A set to 0}
		\end{align*}
		
	\item[\-] and when identifiability conditions hold\dots	
	
		\begin{align*}
		&\underbrace{Pr[Y=1|A=1,E=1]-Pr[Y=1|A=1,E=0]}_\text{effect of $E$ in the subset $A=1$} \\ 
		\neq \\
		&\underbrace{Pr[Y=1|A=0,E=1]-Pr[Y=1|A=0,E=0]}_\text{effect of $E$ in the subset $A=0$} 
		\end{align*}	
		
	\item \underline{Defined by Observed vs. Expected Joint Effects} (i.e. the compact way)	
	
		\begin{align*}
		&\underbrace{Pr[Y^{a=1,e=1}=1]-Pr[Y^{a=0,e=0}=1]}_\text{joint effect when $A$ and $E$ set to 1} \\
		\neq \\
		&\underbrace{(Pr[Y^{a=1,e=0}=1]-Pr[Y^{a=0,e=0}=1])}_\text{main effect of A, when $E$ set to 0}
		+
		\underbrace{(Pr[Y^{a=0,e=1}=1]-Pr[Y^{a=0,e=0}=1])}_\text{main effect of E, when $A$ set to 0}
		\end{align*}
		
	\item[\-] and when identifiability conditions hold\dots

		\begin{align*}
		&\underbrace{Pr[Y=1|A=1,E=1]-Pr[Y=1|A=0,E=0]}_\text{joint effect $A$ and $E$ in total} \\
		\neq \\
		&\underbrace{(Pr[Y=1|A=1,E=0]-Pr[Y=1|A=0,E=0])}_\text{main effect of A in subset $E=0$}  \\ 
		+  \\
		&\underbrace{(Pr[Y=1|A=0,E=1]-Pr[Y=1|A=0,E=0])}_\text{main effect of E in subset $A=0$}
		\end{align*}

	\item Definition of sub/super additive
		\begin{itemize}
		\item subadditive: $\neq$ can be replaced by $<$
		\item superadditive: $\neq$ can be replaced by $>$
		\end{itemize}
	\end{itemize}	
	
\subsubsection{Interaction on the multiplicative scale}
    
	\begin{itemize}
	\item \underline{Defined by Heterogeneity of effects}
	
		\begin{align*}
		&\underbrace{\dfrac{Pr[Y^{a=1,e=1}=1]}{Pr[Y^{a=1,e=0}=1]}}_\text{effect of $E$ when A set to 1}
		\neq
		\underbrace{\dfrac{Pr[Y^{a=0,e=1}=1]}{Pr[Y^{a=0,e=0}=1]}}_\text{effect of $E$ when A set to 0}
		\end{align*}
		
	\item[\-] when identifiability conditions hold\dots
		
		\begin{align*}
		&\underbrace{\dfrac{Pr[Y=1|A=1,E=1]}{Pr[Y=1|A=1,E=0]}}_\text{effect of $E$ in subset $A=1$}
		\neq 
		\underbrace{\dfrac{Pr[Y=1|A=0,E=1]}{Pr[Y=1|A=0,E=0]}}_\text{effect of $E$ in subset $A=0$} 
		\end{align*}	
	
	\item \underline{Defined by Observed vs. Expected Joint Effects}

		\begin{align*}
		&\bigg(  \underbrace{\dfrac{Pr[Y^{a=1,e=1}=1]}{Pr[Y^{a=0,e=0}=1]}}_\text{joint effect when $A$ and $E$ set to 1 } \bigg)\\ \\
		\neq
		&\bigg(  \underbrace{ \dfrac{Pr[Y^{a=1,e=0}=1]}{Pr[Y^{a=0,e=0}=1]}}_\text{main effect of A, when $E$ set to 0}  \bigg)
		+
		\bigg( \underbrace{ \dfrac{Pr[Y^{a=0,e=1}=1]}{Pr[Y^{a=0,e=0}=1]}}_\text{main effect of E, when $A$ set to 0} \bigg)
		\end{align*}
	
	\item[\-] when identifiability conditions hold\dots

		\begin{align*}
		&\bigg( \underbrace{\dfrac{Pr[Y=1|A=1,E=1]}{Pr[Y=1|A=0,E=0]}}_\text{joint effect when $A$ and $E$ in total} \bigg) \\ \\
		\neq
		&\bigg( \underbrace{\dfrac{Pr[Y=1|A=1,E=0]}{Pr[Y=1|A=0,E=0]}}_\text{main effect of A in subset $E=0$} \bigg)
		+
		\bigg( \underbrace{\dfrac{Pr[Y=1|A=0,E=1]}{Pr[Y=1|A=0,E=0]}}_\text{main effect of E in subset $A=0$} \bigg)
		\end{align*}		
	\item Definition of sub/super multiplicative
		\begin{itemize}	
		\item supermultiplicative: $\neq$ can be replaced by $>$
		\item submultiplicative: $\neq$ can be replaced by $<$
		\end{itemize}
	\end{itemize}

\subsubsection{interaction vs. effect modification}
    \begin{itemize}
    \item effect modification: effect of $A$ on $Y$ varies across levels of $M$, interested in $Y^a$
    \item interaction: joint effect of $A$ and $E$ on $Y$, interested in $Y^{a,m}$
    \item when treatment $E$ is randomly assigned, methods to identify effect modification can be used to identify interaction
    \item interaction between $A$ and $E$ without effect modification is possible, but rare (requires exact cancellations)
    \end{itemize}
	
\subsection{Counterfactual Response Types \& Sufficient Cause Model}

\subsubsection{Definitions}

	\begin{itemize}
	\item \textbf{Counterfactual response type}
		\begin{itemize}
		\item a particular counterfactual response under a specified treatment regime
		\item may be related to one or more sufficient causes
		\end{itemize}	
	\item \textbf{Sufficient Cause}	
		\begin{itemize}
		\item the minimal \em set \em of factors (called $U_{i}$) that cause the outcome
			\begin{itemize}
			\item a component is a necessary factor within the sufficient cause, together with the background factors it brings about the outcome
			\item background factors $U$ are not affected by treatment
			\end{itemize}
		\item treatment not necessarily a component in all sufficient causes for outcome
			\begin{itemize}
			\item for simplicity those that do not contain treatment are lumped into $U_0$
			\end{itemize}
		\item for a given outcome there are many sufficient causes
		\end{itemize}
	\end{itemize}	
	
\subsubsection{One binary treatment \& Dichotomous Outomce}

	\begin{enumerate}
	\item there are four counterfactual response types for $A$
	
		\begin{table}[h!]
		\caption{}
		\begin{tabular}{l c c r }
		\hline \hline
		\textbf{Type} & \textbf{$Y^{a=0}$} & \textbf{$Y^{a=1}$} & \textbf{Component Causes} \\
		\hline
		Dommed & 1 & 1 & $U_{0}$ or $\{U_{1} \& U_{2}\}$ \\
		Preventative & 1 & 0 & $U_{2}$ \\
		Causative & 0 & 1 & $U_{1}$ \\
		Immune & 0 & 0 & $none$ \\
		\hline \hline
		\end{tabular}
		\label{responsetype1}
		\end{table}
		
	\item there are three sufficient causes for $A$
	
		\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.4]{sufficientcauses1.png}
		\caption{\em causative, preventative, and doomed}
		\label{sufficientcauses1}
		\end{figure} 
		
	\end{enumerate}
		
\subsubsection{Two binary treatments \& Dichotomous Outcome}	
		
	\begin{enumerate}
	\item there are sixteen counterfactual response types for $A$ and $E$
	\item[\-] the presence of an \em additive \em interaction implies\dots
		\begin{itemize}
		\item individuals of type 1, 2, or 3 must exist
		\item i.e. counterfactual outcome for $A$ requires taking $E$ into account (vice versa)
		\end{itemize}
    \item[\-] the absence of an observed \em additive \em interaction implies \dots 
		\begin{itemize}
		\item all individuals are of type 4
		\item \em or \em type 1, 2, and 3 exist but their departures from additivity cancel each other out
	    \end{itemize}
	
		\begin{table}[h!]
		\caption{}
		\begin{tabular}{ l c c c c c r }
		\hline \hline
		\textbf{Type} & \textbf{Interaction} & \textbf{$Y^{a=1,e=1}$} & \textbf{$Y^{a=0,e=1}$} & \textbf{$Y^{a=1,e=0}$} & \textbf{$Y^{a=0,e=0}$} & \textbf
{Component Causes} \\
		\hline Type 1 & Yes & 1 & 0 & 0 & 0 & $U_{5}$ \\
		& & 0 & 1 & 0 & 0 & $U_{7}$ \\
		& & 0 & 0 & 1 & 0 & $U_{6}$ \\
		& & 0 & 0 & 0 & 1 & $U_{8}$ \\
		\hline Type 2 & Yes & 1 & 0 & 0 & 1 & $U_{5}, U_{8}$ \\
		& & 0 & 1 & 1 & 0 & $U_{6}, U_{7}$ \\
		\hline Type 3 & Yes & 1 & 1 & 1 & 0 & $U_{5}, U_{6}, U_{7}$ \\
		& & 1 & 0 & 1 & 1 & $U_{5}, U_{6}, U_{8}$ \\
		& & 1 & 1 & 0 & 1 & $U_{5}, U_{7}, U_{8}$ \\
		& & 0 & 1 & 1 & 1 & $U_{6}, U_{7}, U_{8}$ \\
		\hline Type 4 & No & 1 & 1 & 1 & 1 & $U_{0}$ or $\{U_{1}\ to\ U_{8}\}$ \\
		& & 0 & 0 & 0 & 0 & none \\
		& & 1 & 1 & 0 & 0 & $U_{3}, U_{5}, U_{7}$ \\
		& & 0 & 0 & 1 & 1 & $U_{4}, U_{6}, U_{8}$ \\
		& & 1 & 0 & 1 & 0 & $U_{1}, U_{5}, U_{6}$ \\
		& & 0 & 1 & 0 & 1 & $U_{2}, U_{7}, U_{8}$ \\
		\hline \hline
		\end{tabular}
		\label{responsetype2}
		\end{table}

	\item there are nine sufficient causes for $A$ and $E$	
		\begin{itemize}
		\item[\-] the last four correspond to an \em additive \em interaction
		
			\begin{figure}[h!]
			\caption{}
			\centering
			\includegraphics[scale=0.3]{sufficientcauses2.png}
			\label{sufficientcauses2}
			\end{figure}
		\end{itemize}	
	\end{enumerate} 

\subsection{Key points about Sufficient-Component Cause (SCC) model}
	\begin{enumerate}
	\item it is deterministic (recent extensions are making it stochastic)
	\item it is limited to dichotomous treatments and outcomes (recent extensions for categorical/ordinal data)
	\item the magnitude of the causal effect of treatment $A$ depends on the distribution (prevalence) of effect modifiers
		\begin{itemize}
		\item explains why an average causal effect depends on the characteristics of the population under study and may not be transportable
		\end{itemize}
	\item explains why it does not make sense to add attributable fractions for two separate treatments $A$ and $E$ for the joint treatment $\{A,E\}$
	  \begin{itemize}
	  \item not all 9 sufficient-component causes for a dichotomous outcome and two dichotomous treatments exist in all settings
	  \item persons can have more than one sufficient cause
		\begin{itemize}
		\item some persons may have sufficient causes involving only $A$, only $E$, or both $A$ and $E$
		\item such persons would be counted more than once if two independent attributable fractions are added.
		\item should calculate an ARF for a joint treatment by 
			\begin{enumerate}
			\item define exposure of interest, say ($A=1,E=1$) 
			\item define non-exposure as all other levels, i.e. (0-0, 1-0, 0-1)
			\item excess risk (or rate) accordingly
			\end{enumerate}
		\end{itemize}
	  \end{itemize}
	\end{enumerate}

\subsection{Sufficient Causes \& Exchangeability}

	\subsubsection*{Definitions}
		\begin{enumerate}
		\item a binary outcome $Y$ and binary treatment $A=1$ if treated, $A=0$ if untreated
		\item three sufficient causes as follows $U_{0}$ (doomed), $U_{1}$ (causative), and  $U_{2}$ (preventative)
		\item see figure~\ref{responsetype1}
		\end{enumerate}
		
	\subsubsection{Partial exchangeability}
		\begin{itemize}
		\item Only allows us to estimate effect in the subgroup of the treated or the untreated (depending on the type of partial exchangeability we have)
		\item $Y^{a=0} \amalg A$
			\begin{itemize}
			\item to simulate the counterfactual outcome under no treatment for those who were \em actually treated \em, we need to be able to assume that the risk of death \em under no treatment \em is the same in both groups.
			\item that is, the frequency of the combination of $U_{0}$ and $U_{2}$ (doomed and preventative persons) who would develop the outcome \em in the absence \em treatment, is the same in the persons who were treated and untreated
			\end{itemize}
		\item $Y^{a=1} \amalg A$
			\begin{itemize}
			\item to simulate the counterfactual outcome under receiving treatment for those who were \em actually untreated \em, we would need to be able to assume that the risk of death \em under receiving treatment \em is the same in both groups.
			\item that is, the frequency of the combination of $U_{0}$ and $U_{1}$ (doomed and causative persons) who would develop the outcome \em in the presence \em of treatment, is the same in persons who were treated and untreated
			\end{itemize}
		\end{itemize}
		
	\subsubsection{Full exchangeability}
		\begin{itemize}
		\item Allows us to estimate effect in entire population and in both subgroups of treatment levels
		\item $Y^{a} \amalg A \ \mbox{for all} \ a$ (continuous treatment)
		\item $Y^{a=0, a=1} \amalg A \rightarrow Y^{a} \amalg A \ \mbox{for all} \ a$ (dichotomous treatment)
		\item for a dichotomous treatment, we see that both cases of partial exchangeability must be assumed in order to assume full exchangeability.
		\end{itemize}


\subsection{Sufficient Cause Interaction}
    
	\subsubsection{Definition}
		\begin{itemize}
		\item sufficient cause interaction: interaction within the sufficient-component-cause framework
		\item defined as joint presence of treatments $A$ and $E$ in the same \em sufficient cause \em
		\item sufficient cause interaction can be \em synergism \em or \em antagonism \em
				\begin{description}
				\item[synergism] $A=1$ and $E=1$ present in the same sufficient cause
				\item[antagonism] ($A=1$ and $E=0$) or ($A=0$ and $E=1$) present in the same sufficient cause
				\end{description}
		\item makes explicit reference to the causal mechanisms involving the treatments $A$ and $E$
		\item however, sometimes it does not require knowledge of mechanism to identify sufficient cause interaction
		\item sufficient condition for synergism
				$$  Pr[Y^{a=1,e=1}=1] - (Pr[Y^{a=0,e=1}=1]+Pr[Y^{a=1,e=0}=1])>0$$
				\begin{itemize}
				\item this is a sufficient but not necessary condition of synergistic interaction
					\begin{itemize}
					\item implies observed joint effect is greater than expected joint effect 
					\item to see this divide all terms by $Pr[Y^{a=0,e=0}]$
					\end{itemize}
				\item fulfillment of above condition do \textbf{NOT} always imply synergism
				\item synergism \textbf{always} imply the fulfillment of above condition
				\item this is a very strong condition that misses most cases of synergism
				\item conceptually, this condition implies 
				  $$\left\{U_{5}=1\right\} - \left\{U_{0}=1\right\} - \left\{U_{2}=1\right\} - 
					\left\{U_{4}=1\right\} - \left\{U_{6}=1\right\} - \left\{U_{7}=1\right\} > 0$$ 
				\end{itemize}
		\end{itemize}		
			
	\subsubsection{Monotonicity}
		\begin{itemize}
		\item is a weaker sufficient condition of synergism
        \item refers to monotonic effect of A and E 
			\begin{itemize}
			\item no preventive type persons for the \em joint \em treatment $\{A,E\}$ in the population
			\item the absence of one component $A$ does not increase E[Y] compared to the absence of $A$, holding the other component constant $E$
			\end{itemize}
        \item sufficient condition for synergism with monotonicity assumption
        $$ Pr[Y^{a=1,e=1}=1] - (Pr[Y^{a=0,e=1}=1] > Pr[Y^{a=1,e=0}=1] - Pr[Y^{a=0,e=0}=1])$$
        \item conceptually, this condition implies $$\left\{U_{5}=1\right\} > 0$$ 
        \end{itemize}
		
		\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.4]{sufficientcauses3.pdf}
		\caption{With monotonicity assumption, some sufficient causes are guaranteed not to exist}
		\label{sufficientcauses3}
		\end{figure}
	
	\subsubsection{Interpretation}
		\begin{itemize}
		\item counterfactual framework and sufficient cause framework answer different questions
			\begin{enumerate}
			\item counterfactual: the \underline{outcome} that would have occurred under a particular action
				\begin{itemize}
				\item what happens?
				\item a way to study causes and effects
				\end{itemize}
			\item sufficient cause: the \underline{mechanism} that caused the outcome
				\begin{itemize}
				\item how does it happen?
				\item a way to study the effects of causes
				\end{itemize}
			\end{enumerate}	
		\item biologic interaction
			\begin{itemize}
			\item although sufficient cause interaction is often referred to as biologic interaction, sufficient cause interaction does not imply that A and E physically interact with each other
			\end{itemize}
		\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Bias}
\label{chap:Bias}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Directed Acyclic Graphs: concepts}

\subsection{Definition}
	\begin{itemize}
	\item a DAG is a tool to...
		\begin{itemize}
		\item represent our qualitative \em expert knowledge \em and a priori \em assumptions \em about the causal structure of interest
		\item classify bias
		\item identify potential problems in study design and analysis
		\end{itemize}
	\item communicate our assumptions (parsimony is desirable)	
	\item definitions of \textbf{directed acyclic graph} (DAG)
		\begin{itemize}
		\item \textbf{node}: $A, L, Y, U$, etc.
		\item \textbf{edge}: $\rightarrow$
		\item properties
			\begin{itemize}
			\item \textit{Directed}: No bi-directional arrows
			\item \textit{Acyclic}: No loops
			\item \textit{Graph}: it's a visual representation of statistical (or causal) associations 
			\item temporally ordered DAG: time flows from left to right
			\end{itemize}
		\item in a causal DAG or its statistical equivalent (later):
			\begin{itemize}
			\item no arrow from $X$ to $D$ means:
				\begin{enumerate}
				\item $X$ has NO causal effect on $D$ for \em any individual \em in the population
				\item we \em assume \em that $X$ has NO causal effect on $D$ for \em any individual \em in the population
				\end{enumerate}
			\item arrow from $A$ to $Y$ means:
				\begin{enumerate}
				\item $A$ has causal effect on $Y$ for \em at least one \em individual in the population
				\item We do \em NOT \em want to assume that $A$ does not have a causal effect on any individual in the population
				\end{enumerate}
			\item $A \rightarrow L \leftarrow Y$: L is a \textbf{collider}
			\end{itemize}
		\end{itemize}	
	\item formal definition
		\begin{enumerate}
		\item nodes: random variables $V=(V_{1}, ...,V_{M})$
		\item $PA_{m} \ \mbox{are the \textbf{parents} of} \ V_{m}$: the \em set \em of nodes with arrows into $V_{m}$
		\item $V_{j}$ is a \textbf{descendant} of $V_{m}$ if there is a sequence of nodes connected by edges between $V_{j} \ and\ V_{m}$
			\begin{itemize}
			\item $V_{j}$ need not be a direct descendant (child)
			\end{itemize}
		\end{enumerate}
	\end{itemize}   

\subsubsection*{Conditioning}
	\begin{itemize}
	\item conditioning is calculating the effect measure \em within \em levels of the confounder (i.e. using only the subsets' data) to give 
	stratum-specific effect measures (i.e. stratification or restriction)
	\item \textbf{conditional independence} implies that we can remove an association between two variables by stratifying on a third variable
		\begin{itemize}
		\item we generally stratify on a variable $L$ that lies along a path between treatment $A$ and outcome $Y$ to remove sources of association that are non-causal
		\item how the stratification affects $A$ and $Y$'s association depends on what kind variable $L$ is
		\end{itemize}
	\item let's assume treatment $A$, outcome $Y$, and covariate $B$, $L$, and/or $U$ (unmeasured)
	\item stratifying on a third variable:
		
		\begin{enumerate}
		\item Path contains only \textbf{intermediate variable} (\textit{see figure ~\ref{figure_CI_6_5}})
			\begin{enumerate}
			\item prior to stratification $A$ and $Y$ are marginally \em associated \em
			\item stratifying on $B$ removes the association between $A$ and $Y$
			\item thus, $A$ and $Y$ are conditionally independent given $B$
			\item usually we prefer to not stratify on intermediates because:
				\begin{enumerate}
				\item it removes the indirect effect of $A$ on $Y$
				\item there may be an unmeasured variable $U$ that is a common cause of $B$ and $Y$
				\end{enumerate}
			\end{enumerate}
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.5]{figure_CI_6_5.png}
			\caption{Conditioning on an Intermediate}
			\label{figure_CI_6_5}
			\end{figure}

		\item Path contains a \textbf{common cause} (\textit{see figure ~\ref{figure_CI_6_6}})
			\begin{enumerate}
			\item prior to stratification $A$ and $Y$ are marginally \em associated \em
			\item stratifying on $L$ removes the association between $A$ and $Y$
			\item thus, $A$ and $Y$ are conditionally independent given $L$
			\end{enumerate}
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.5]{figure_CI_6_6.png}
			\caption{Conditioning on a Common Cause}
			\label{figure_CI_6_6}
			\end{figure}

		

		\item Path contains a \textbf{common effect} (i.e. collider, \textit{see figure ~\ref{figure_CI_6_7}})
			\begin{enumerate}
			\item prior to stratification $A$ and $Y$ are marginally \em independent \em
			\item stratifying on $L$ creates an association between $A$ and $Y$
			\item thus, $A$ and $Y$ are \em not \em conditionally independent given $L$ when it is a common effect
			\end{enumerate}

\item[\-] when a variable is the common effect of two other variables, those two variables are guaranteed to provide information about each other within levels of the common effect (Sprinkler-Rain-Grass-is-wet analogy)
		
			\begin{enumerate}
			\item the sprinkler is on an automatic timer
			\item both the sprinkler and rain cause the grass to be wet
			\item if the grass is wet and I tell you the sprinkler was off, do you know that it rained?
			\end{enumerate}
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.5]{figure_CI_6_7.png}
			\caption{Conditioning on a Common Effect}
			\label{figure_CI_6_7}
			\end{figure}

		\item Conditioning on a variable's \textbf{descendant}
		\begin{itemize}
		\item Let's say that $A$ has a direct cause on $B$ so that $A \rightarrow B$
		\item to the degree that $A$ and $B$ are associated, stratifying on $A$'s descendant $B$ will cause stratification on $A$ also. Two implications:
			\begin{description}
			\item[descendant of a common cause] (\textit{see figure ~\ref{figure_CI_7_5_and_9_8_mod}})
				\begin{itemize}
				\item cannot measure $U$ directly but can measure its descendant $L$
				\item adjusting $L$ will control for confounding to the degree that $L$ is associated with $U$
				\item residual confounding occurs when the $L$ is not highly correlated with $U$
				\item the same concept applies to \em measurement error \em ($L$ = truth, $L^*$ = measurement)
				\end{itemize}
			
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.25]{figure_CI_7_5_and_9_8_mod.png}
			\caption{Conditioning on the Descendant of a Common Cause (Left: Surrogate Confounder; Right: Measurement Error)}
			\label{figure_CI_7_5_and_9_8_mod}
			\end{figure}
			
		\item[descendant of a common effect] (\textit{see figure ~\ref{figure_CI_6_8}})
			\begin{itemize}
			\item same effect as stratifying on the parent, which is a collider
			\item the path that the parent formerly blocked is now open
			\item in figure ~\ref{figure_CI_6_8}, $A$ and $Y$ are associated because $C$ is stratified upon
			\item this situation happens often in the study of direct effects, joint effects, and 
		the effects of time-varying treatments.
			\end{itemize}
		\end{description}
		
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.5]{figure_CI_6_8.png}
			\caption{Conditioning on the Descendant of a Common Effect}
			\label{figure_CI_6_8}
			
			\end{figure}
		\end{itemize}
	\end{enumerate}
\end{itemize}
\subsection{Statistical assumptions in a DAG}

\subsubsection*{Markov Factorization}
	\begin{itemize}
	\item a DAG describes conditional associations between the included variables
	\item for a DAG, we can write the joint distribution as a Markov factorization
 $$\displaystyle f(v) = \prod^{M}_{j=1} f(v_{j}|pa_{j})$$
		\begin{itemize}
		\item the joint distribution captures the presence or lack of arrows (associations/effects)
		\end{itemize}
	\item A complete DAG: each variable is a descendant of all variables preceding it
	
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_complete.png}
			\caption{a Complete DAG}
		\label{dag_complete}
		\end{figure}

		\begin{align*}
		f(V_3,V_2,V_1)=\underbrace{f(V_3|V_2,V_1) \times f(V_2|V_1) \times f(V_1)}_{Markov factorization}
		\end{align*}
		
	\item with assumptions we remove arrows (the independence is reflected in the joint distribution)
		
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_incomplete.png}
			\caption{Incomplete DAG}
		\label{dag_incomplete}
		\end{figure}
		
		\begin{align*}
		f(V_3,V_2,V_1)=f(V_3|V_1) \times f(V_2|V_1) \times f(V_1)
		\end{align*}
	
	\item comparing the complete and incomplete DAGs we find that
		\begin{itemize}
		\item $f(V_3|V_2,V_1) = f(V_3|V_1) \rightarrow V_3 \amalg V_2|V_1$
		\item $f(V_2|V_1) = f(V_2|V_1) \rightarrow V_2 \not\amalg V_1$
		\end{itemize}
	
	\item we can marginalize over a variable if the conditional associations of other variables are preserved
		\begin{itemize}
		\item consider the two DAGs:
	
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_margin.png}
			\caption{marginalize over $V_3$}
		\label{dag_incomplete}
		\end{figure}
		
		\item these two dags are statistically equivalent
		\end{itemize}	
	\end{itemize}

\subsection{Causal DAGs}

\subsubsection*{Intuitive Examples}
    \begin{itemize}
    \item figure ~\ref{dags_a}: 
        \begin{enumerate}
        \item conditionally randomized experiment
        \item observational study with the assumption that A depends on L but there are \em no other causes of Y\em 
        \end{enumerate}
    \item figure ~\ref{dags_b}: 
        \begin{enumerate}
        \item marginally randomized experiment 
        \item $\underbrace{Pr[Y^{a=1}=1] \neq Pr[Y^{a=0}=1]}_{causation}$ and $\underbrace{Pr[Y=1|A=1] \neq Pr[Y=1|A=0]}_{association}$ 
        \end{enumerate}
    \item figure ~\ref{dags_c}:
        \begin{enumerate}
        \item confounding by L
        \item $\underbrace{Pr[Y^{a=1}=1] = Pr[Y^{a=0}=1]}_{no \ causation}$ \textbf{BUT} $\underbrace{Pr[Y=1|A=1] \neq Pr[Y=1|A=0]}_{association}$
        \end{enumerate}
    \item figure ~\ref{dags_d}:
        \begin{enumerate}
        \item L is a collider of A and Y
        \item $\underbrace{Pr[Y^{a=1}=1] = Pr[Y^{a=0}=1]}_{no \ causation}$ and $\underbrace{Pr[Y=1|A=1] = Pr[Y=1|A=0]}_{no \ association}$
        \item \em remember that \textbf{being a collider is path-specific!} \em
        \end{enumerate}
    \end{itemize}

		\begin{figure}[h!]
		\centering
			\subfigure[]{
				\includegraphics[scale=.5]{dags_a.png} 
				\label{dags_a}
			} \hspace{1 cm}
			\subfigure[]{
				\includegraphics[scale=.5]{dags_b.png} 
				\label{dags_b}
			} \hspace{1 cm}
			
			\subfigure[]{
				\includegraphics[scale=.5]{dags_c.png}
				\label{dags_c} 
			} 
			\subfigure[]{
				\includegraphics[scale=.5]{dags_d.png} 
				\label{dags_d}
			} \hspace{1 cm}
		\caption{The simplest cases}
		\label{dags}	
		\end{figure}	

\subsubsection*{Formal Definition}
    \begin{itemize}
    \item A DAG is a \em causal \em DAG if...
        \begin{enumerate}
        \item lack of arrow means ``no direct causal effect" with respect to other variables in the DAG
        \item \em all \em common causes are on the DAG (even unmeasured)
        \end{enumerate}
	\end{itemize}
	
\subsubsection*{Statistical vs. Causal DAG}
	\begin{itemize}
	\item a statistical DAG represents associations because it does not have all common causes of treatment and outcome on the graph
	\item a causal DAG can be translated to a statistical DAG if all the implied conditional associations are preserved
	\item in a statistical DAG the following all represent association:
		$$V_1 \rightarrow V_2 \rightarrow V_3 \qquad V_1 \leftarrow V_2 \rightarrow V_3 \qquad V_3 \rightarrow V_2 \rightarrow V_1$$
	\item we use a statistical DAG when we have unmeasured confounders in the causal DAG
	\item[\-] Examples
		\begin{enumerate}
		\item For both DAGs $Y \not \amalg A$ and $Y \amalg A|L$ \textit{see figure ~\ref{dag_a}}
			\begin{figure}[h!]
			\centering
				\subfigure[Causal]{
				\includegraphics[scale=.5]{dag_b_caus.PNG}
				\label{dag_b_caus}
				} 
				\hspace{1 cm}
				\subfigure[Statistical]{
				\label{dag_b_stat}
				\includegraphics[scale=.5]{dag_b_stat.PNG}
				}				 
			\caption{$A$ randomized but $L$ observed 
				\newline (a) $f(Y|A,L,U) \times f(A|L,U) \times f(L|U) \times f(U)$ 
				\newline (b) $f(Y|A,L) \times f(A|L) \times f(L)$
				}
			\label{dag_a}	
			\end{figure}	
			
			\begin{itemize}
			\item for ~\ref{dag_b_caus}: 
			\item for ~\ref{dag_b_stat}: 
			\end{itemize}
			
		\item For both DAGs $Y \amalg A$ and $Y \not \amalg A|L$ \textit{see figure ~\ref{dag_d}}
			\begin{figure}[h!]
			\centering
			\subfigure[Causal]{
				\label{dag_b_caus}
				\includegraphics[scale=.5]{dag_d_caus.PNG} 
				}
			 \hspace{1 cm}
			\subfigure[Statistical]{			
				\label{dag_d_stat} 
				\includegraphics[scale=.5]{dag_d_stat.PNG}
				} 	
			\caption{ $A$ randomized but $L$ observed  
				\newline (a) $f(L|A,U) \times f(Y|U) \times f(A)$ 
				\newline (b) $f(L|Y,A) \times f(Y) \times f(A)$
				}
			\label{dag_d}
			\end{figure}	
			
		\item Cannot get rid of U \textit{see figure ~\ref{dag_c}}
		
			\begin{figure}[h!]
			\centering
				\includegraphics[scale=0.5]{dag_c_caus.PNG}
				\caption{Observational Study}
			\label{dag_c}
			\end{figure}
		\end{enumerate}

			\begin{itemize}
			\item because $U$ is a direct cause of $A$
			\end{itemize}

		
	\end{itemize}	
			
\subsubsection*{Causal Markov Assumption}
	\begin{itemize}
	\item need to make \em assumptions \em to \textbf{link the causal DAG to the observed data in a study}
    \item causal Markov assumption: $\displaystyle f(v) = \prod^{M}_{j=1} f(v_{j}|pa_{j})$
	\item we assume this holds true for our causal DAG
	\end{itemize}

\subsubsection*{Non parametric structural equation model (NPSEM)}
    \begin{enumerate}
    \item a variable is a function of it's parents and a random error term
		\begin{itemize}
        \item deterministic unknown function $f_{m}=(pa_{m},\epsilon_{m})$
			\begin{itemize}
			\item independent random errors $\epsilon_{v_{m}}$ equivalent to ``no unmeasured confounders" 
			\end{itemize}
		\end{itemize}	
    \item a DAG representing a NPSEM is a \em causal \em DAG, which satisfies the causal Markov assumption (implied by mutually independent $\epsilon$)
    \item a NPSEM is a causal DAG, \em but \em not all causal DAGs are NPSEMs
    \item a NPSEM is a \em fully randomized causally interpreted structured tree graph \em (the IPW tree)
    \end{enumerate}

\subsection{Faithfulness}
\subsubsection*{Definition}
	\begin{itemize}
	\item Faithfulness occurs when the associations implied by the $causal$ DAG's structure exist in the data
	\item the distribution of the data is \em not \em faithful to the causal DAG when the joint distribution of the data does not reflect associations implied by the causal DAG.
	\end{itemize}

\subsubsection*{Sources of Faithfulness Violations}
	\begin{itemize}
	\item the average causal null holds but the sharp causal null does not
		\begin{itemize}
		\item ex: equal and opposite cancellation
		\end{itemize}
	\item rarely occurs by chance, but can be guaranteed for some study designs (matching)
	\end{itemize}

\subsubsection*{Example: Matched Prospective Cohort Study}	

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figure_CI_6_9.png}
\caption{Matched Prospective Cohort Study}
\label{figure_CI_6_9}
\end{figure}

\begin{itemize}
\item $S$ represents a matching factor that is determined by treatment and some confounding variable you wish to control for
	\begin{itemize} 
	\item $S$ is a common cause because being selected depends on where the subject lies in the joint distribution of treatment and confounder
	\item there may not be suitable matches for someone who has that particular level of confounder \em and \em has the opposite treatment history
	\end{itemize}
\item counter to the \em causal \em DAG, the data shows no association between $L$ and $A$ in the matched study population
\item $L \rightarrow A$ association is equal in magnitude and opposite in sign to the $L \rightarrow  _{\textSFii}S ^{\textSFiii} \leftarrow 
A$ association
\item matching has the inverse magnitude of the propensity score
\item there is no need to adjust for $L$ to eliminate bias (in this particular example), however one might adjust for $L$ for gains in 
statistical efficiency
\end{itemize}
	
\subsection{Intervention vs. Stratification}

		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_original.png}
			\caption{original DAG}
		\label{dag_original}
		\end{figure}

	\begin{itemize}
	\item g-methods remove arrows by simulating counterfactual worlds where we ``intervene" and set treatment to a particular value
		\begin{itemize}
		\item in each world, everybody receives the same level of treatment (thus treatment is constant)
		\item in a DAG we express this by removing all arrows flowing into the ``set" variable and making it lowercase
		\item the causal effect contrasts the entire population under different treatment assignments
		\end{itemize}

		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_intervene.png}
			\caption{$V_1$ and $V_3$ set to $v_1$ and $v_3$}
		\label{dag_intervene}
		\end{figure}
		
	\item stratification constricts the analysis to occur within strata
		\begin{itemize}
		\item in a DAG we express this by placing boxes around all stratification factors
		\item the causal effect contrasts two subgroups that happen to
		\item have the same values for stratification factors
		\item differ only in treatment status
		\end{itemize}
		
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{dag_stratify.png}
			\caption{$V_1$ and $V_3$ stratified upon}
		\label{dag_stratify}
		\end{figure}
		
	\item when all stratification covariates are set, the causal interpretation of g-methods and stratification are equivalent
	\end{itemize}
			
\subsection{Exchangeability \& D-separation}

\subsubsection*{Exchangeability}
\begin{itemize}
\item for basics on exchangeability, see Chapter ~\ref{chap:Concepts Definitions}.~\ref{sec:CCI}
\item what we want to know is if the counterfactual outcome $Y^a$ is independent of treatment assignment $A=a$ with respect to other covariates, that is are the treatment groups comparable
	\begin{itemize}
	\item exchangeability can be a result of a random natural distribution or the result of randomization ($Y^a\amalg A$)
	\item exchangeability can be conditional on a variable $L$ ($Y^a\amalg A|L$)
	\end{itemize}
\item DAGS can be useful tools to assess exchangeability:
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.65]{exchangeability_0.png}
			\caption{DAG integrating the counterfactual world  $A=a$}
		\label{fig:dag_exch_0}
		\end{figure}

	\begin{itemize}
	\item the DAG in figure~\ref{fig:dag_exch_0} gives an example of how an unmeasured variable $U$ can cause a lack of exchangeability (association) between treatment $A$ and counterfactual outcome $Y^a$
	\item a simple rule of thumb is to think of $Y^a$ as a pre-treatment variable (part of U) and assess if there is an unblocked backdoor pathway between $A$ and $U$
	\item exchangeability can always be created by conditioning on the observed past (Markov factorization) of $A$ (figure~\ref{fig:dag_exch_1})

		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.65]{exchangeability_1.png}
			\caption{DAG showing how conditioning on the observed past ($L$) can create exchangeability ($Y^a\amalg A|L$)}
		\label{fig:dag_exch_1}
		\end{figure}
	\end{itemize}
\end{itemize}



\subsubsection*{D-seperation}($\amalg_{d-sep}$)
	\begin{itemize}
	\item a set of graphical rules to decide if two variables in a DAG are independent (d-seperated) or not independent (d-connected)
	\item ``d-" stands for directional
	\item a path is any set of nodes (variables) connected by edges
	\item a path is blocked or unblocked depending on the following rules:
		\begin{enumerate}
		\item A path is blocked it two arrowheads on a path collide
			\begin{itemize}
			\item $ L \rightarrow A \rightarrow Y$ is unblocked
			\item $A \rightarrow Y \leftarrow L$ is blocked (collider)
			\end{itemize}
		\item A path that contains a non-collider that is conditioned on is blocked
		\item A collider that has been conditioned on does not block a path
		\item A collider that has decedents that have been conditioned on does not block a path
		\item \textbf{being a collider is path-specific!}
		\end{enumerate}
	\item \em unconditional d-separation\em: $A$ is d-separated from $B$ in DAG $G$ if and only if all paths between them are blocked, $(A \amalg_{d-sep} B)_G$
	\item \em conditional d-separation\em: $A$ is d-separated from $B$ in DAG $G$ given $L$ if and only if all paths between them are blocked when conditioning on $L$, $(A \amalg_{d-sep} B|L)_G$
	\item Conclusions:
		\begin{itemize}
		\item a path is blocked if it contains a non-collider that has been conditioned on, or contains a collider that has not been conditioned on and that collider has no descendants that have been conditioned on
		\item causes (ancestors) are not independent of their effects (descendants) and vice versa
		\item generally, two variables are associated if they share a common cause, but sharing a common effect does not always imply the two causes are associated 
		\end{itemize}
	\end{itemize}

\subsubsection{Back-door Criterion \& Sequential Back-Door Criterion}
	\begin{itemize}
	\item \textit{Point Exposures}
		\begin{itemize}
		\item $(Y \amalg_{d-sep} A|L)_G \Longrightarrow Y^a \amalg A|L$
		\item achieving d-separation with measured confounders $L$ implies that, given $L$, we have conditional exchangeability for treatment $A$
		\end{itemize}		
	\item \textit{Joint Exposures}
		\begin{itemize}
		\item a stronger version, the \em Sequential Back-Door Criterion\em, is required for time-varying exposures, treatment regimes, etc.
		\item $(Y\amalg_{d-sep} A_{k}| \bar{L}_{k}, \bar{A}_{k-1})_G \Longrightarrow Y^a \amalg A_{k}| \bar{L}_{k}, \bar{A}_{k-1}$
		\item implication: to estimate causal effect of a joint treatment
			\begin{enumerate}
			\item need to fulfill backdoor criterion (i.e. conditional exchangeability) for \em each component \em of treatment $A$ (i.e $A_i$)
			\item required for all functions of a joint exposure (e.g. cumulative, average, etc.)
			\end{enumerate}	
		\end{itemize}
	\item recall that we fulfill the back-door criterion by testing for conditional independence between $Y$ and $A$ given a set of variables $S$
		\begin{itemize}
		\item for point exposures, $S$ is the set of variables in $L$
		\item for joint exposures, $S$ is the set of variables in $\bar{A}_{k-1}$ and $\bar{L}_{k}$
		\item[\-] where $\bar{A}_k$ is history of treatment up to time $k \Longrightarrow \{A_0,A_1,\dotsc,A_k\}$ 
		\item[\-] where $\bar{L}_k$ is covariate history up to time $k \Longrightarrow \{L_0,L_1,\dotsc,L_k\}$ 
		\end{itemize}
	\item fulfilling the back-door criterion can be achieved by removing or blocking all back-door pathways\dots
		\begin{itemize}
		\item in the study design (restriction, matching)
		\item in the analysis (stratification, g-methods) 
		\end{itemize}
	\end{itemize}

\subsection{Graphs, Counterfactuals, and Interventions}
	\begin{itemize}
	\item causal diagrams encode expert knowledge and assumptions about the causal structure of the problem			
	\item emphasizes not all variables are created equal
		\begin{itemize}
		\item requirements on treatment variables $A$ that do not apply to other variables like $L$ (exchangeability, positivity, and consistency)
		\item decision nodes (representing the potential interventions) are not included for simplicity and to avoid redundancy
		\end{itemize}
	\end{itemize}

\subsubsection*{Identifying Assumptions Represented in Causal DAGs}
	\begin{itemize}
	\item \textit{Nodes}
		\begin{enumerate}
		\item \textsl{treatment node}: a node $A$ for which we want $Y^{A=a^{\prime}}$ vs. $Y^{A=a^{\prime \prime}}$
			\begin{itemize}
			\item must fulfill positivity, consistency, and exchangeability
			\item enclosed in circles in causal tree graphs (i.e. see IPW figure)
			\end{itemize}
		\item \textsl{non-treatment node}: does \em not \em have to fulfill positivity and consistency 
		\end{enumerate}
	\item \textit{Positivity}
		\begin{itemize}
		\item arrows from the nodes $L$ to the treatment node $A$ (i.e. $L \rightarrow A$) are \em not \em deterministic. When this is fulfilled there is variation in $A$ for each level of $L$
		\item only concerns arrows arriving \em into \em treatment nodes ($\rightarrow A$)
		\end{itemize}
	\item \textit{Consistency}
		\begin{itemize}
		\item the effect of treatment, represented by arrows from treatment node $A$ to outcome $Y$ (i.e. $A \rightarrow Y$) is well defined
		\item only concerns arrows departing \em from \em treatment nodes (A $\rightarrow$)
		\end{itemize}
	\item \textit{Exchangeability}
		\begin{itemize}
		\item lack of paths between $A$ and $Y$ nodes, other than those originating between $A$, that would result in an association between 
	$A$ and $Y$
		\item i.e. back-door criterion fulfilled
		\end{itemize}
	\end{itemize}

\subsection{Structure of Effect Modification}
	\begin{itemize}
	\item causal DAGs are helpful in illustrating effect modification
	\item $M \rightarrow Y \leftarrow A$
		\begin{itemize}
		\item $M$ is the effect modifier
		\item $Y$ is the outcome
		\item $A$ is the treatment
		\item example: how quality of care $M$ can modify the effect of heart transplant $A$ on survival $Y$
		\end{itemize}
	\item two important caveats:
		\begin{enumerate}
		\item causal DAG would still be valid if it did not include $M$
			\begin{itemize}
			\item because the causal question makes reference to $M$, $M$ is included
			\end{itemize}
		\item the causal diagram does not necessarily indicate the presence of effect modification by $M$ and cannot distinguish between:
			\begin{enumerate}
			\item the causal effect in stratum $M=1$ is in the same direction as $M=0$
			\item the causal effect in stratum $M=1$ is in the opposite direction as $M=0$
			\item treatment $A$ has a causal effect in one stratum of $M$ but not in another
			\end{enumerate}
		\end{enumerate}
	\item many effect modifiers do not have an effect on the outcome, but must be associated with the outcome (different from $M \rightarrow Y 
	\leftarrow A$)
		\begin{itemize}
		\item true effect modifier: $M$ has a direct causal effect on $Y$
		\item surrogate effect modifier: a variable associated with $M$ that is associated with $Y$ through $M$
			\begin{itemize}
			\item simply a variable associated with the true effect modifier
			\item can be associated with a true effect modifier $M$ by structures including common causes, conditioning on common effects, or cause and effect
			\end{itemize}
		\end{itemize}
	\item causal diagrams are agnostic about interactions between treatments $A$ and $E$
		\begin{itemize}
		\item \em however\em, can encode information about interactions when using nodes that represent sufficient-component causes
		\end{itemize}
	\end{itemize}


\subsection{Structural Classification of Bias}
	\begin{itemize}
	\item	An association between two variables may exist because of:
		\begin{enumerate}
		\item cause and effect
		\item bias
		\item chance (because of sampling variability)
		\end{enumerate}
	\item common causes and stratification on a common effect are structural sources of bias (inappropriate stratification)
	\item chance is not bias because it is not structural
	\item \em bias \em can only be defined in terms of data's structure, thus bias is the result of a structural source of non-causal association
	\item there are a finite number of causal structures that can give rise to bias:
		\begin{enumerate}
		\item when treatment and outcome share a common cause
		\item when treatment and outcome share a common effect that it, or its descendant, is stratified upon
		\item cause and effect: reverse causation and information bias
			\begin{description}
			\item[reverse causation] outcome precedes (and has a causal effect on) measurement of treatment
			\item[information bias] measurement error of treatment, outcome, or confounders
			\end{description}
		\end{enumerate}
	\end{itemize}
	
\section{Confounding}
\subsection{Association is not Causation}
\begin{itemize}
\item due to common cause or due to unblocked backdoor path
\item 2 sources of association between $A$ and $Y$
	\begin{enumerate}
	\item $A \rightarrow Y$ \textbf{(A has causal effect on Y)}
	\item $A \leftarrow L \rightarrow Y$ \textbf{(backdoor path)}
	\end{enumerate}
\end{itemize}

\subsection{Examples of Confounding}
\begin{enumerate}
\item healthy worker bias (figure ~\ref{confounding} part (a))
\item confounding by indication or channeling (figure ~\ref{confounding} part (b) or (b))
\item population stratification (figure ~\ref{confounding} part (c))
\end{enumerate}
\begin{figure}
\centering
\includegraphics[scale=0.4]{confounding.pdf}
\caption{Example DAGs for confounding}
\label{confounding}
\end{figure}
	
\subsection{Backdoor Criteria} The causal effect of $A$ on $Y$ is  identifiable  if  all  backdoor  paths between  them  can  be  blocked.
\begin{itemize}
\item No common causes: marginally randomized experiment
\item Common causes but enough measured variables to block all backdoor paths: conditionally randomized experiment
\item 3 questions about confounding:
	\begin{enumerate}
	\item Does confounding exist? Yes, if there is any unblocked path between $A$ and $Y$
	\item Can confounding be eliminated? Yes, if all backdoor paths can be blocked using measured variables
	\item What variables are necessary to eliminate the confounding? The \em minimal \em set of variables that block all 
backdoor paths
	\end{enumerate}
\item backdoor criteria does \textbf{NOT} show the magnitude or direction of confounding
\end{itemize}

\subsection{Definition of a Confounder}
\begin{itemize}
\item any variable that can be used to reduce the bias caused by common cause between $A$ and $Y$
\item any variable that can be used to block backdoor path between $A$ and $Y$
\item traditional definition
	\begin{enumerate}
	\item confounder is associated with $A$
	\item confounder is associated with $Y$ \em conditional on $A$ \em (or among the \em untreated \em)
	\item confounder is \em not \em on the causal pathway between $A$ and $Y$	
	\end{enumerate}
\item according to traditional definition, in figure ~\ref{confounding}, all $L$s are confounders 
\begin{figure}
\centering
\includegraphics[scale=0.2]{confounding2.pdf}
\caption{A counter example of traditional definition of confounder}
\label{confounding2}
\end{figure}
\item in figure ~\ref{confounding2}, $L$ matches the traditional definition of confounder \textbf{BUT} conditioning on $L$ does \em not \em 
eliminate confounding (there was no confounding at all) but creates selection bias
\end{itemize}

\subsection{Structural vs. Statistical Definition of Confounding}
\begin{itemize}
\item structural definition of confounding showes that \textbf{statistical criteria is NOT sufficient to define confounding}
\item the structural definition first characterizes confounding as the bias resulting from the presence of common causes and then 
characterizes confounder as any variable that is necessary to eliminate the bias in the analysis
\item confounding is an absolute concept; confounder is a relative concept
\item confounding defined by counterfactuals
	\begin{enumerate}
	\item $Y_{a} \not{\amalg} A $
		\begin{itemize}
		\item no marginal exchangeability
		\item no ``marginally randomized experiment''
		\item there is common cause of $A$ and $Y$
		\item there is confounding between $A$ and $Y$
		\end{itemize}
	\item $Y_{a} \not \amalg A|L$
		\begin{itemize}
		\item no conditional exchangeability
		\item no exchangeability conditional on $L$
		\item no ``conditionally randomized experiment''
		\item there is unblocked backdoor paths between $A$ and $Y$ given $L$
		\item there is \em unmeausred \em confounding between $A$ and $Y$ (residual and unmeasured confounding)
		\end{itemize}
	\end{enumerate}
\item our hope for observational study: even though there are common causes, there will be no \em unblocked \em backdoor path given measured 
$L$s
\item additional assumptions (no selection bias and no measurement error) are needed for causal inference in observational study
\end{itemize}

\subsection{Surrogate \& Time-Varying Confounders}
\begin{description}
\item[Surrogate confounder] see figure~\ref{finepoint71}
	\begin{enumerate}
	\item is not on any path between treatment $A$ to $Y$
	\item is a direct descendant of a variable that does lie on a path from treatment $A$ to $Y$ which, if measured, would be used to block all non-causal paths between $A$ and $Y$
	\item because it is a descendant of a confounder, stratifying on this variable causes stratification of it's parent, the confounder
	\item the path is blocked only to the degree that the surrogate confounder $L$ and the unmeasured confounder $U$ are correlated
	\end{enumerate}
	
\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{figure_CI_7_5.png}
\caption{A surrogate Confounder}
\label{finepoint71}
\end{figure}
	
\item[Time-varying confounder] a time-varying variable that is needed to block the backdoor paths of a time-varying treatment
\end{description}

\subsection{Methods to adjust for confounding}
\begin{itemize}
\item can occur in the design or analysis phase

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.4]{coptions.png}
		\caption{Strategies for handling confounding}
		\label{coptions}
	\end{figure}

\item ideal randomized experiment: do not need subject-matter knowledge; randomization does everything
\item observariobal study: need subject-matter knowledge to identify $L$
	\begin{enumerate}
	\item G-methods (``generalized'')
		\begin{enumerate}
		\item standardization, IPW, G-estimation 
		\item \textbf{estimate causal effect}
		\item use conditional exchangeability in subsets defined by $L$ to estimate the causal effect of $A$ on $Y$ in the entire 
population or in any subset of it
		\item simulate the distribution of counterfactual outcomes in the population if backdoor paths involving the measured 
variables $L$ did not exist
		\item \textbf{in DAG}: delete the arrow from $L$ to $A$
		\end{enumerate}
	\item stratification-based methods
		\begin{enumerate}
		\item stratification, restriction, matching 
		\item \textbf{estimate association}
		\item use conditional exchangeability in subsets defined by $L$ to estimate the association between $A$ and $Y$in those 
subsets only
		\item estimate the association between $A$ and $Y$ in one or more subsets of the population in which the treated and the 
untreated are assumed to be exchangeable
		\item \textbf{in DAG}: stratification/restriction: box around $L$
		\item \textbf{in DAG}: matching: add a ``selection'' ($S$) node that is conditioned on
		\end{enumerate}
	\end{enumerate}
\end{itemize}

\subsection{Identification vs Control of Confounding}
\begin{itemize}
\item both stratification-based and g-methods require conditional exchangeability given $L$ to identify the causal effect of $A$ on $Y$
	\begin{enumerate}
	\item causal effect in \em entire \em population: conditional exchangeability in all levels of $L$
	\item causal effect in \em one level of $L$ \em : conditional exchangeability in that level of $L$
	\end{enumerate}
\item expert knowledge can be used to select confounders and avoid adjusting for nonconfounders
\item the definition of confounding does not depend on the adjustment method
\end{itemize}

\subsection{Strength and Direction of Confounding Bias}
\begin{itemize}
\item relationship between the confounder and the exposure of interest is a feature of the particular study base and the individuals sampled 
into study
	\begin{enumerate}
	\item matching in \em cohort \em study
	\item restriction
	\item selection of a population base that is not characterized by the confounder-exposure association
	\item randomization
	\end{enumerate}
\item confounding affects both \em point estimates \em and \em validity of statistical inference \em (e.g. p-value, confidence interval, 
etc.)
\item \textbf{Direction}
	\begin{itemize}
	\item apparent effect estimate is higher or lower than the true estimate
	\item figure ~\ref{confoundingdirection}
	\begin{figure}
	\centering
	\includegraphics[scale=0.6]{confoundingdirection.png}
	\caption{the direction of confounding}
	\label{confoundingdirection}
	\end{figure}
	\item \em Definitions \em
		\begin{description} 
		\item[Upward Bias] apparent effect measure  $>$ true effect measure 
		\item[Downward Bias] apparent effect measure  $<$ true effect measure 
		\end{description}
	\item \em Signed Causal DAGs \em
		\begin{enumerate}
		\item assess confounder-treatment and confounder-outcome associations
			\begin{itemize}
			\item add a positive sign $\oplus$ over the arrow $L \Longrightarrow A$ if the $L$ has a positive causal effect on $A$ 
(similarly for $L \Longrightarrow Y$)
			\item add a negative sign $\ominus$ over the arrow $L \rightarrow A$ if the $L$ has a negative causal effect on $A$ 
(similarly for $L \rightarrow Y$)
			\end{itemize}
		\item if $L \rightarrow A$ and $L \rightarrow Y$ are either (a) both positive or (b) both negative $\longrightarrow$ upward 
bias
		\item if $L \rightarrow A$ and $L \rightarrow Y$ are of opposite sign $\longrightarrow$ downward bias
		\end{enumerate}
	\item the above procedure only works in simple settings and when the variables are dichotomous
	\end{itemize}	
\item \textbf{Magnitude}
	\begin{itemize}
	\item How far will the bias move my estimate?
	\item A \textbf{sensitivity analysis} can repeat the analysis under several assumptions
		\begin{itemize}
		\item for each analysis assume (simulate) the magnitude of the bias
	 	\item see how much your effect estimate changes under the different scenarios
	 	\item quantify the bias by setting bounds as the amount of confounding that would have to occur to meaningfully change your 
estimate
	 	\end{itemize}
	\item For dichotomous outcome, treatment, and confounder the bias is limited 	

		\begin{figure}[h!]
		  \begin{center}
			\subfigure[Measured Confounding]{\label{sensitivity1}\includegraphics[scale=0.4]{sensitivity1.png}} \hspace{2 cm}
			\subfigure[Measured \& Unmeasured Confounding]{\label{sensitivity2}\includegraphics[scale=0.4]{sensitivity2.png}} \hspace{2 cm}
		  \end{center}
		  \caption{Strategies for controlling confounding}
		  \label{sensitivity}
		\end{figure}
 	
		\begin{itemize}
		\item Definitions
			\begin{enumerate}
			\item $RR_E$: the RR for disease associated with the exposure of interest
			$$RR_{E} = \frac{Risk_{exposed}}{Risk_{unexposed}}$$
			\item $RR_{CD}$: the RR for disease associated with the confounder
			$$RR_{CD} = \frac{Risk_{L=1}}{Risk_{L=0}}$$
			\item $RR_{EC}$: the RR for confounder associated with exposure 
			$$RR_{EC} = \frac{Pr[L=1|E=1]}{Pr[L=1|E=0]}$$
			\item $Pr[L=1]$: prevalence of confounder in the study population
			\end{enumerate}
		\item \textbf{assuming $RR_E > 1$, $RR_{CD} > 1$, $RR_{EC} > 1$}
		\item for preventive effect, recode the exposure
		\end{itemize}
		\begin{enumerate}
		\item $RR_{CD} > 1 \longrightarrow RR_E < RR_{CD}$
		\item $RR_{CD} > 1 \ and \ RR_{EC} > 1 \longrightarrow RR_E < RR_{EC}$
		\item thus, $1 < RR_E < min(RR_{CD}, RR_{EC})$
		\item further limited by the prevalence of the confounder $\Rightarrow Pr[L=1]$
			\begin{itemize}
			\item lowest when $Pr(L=1) = 1$ or $0$
			\end{itemize}
		\item also, can use $OR_{EC} = \frac{Pr[L=1|A=1]/Pr[L=0|A=1]}{Pr[L=1|A=0]/
[L=0|A=0]} = \frac{Pr[L=1|A=1] \times Pr[L=0|A=0]}{Pr[L=1|A=0] \times Pr[L=1|A=0]}$ instead of $RR_{EC}$
		\end{enumerate}
	\end{itemize}
\end{itemize} 

\begin{figure} [H]
\centering
\includegraphics[scale=0.4]{confoundingmagnitude.pdf}
\caption{Bounds of the magnitude of confounding}
\label{confoundingmagnitude}
\end{figure}

\pagebreak

\begin{table} [H]
		\centering
		\begin{tabular}{| p{1.1in} | p{1.1in} | p{1.8in} | p{1.8in} |}
		\hline 
		\textbf{Adjustiment stage} & \textbf{Method} & \textbf{Strengths} & \textbf{Limitations} \\ \hline
		\multirow{2}{1.1in}{\textsf{Design}} & \textsf{Restriction} & \textsf{Completely removes confounding in crude analysis, may also preserve positivity} & \textsf{limits possible study questions and generalizability} \\& \textsf{Matching} & \textsf{removes confounding in crude analysis for cohort studies, ensures positivity} & \textsf{for ca-c-: bias if matching on factor affected by both outcome and exposure, reduced efficiency if matching on factor only associated with exposure, bias without stratified analysis in case-control studies, cannot assess effect of matching factors only effect modification by them, inappropriate for time-varying situations and intermediates} \\ \hline \multirow{5}{1.1in}{\textsf{Analysis}} & \textsf{Standardization} & \textsf{can estimate effect in subsets of population, can be used in time-varying situations (g-formula) and for intermediate variables} & \textsf{not valid with non-positivity, G-null paradox} \\& \textsf{IPW} & \textsf{effect in subsets of population, can be used in time-varying situations, for intermediates and for censoring, can also be used with random nonpositivity} & \textsf{non-positivity, treatment model assumptions that can be mispecified, cannot assess time varying EM} \\& \textsf{G-estimation} & \textsf{causal effect and CIs, allows for interaction between exposure and confounders} & \textsf{computationally complex, additional model with assumptions, cannot use logistic models} \\& \textsf{Stratification} & \textsf{can be used even if it results in non-positive cells, but with loss of power, simple} & \textsf{not appropriate for time-varying situations or intermediates, bias efficiency trade-off} \\& \textsf{Regression} & \textsf{same as stratification} & \textsf{same as stratification} \\ \hline 
		
		\end{tabular}
	\label{tab:confoundingadjustment1}
	\caption{Comparison between methods for adjusting for confounding with measured condfounders}
\end{table}

\begin{table} [H]
		\centering
		\begin{tabular}{| p{1.1in} | p{1.1in} | p{1.8in} | p{1.8in} |}
		\hline 
		\textbf{Adjustiment stage} & \textbf{Method} & \textbf{Strengths} & \textbf{Limitations} \\ \hline
		\multirow{2}{1.1in}{\textsf{Design}} & \textsf{Two stage Analysis} & \textsf{Reduced cost in data collection} & \textsf{complex ML analysis, additional assumptions about subsample and external data}\\ & \textsf{Crossover designs} & \textsf{no need to measure or control time-invariant confounders} & \textsf{does not address time-varying confounders, assumptions about hazard periods and carry over effects}\\  \hline \multirow{2}{1.1in}{\textsf{Analysis}} & \textsf{Instrumental Variables} & \textsf{no unmeasured confounding} & \textsf{additional unverifiable assumptions} \\& \textsf{Sensitivity Analysis}  & \textsf{no additional data needed} & \textsf{assumptions about different model specification, external data}\\ \hline 
		
		\end{tabular}
	\label{tab:confoundingadjustment2}
	\caption{Comparison between methods for adjusting for confounding with unmeasured condfounders}
\end{table}





\section{Selection Bias}
\subsection{Definition of Selection Bias}
\begin{itemize}
\item \em Selection bias \em is formally defined as the bias resulting from conditioning on the common effect of two variables:
  \begin{enumerate}
  \item treatment or a variable associated with treatment
  \item outcome or a variable associated with outcome
  \end{enumerate}
\item the process by which individuals are selected into the analysis creates a joint distribution of treatment and outcome where the two 
variables bear an association that is \em not causal \em. In general this can happen in three ways:
	\begin{enumerate}
	\item Selection into study (matching, sampling frame in case-controls, institutional study, survivors etc.)
	\item Effect measures (e.g. hazard ratios) that condition on survival when the distribution of confounders in the exposed and 
unexposed changes over time (i.e. differential loss to follow up or survival, etc.)
	\item Inappropriate stratification during data analysis
	\end{enumerate}
\item In DAG language, selection bias \em always \em involves a collider that has been conditioned upon
\item Selection bias lacks both internal and external validity. Thus it is \em not \em the same as a mere lack of generalizability
	\begin{description}
	\item[internal validity] the effect measure for the population under study is causal
	\item[external validity] the effect measure for the population under study is both causal and transportable
	\end{description}
\end{itemize}

\subsection{Selection Bias vs Confounding}
\begin{itemize}
\item key differences
	\begin{enumerate}
	\item What randomization \em \textbf{does} \em do
		\begin{enumerate}
		\item protect against confounding
		\item protect against selection bias when treatment assignment occurs \em AFTER \em selection
		\item thus figures ~\ref{fig_ci_83_84} and ~\ref{fig_ci_85_86} could \underline{not} represent \em volunteer bias \em in a 
randomized trial
			\begin{itemize}
			\item figure ~\ref{fig_ci_83_84} because in RCT treatment cannot determine selection
			\item figure ~\ref{fig_ci_85_86} because in RCT treatment is determined solely by the investigator
			\end{itemize}
		\end{enumerate}
	\item What randomization \em \textbf{does not} \em do
		\begin{enumerate}
		\item does not protect against selection bias when treatment occurs \em BEFORE \em selection
		\item does not protect against differential loss to follow up
		\end{enumerate}
	\end{enumerate}
\item a confounder is therefore any variable $L$ on which one has to adjust for to remove bias due to a common cause \em or \em common effect
\item understanding the structure of the problem can help guide:
	\begin{enumerate}
	\item the choice of study design
	\item the choice of analytical method
	\end{enumerate}
\item selection on pre-treatment variables can introduce bias (i.e. \em see figure ~\ref{fig_ci_85_86} \em). This explains why some 
variables act as confounders in one study and not in another
\end{itemize}

\subsection{The Structure of Selection Bias}

The square around $C$ indicates that the \em analysis is restricted to subjects with $C=0$ \em by study design or choice of analysis

\begin{enumerate}
\item \em \textbf{Conditioning on a common effect of treatment $A$ and outcome $Y$} \em
\begin{figure}
  \begin{center}
    \subfigure[condition on common effect]{\label{fig_ci_81}\includegraphics[scale=0.5]{fig_ci_8_1.png}} \hspace{2 cm}
    \subfigure[condition on descendant of common effect]{\label{fig_ci_82}\includegraphics[scale=0.5]{fig_ci_8_2.png}} \hspace{2 cm}
  \end{center}
  \caption{Selection Bias in a Randomized Trial}
  \label{fig_ci_81_82}
\end{figure}
	\begin{itemize}
	\item \em see figure ~\ref{fig_ci_81_82} \em
	\item association between $A$ and $Y$ is \em induced \em by conditioning on $C$
		\begin{enumerate}
		\item conditioning on $C$ opens the path $A \rightarrow C \leftarrow Y$ 
		\item $\underbrace{\frac{Pr[Y=1|A=1,C=0]}{Pr[Y=1|A=0,C=0]}}_\text{associational} \neq \underbrace{\frac{Pr[Y^{a=1}]}{Pr[Y^
{a=0}]}}_\text{causal}$
		\item Example
			\begin{itemize}
			\item analysis of only survivors \em see figure ~\ref{fig_ci_81} \em
			\item selection into study or analysis depends on a variable affected by survival $S$  \em see figure ~\ref
{fig_ci_82} \em
			\end{itemize}
		\end{enumerate}	
	\end{itemize}

\item \em \textbf{Conditioning on a common effect of treatment $A$ and a variable $L$ that is \underline{also} a cause of the outcome} \em
\begin{figure}
  \begin{center}
    \subfigure[note common cause of outcome and selection]{\label{fig_ci_83}\includegraphics[scale=0.5]{fig_ci_8_3.png}} \hspace{2 cm}
    \subfigure[note common cause of outcome and determinant of selection]{\label{fig_ci_84}\includegraphics[scale=0.5]{fig_ci_8_4.png}} 
\hspace{2 cm}
  \end{center}
  \caption{Selection Bias in a Randomized Trial}
  \label{fig_ci_83_84}
\end{figure}
	\begin{itemize}
	\item \em see figure ~\ref{fig_ci_83_84} \em
	\item association between $A$ and $Y$ is \em induced \em by conditioning on $C$
		\begin{enumerate}
		\item conditioning on $C$ opens the path $A \rightarrow C \leftarrow U \rightarrow Y$ 
		\item $\underbrace{\frac{Pr[Y=1|A=1,C=0]}{Pr[Y=1|A=0,C=0]}}_\text{associational} \neq \underbrace{\frac{Pr[Y^{a=1}]}{Pr[Y^
{a=0}]}}_\text{causal}$  
		\item Example
			\begin{itemize}
			\item loss to follow up due to disease severity $U$ and side-effects $A$.  If $L$ is measured could be used to block 
opened path \em see figure ~\ref{fig_ci_83} \em
			\item \em prior \em treatment $A$ and disease severity have direct effects on symptoms $L$  \em see figure ~\ref
{fig_ci_84} \em
			\end{itemize}
		\end{enumerate}
	\end{itemize}

\item \em \textbf{Conditioning on a common cause on some trait that is \underline{associated with treatment} and a variable $L$ that is 
\underline{also} a cause of the outcome} \em
\begin{figure}
  \begin{center}
    \subfigure[note common cause of treatment and selection]{\label{fig_ci_85}\includegraphics[scale=0.5]{fig_ci_8_5.png}} \hspace{2 cm}
    \subfigure[note common cause of treatment and determinant of selection]{\label{fig_ci_86}\includegraphics[scale=0.5]{fig_ci_8_6.png}}
    \subfigure[note treatment occurring after selection]{\label{fig_ci_89}\includegraphics[scale=.5]{fig_ci_8_9.png}}
  \end{center}
  \caption{Selection bias in a \em Non\em-Randomized Trial}
  \label{fig_ci_85_86}
\end{figure}
	\begin{itemize}
	\item \em see figure ~\ref{fig_ci_85_86} \em
	\item association between $A$ and $Y$ is \em induced \em by conditioning on $C$
		\begin{enumerate}
		\item conditioning on $C$ opens the path $A \leftarrow W \rightarrow C \leftarrow L \leftarrow U \rightarrow Y$ 
		\item $\underbrace{\frac{Pr[Y=1|A=1,C=0]}{Pr[Y=1|A=0,C=0]}}_\text{associational} \neq \underbrace{\frac{Pr[Y^{a=1}]}{Pr[Y^
{a=0}]}}_\text{causal}$  
		\item Example
			\begin{itemize}
			\item unmeasured variables that determine both treatment and attitudes towards participation or keeping study visit 
appointments \em see figure ~\ref{fig_ci_85} \em
			\item unmeasured variables that determine both treatment and threshold for reporting symptoms \em see figure ~\ref
{fig_ci_86} \em
			\end{itemize}
		\end{enumerate}
	\end{itemize}
\end{enumerate}

\subsection{Examples of Selection Bias}

\subsubsection{Differential loss to follow up} Also known as informative censoring
\begin{itemize}
\item loss to follow up differs between the treated and untreated
\item diseased have a different rate of loss to follow up
\end{itemize}
\subsubsection{Missing data bias \& Non-response bias} 
\begin{itemize}
\item missingness differs between the treated and untreated (perhaps more data collected on treated persons in observational study, or 
subjects receiving unblinded intervention more willing to disclose data)
\item missingness also related to outcome (perhaps a greater willingness to disclose data among diseased subjects).
\end{itemize}

\subsubsection{Healthy worker bias} 
\begin{itemize}
\item underlying true health status is determinant of outcome and being at work
\item study restricted to individuals at work at time of outcome ascertainment
\item treatment reduces probability of being at work in the near future (directly or through common cause)
\end{itemize}

\subsubsection{Self-selection bias \& Volunteer bias}
\begin{itemize}
\item study restricted to those who agreed to participate
\item can only occur when selection precedes treatment
\item thus cannot occur in randomized trials
\end{itemize}

\subsubsection{Selection affected by treatment received before study entry} 
\begin{itemize}
\item treatment occurs before start of study
\item[]\em or \em 
\item there is a pre-study component
\end{itemize}

\subsubsection{Berksonian Bias}
\begin{itemize}
\item treatment and disease are simultaneously associated with selection into an institution
\item the joint distribution of treatment and disease in the institution differs from the source population
	\begin{itemize}
	\item persons in institution more likely to have multiple diseases (or determinants) for selection into institution
	\item sampling a control from a reference group (such as an ``unrelated disease") is likely to induce an association between 
treatment and outcome
	\end{itemize}
\item can happen even when selection frequencies for treatment and disease are equal
\end{itemize}

\subsubsection{Survivor Bias}
\begin{itemize}
\item can be thought of as selecting on a consequence of the outcome (e.g. survival)
\item for example, if outcome is cancer and only study those who are 60 years old
	\begin{itemize}
	\item the treatment may have had such a strong effect that most of those who were exposed to it died
	\item treatment would look beneficial after this point (e.g. crossing hazards)
	\end{itemize}
\end{itemize}

\subsubsection{Incidence-Prevalence Bias}
\begin{itemize}
\item occurs when sampling prevalent cases in a case control study, either at the end or during the course of follow up 
\item occurs when exposure affects disease duration
\item when this happens exposure is associated with the outcome, resulting in selection bias
\end{itemize}

\subsection{Selection Bias in Case-Control Studies}
\begin{itemize}
\item the arrow $Y \rightarrow C$ is present by design (cases are oversampled)
\item the concern is if the arrow $A \rightarrow C$ \em or \em the structure  $A \leftarrow L \rightarrow C$ is present
	\begin{enumerate}
	\item ask yourself...has the selection process resulted in a different treatment distribution among cases and controls than exists 
in the source population?
	\item if yes, then selection is related to treatment and $\longrightarrow$ selection bias is present
	\item if there is selection into the institution where your study is based,  the following are possible solutions:
		\begin{enumerate}
		\item if the structure  $A \leftarrow L \rightarrow C$ is present could collect data on $L$
		\item sample controls from a disease that is unrelated to your treatment
		\end{enumerate}
	\item want controls to be sampled independently of treatment
	\end{enumerate}
\end{itemize}

\subsection{The Built-in Selection Bias of Hazard Ratios}
\begin{figure}[h!]
  \begin{center}
    \subfigure[Association]{\label{fig_ci_87}\includegraphics[scale=0.5]{fig_ci_8_7.png}}
    \subfigure[Causation]{\label{fig_ci_88}\includegraphics[scale=0.5]{fig_ci_8_8.png}}
  \end{center}
  \caption{The selection bias from hazard ratios can render the causal effect unidentifiable - Which one is the true DAG?}
  \label{fig_ci_87_88}
\end{figure}
	\begin{itemize}
\item \em see figure ~\ref{fig_ci_87_88} \em
\item in \em discrete time\em, the \textbf{hazard ratio} is defined as the probability of dying at time $T=t$ given that you have 
survived up to time $t$
$$IRR_{A,Y_{t}|Y_{t-1}} = \frac{Pr[Y_{t}=1|A=1,Y_{t-1}=0]}{Pr[Y_{t}=1|A=0,Y_{t-1}=0]}$$
\item $IRR_{A,Y_{t}|Y_{t-1}}$ is the apparent hazard ratio and may not be causal
\item by definition, the hazard ratio conditions upon survivors  
\item the bias arises from conditioning on the common effect $Y_{t-1}$ of treatment $A$ and an unmeasured confounder $U$, where $U$ is also a 
\em cause \em of $Y_{t}$
\item this opens the path $A \rightarrow Y_{t-1} \leftarrow U \rightarrow Y_{t}$
\item an unmeasured cause of death that is marginally unassociated with treatment such as $U$ is often referred to as \textbf{frailty}
\item this bias can occur in both randomized and observational studies
\end{itemize}

\subsection{Identifiability of Causal Effects}
\begin{figure}
	\centering
      	\includegraphics[scale=0.4]{figure_sb_8_3.jpg}
       	\caption{Identifiability and analytical control of censoring by treating it as an intervention}
       	\label{figure_sb_8_3}
\end{figure}
\begin{itemize}
\item censoring  $C$ can result in selection bias when there is differential censoring among treatment groups (Figure ~\ref{figure_sb_8_3})
\item censoring can be viewed as another ``treatment" where measurement error and confounding need to be eliminated
	\begin{itemize}
	\item causal question of interest: $Pr[Y^{a=1}=1]$ vs $ Pr[Y^{a=0}=1]$
		\begin{itemize}
		\item what we are interested in is the results \em had no one been censored \em
			\begin{itemize}
			\item $Pr[Y^{a=1,C=c}] $ vs $ Pr[Y^{a=0,C=c}]$
			\end{itemize}
		\item with censoring, information is only available for the subgroup $C=0$
		\item only investigating $C=0$ subset results in biased results due to selection bias
		\item $A$ and $C$ can be seen as joint interventions, making it important that exchangeability, positivity, and consistency 
hold for both $A$ and $C$
		\item selection bias can be eliminated by analytic adjustment for confounding of ``treatment" $C$
		\item selection bias does not hold for competing risks
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Adjusting for Selection Bias}
\begin{itemize}
\item selection bias can sometimes be avoided by adequate design, however it is often unavoidable
	\begin{itemize}
	\item loss to follow-up, self-selection, and missing data bias can occur no matter how careful the investigator
	\end{itemize}
\item selection bias needs to be corrected in the analysis
	\begin{itemize}
	\item IPW (or standardization) can be used to correct selection bias
	\item procedure
		\begin{itemize}
		\item formally, we modify IPW to weight by the inverse probability of receiving the \em joint treatment \em of $A$ and $C=0$ 
conditional upon variables $L$ needed to block the back door path between
			\begin{enumerate}
			\item $A$ and $Y$
			\item $C$ and $Y$
			\end{enumerate}
		\item we estimate the censoring and treatment parts of the probability of the joint treatment $\{C=0,A\}$  separately and 
use them to construct a single inverse probability weight
	$$Pr[C=0,A=a] = Pr[C=0|A] \times Pr[A]$$
      $$\underbrace{\frac{1}{Pr[C=0,A=a|L]}}_\text{joint treatment weight}	= \underbrace{\frac{1}{Pr[C=0|A,L]}}_\text{censoring weight} 
\times \underbrace{\frac{1}{Pr[A|L]}}_\text{treatment weight}	$$	
		\item assign the weight to each selected subject ($C=0$) to account for themself and other non-selected ($C=1$) subjects 
with the same distribution of $L$ and $A$
		\item the goal is to make a pseudo-population the same size as the original population where censoring $C$ is independent of 
$Y$ within levels of $L$: $Y \amalg C|L$
		\end{itemize}
	\item the association measure in the pseudo-population equals the effect measure in the original population if:
		\begin{enumerate}
		\item exchangeability: $E[Y|C=0,L=l,A=a]= \overbrace{E[Y|C=1,L=l,A=a]}^{not \ observed}$: the average outcome in $C=0$ must 
equal the unobserved average outcome in $C=1$ with the same values of $A$ and $L$
			\begin{itemize}
			\item the probability of selection used in weighting is conditional upon
				\begin{enumerate}
				\item treatment
				\item all factors that independently predict both selection and outcome
				\end{enumerate}
			\item if this holds the variables included in $Pr[C=0|A,L]$ are sufficient to block all backdoor paths \em between 
$C$ and $Y$ \em
			\end{itemize}
		\item $Pr[C=0|L=l]>0$ for all $l$: all conditional probabilities of being uncensored given $L$ must be greater than zero
			\begin{itemize}
			\item positivity assumption required for $C=0$, but not for $C=1$
			\item not interested in inferring what would have happened if at least one subjects had been censored in each strata
			\end{itemize}
		\item consistency: the effect of treatment $A$ is the same in the pseudo-population as in the original population if nobody 
had been censored
			\begin{itemize}
			\item interpretation is valid when censoring the result of loss to follow-up or nonresponse
			\item interpretation is questionable in the presence of competing events
			\end{itemize}
		\end{enumerate}
	\item could argue that stratification by level of $L$ (rather than IPW or standardization) would be sufficient to remove selection 
bias
		\begin{itemize}
		\item care should be taken to ensure colliders are not opened
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Selection without Bias}
\begin{itemize}
\item conditioning on a collider \em always \em induces an association between its common causes
	\begin{itemize}
	\item the association may be restricted to only \em certain \em levels of the common effect
	\item it is possible that selection on a common effect does not result in selection bias when the analysis is restricted to a single 
level of the common effect
		\begin{itemize}
		\item example: 3 cause-specific mortality variables (1) death from tumor $Y_A$, (2) death from heart attach $Y_E$, and (3) 
death from other causes $Y_O$ (Figure ~\ref{figure_sb_8_13})
		\item the path between surgery $A$ and haplotype $E$ though opened by conditioning on collider $Y$ ($Y=0$) is blocked by 
conditioning on the noncolliders $Y_A, Y_E$, and $Y_O$ ($Y_A=Y_E=Y_O=0$)
		\item $A$ and $E$ affect survival through independent mechanisms
		\item multiplicative survival model: when the conditional probability of survival given $A$ and $E$ ($Pr[Y=0|A=a,E=e]$) is 
the product of functions $a$ and $e$ ($g(a) \times h(e)$) 
		\end{itemize}
	\end{itemize}

\begin{figure}[h!]
	\centering
      	\includegraphics[scale=0.4]{figure_sb_8_13.jpg}
       	\caption{Selection without bias.  When $Y=0$, $A$ and $E$ are conditionally independent}
       	\label{figure_sb_8_13}
\end{figure}

\item in practice it is important to consider the expected direction and magnitude of selection bias
	\begin{itemize}
	\item direction
		\begin{itemize}
		\item depends on how the two causes $A$ and $E$ interact to cause $Y$
			\begin{itemize}
			\item factor $U$ is associated with $A \ or \ E$, $A$ and $E$ are negatively associated
			\item factor $U$ is associated with $A \ and \ E$, $A$ and $E$ are positively associated
			\end{itemize}
		\item DAGs fail to distinguish between the \em and \em and \em or \em mechanisms
		\end{itemize}
	\item magnitude
		\begin{itemize}
		\item a large selection bias requires strong associations between the collider and both treatment and outcome
		\end{itemize}
	\end{itemize}
\end{itemize}



\section{Information Bias}
\subsection{Measurement Error}
\begin{itemize}
\item synonymous for ``misclassification'' for discrete variable
\item graphical representation:

\begin{figure}[h!]
\centering
\includegraphics[scale=.5]{Fig_9_1new.png}
\caption{$A$: true A; $A^{*}$: measured A; $U_{A}$: a measurement error (All variables associated with $A^{*}$ other than $A$)}
\label{Fig_9_1}
\end{figure}

\item in the presence of measurement bias, the identifiability conditions exchangeability, positivity, and consistency are insufficient to compute the causal effect of treatment $A$ and outcome $Y$. On risk ratio scale:
$$Pr[Y^{*}=1|A^{*}=1]/Pr[Y^{*}=1|A^{*}=0] \quad \neq \quad Pr[Y^{a=1} =1]/Pr[Y^{a=0}=1]$$
\item[*] in general, methods for measurement error rely on a combination of modeling assumptions and validation samples
	\begin{itemize}
	\item \textbf{validation samples} are subsets of the study sample in which key variables are measured with little or no error.
	\end{itemize}
\end{itemize}

\subsection{The Structure of Measurement Error}
\begin{itemize}
\item The structure of measurement error is classified according to two properties:
	\begin{enumerate}
	\item \textbf{Idependence:} 
		\begin{itemize}
		\item the measurement error of variable $A$ (i.e. $U_{A}$) is not associated with the \em measurement error \em of another variable Y (i.e. $U_{Y}$)
		\item Independence of $U_{A}$ and $U_{Y} \; \Longrightarrow \; f(U_{A}, U_{Y})= f(U_{A}) \times f(U_{Y}) $
		\end{itemize}
	\item \textbf{Non-differentiality:} 
		\begin{itemize}
		\item the measurement error of variable $A$ (i.e. $U_{A}$) is not associated with the \em value \em of another variable $Y$ (i.e. $Y=y$)
		\item $ f (U_{A}|Y) = f (U_{A}) \quad or \quad f (U_{Y}|A) = f (U_{A}) $
		\end{itemize}
	\end{enumerate}
\item Independence and non-differentiality produce distinct structures of measurement error:

\begin{figure}[h!]
	\centering
	\subfigure[Independent]{
	\includegraphics[scale=.5]{Fig_9_2new.png} 
	\label{fig_9_2}
	} \hspace{1 cm}
	\subfigure[Dependent]{
	\includegraphics[scale=.5]{Fig_9_3new.png}
	\label{fig_9_3} 
	}
	\caption{Non-differential measurement error}
	\label{fig_9_23}
	\end{figure}

	\begin{enumerate}
	\item \textbf{Independent nondifferential} \\
	\textit{see figure ~\ref{fig_9_2} for DAG}
		\begin{itemize}
		\item measurement errors of exposure and outcome are \em not \em associated and do \em not \em depend on the value of the other variable
		\item $Y^{*}$: measured Y; $U_{Y}$: all variables associated with $Y^{*}$ other than $Y$
		\item e.g. Data entry error for $A^{*}$ and $Y^{*}$
		\end{itemize}
		
\item \textbf{Dependent nondifferential}\\
\textit{see figure ~\ref{fig_9_3} for DAG}
	\begin{itemize}
	\item the measurement errors of exposure and outcome \em are \em associated but do \em not \em depend on the value of the other variable
	\item e.g. Recall error for $A^{*}$ and $Y^{*}$, $U_{AY}$: ability to recall
	\end{itemize}
\item \textbf{Independent differential}  \\
\textit{see figure ~\ref{fig_9_5} for DAG where outcome affects measurement of exposure}
	\begin{itemize}
	\item measurement errors of exposure and outcome are \em not \em associated but \em do \em depend on the value of the other variable
	\item e.g. Recall bias: Study of the drug use in gestation period ($A$) on congenital malformation ($Y$), drug use ascertained by recall ($U_{A}$). Arrow: Outcome $Y$ affects $U_{A}$
	\item e.g. Reverse causation bias: Study of liver toxicity ($Y$) related to drug ($A$), drug level ($A^{*}$) measured retrospectively, which is affected by the liver metabolism ($U_{A}$), liver metabolism was impaired due to liver toxicity ($Y \rightarrow U_{A}$) \\
	\\
\textit{see figure ~\ref{fig_9_5} for DAG}
	\item e.g. Detection bias: the physician prescribed a new drug to patients and ordered more monitoring tests ($A \rightarrow U_{Y}$), resulting in higher frequencies of liver toxicity in the treatment group ($A \rightarrow Y$)
	\item e.g. Detection bias: estrogen and endometrial cancer
	\item e.g. Detection bias: mammography screening and breast cancer
	\end{itemize}
	\begin{figure}
	\centering
	\subfigure[]{
	\includegraphics[scale=.5]{Fig_9_4new.png} 
	\label{fig_9_4}
	} \hspace{1 cm}
	\subfigure[]{
	\includegraphics[scale=.5]{Fig_9_5new.png}
	\label{fig_9_5} 
	}
	\caption{Independent differential measurement error}
	\label{Fig_9_45}
	\end{figure}
\item \textbf{Dependent differential:} \\
\textit{see figure ~\ref{Fig_9_67}}
	\begin{itemize}
	\item measurement errors of exposure and outcome \em are \em associated and \em do \em depend on the value of the other variable
	\end{itemize}
	\begin{figure}[h!]
	\centering
	\subfigure[]{
	\includegraphics[scale=.5]{Fig_9_6new.png}
	} \hspace{1 cm}
	\subfigure[]{
	\includegraphics[scale=.5]{Fig_9_7new.png}
	}
	\caption{Dependent differential measurement errors}
	\label{Fig_9_67}
	\end{figure}
\end{enumerate}
\end{itemize}	

\subsection{Mismeasured confounders} 
\begin{itemize}
\item[\-] \textit{see figure ~\ref{Fig_9_8}}
\item e.g. Study of the drug therapy and liver toxicity, past history of hepatitis was a potential confounder ($L$) and measured by recall ($L^{*}$)
	\begin{itemize}
	\item Mismeasured confounder can be viewed as surrogate confounder
	\item Controlling for mismeasured confounder does not completely block the backdoor path (residual confounding), measurement bias or information bias
	\end{itemize}
\item[\-] \textit{see figure ~\ref{Fig_9_9}}
\item e.g. $L$ has a indirect relationship between $A$ and $Y$ through $U$, controlling for $L^{*}$ does not block the backdoor path
	\begin{itemize}
	\item Mismeasurement of confounders can even lead to what looks like effect modification
	\end{itemize}
\item[\-] \textit{see figure ~\ref{Fig_9_10}}
\item conditioning on a mismeasured common effect ($C^{*}$) still opens the backdoor path
\end{itemize}

\begin{figure}[h!]
\centering
\subfigure[]{
\includegraphics[scale=0.5]{Fig_9_8new.png}
\label{Fig_9_8}
} \hspace{.5 cm}
\subfigure[]{
\includegraphics[scale=.5]{Fig_9_9new.png}
\label{Fig_9_9} 
} \hspace{.5 cm}
\subfigure[]{
\includegraphics[scale=.5]{Fig_9_10new.png}
\label{Fig_9_10} 
}
\caption{Mismeasured adjustment variables}
\label{Fig_9_8910}
\end{figure}

\subsection{Strength and direction of measurement bias}
\begin{itemize}
\item If A-Y association is null
	\begin{itemize}
	\item only \em independent and nondifferential \em measurement error will not affect the results ($A^{*}-Y^{*}$ association is null)
	\item Other than this circumstance, the measurement error may lead to bias either away from or close to the null
	\end{itemize}
\item For non-dichotomous treatment, measurement error may result in $A-Y$ and $A^{*}-Y^{*}$association in opposite direction, even under independent and nondifferential measurement error, as long as $A^{*}-A$ association in non-monotonic.
\item The magnitude of measurement bias depends on the measurement error. Measurement bias generally increase as the strength of $U_{A} \rightarrow A^{*}$ or $U_{Y} \rightarrow Y^{*}$ increase
\end{itemize}

\subsection{Types of Misclassification and Expected Bias}
\begin{itemize}
\item Examples
	\begin{itemize}
	\item \em cumulative exposure \em that include time outside of the induction period
	\item \em grouped exposures\em, some of which do not contain the etiological agent
	\end{itemize}
\item Definitions	
	\begin{itemize}
	\item \textit{Non-differential}: both  Sn and Sp are the same in group A=1 and group A=0
	\item \textit{Differential}: either Sn or Sp varies by group status
	\item \textit{Dependent}: the error is associated with the \em measurement error \em of another variable
	\end{itemize}
\end{itemize}

\subsubsection{Non-Differential Misclassification}
\begin{itemize}
\item is ``random" misclassification
\item \em is expected  \em to bias the  \em average value \em of the effect measure (i.e. from repeated studies) to the null under the following conditions: 
	\begin{enumerate} 
	\item the variable is dichotomous
	\item the misclassification is \underline{exactly} non-differential
	\item misclassification errors are independent of measurement errors in other variables in the analysis
	\item misclassification errors are not structurally related to other biases (confounding, collider-bias, etc.)
	\item[-] even when these hold, in a given study with misclassification the effect measure may be \em further away \em from the null due to random variation
	\end{enumerate} 
\item \textbf{can also affect statistical inference}:
	\begin{enumerate} 
	\item reduce power of hypothesis tests
	\item incorrectly centered and narrower confidence intervals
	\item sample size calculations that overestimate power
	\item \em however \em the test of the null hypothesis is still valid (ie.e correct Type I error rate)
	\end{enumerate} 
\end{itemize}

\subsubsection{Differential misclassification}
\begin{itemize}
\item Examples
	\begin{itemize}
	\item better (or more frequent) measurement applied to one group vs. the other
	\item differences in specimen handling, storage, assay, or time until analysis (e.g. deterioration)
	\item recall bias
	\item categorization of continuous variables can change nondifferential to differential error
	\end{itemize}
\item may bias towards or away from the null
\end{itemize}

\subsubsection{Recall Bias}
\begin{itemize}
\item A form of differential measurement error/misclassification
\item knowledge or feelings about the outcome affects how exposure is \em recalled \em (i.e. retrospectively measured)
\item is expected to result in bias either away or towards the null
\item Some things that may affect recall:
	\begin{itemize}
	\item time since exposure can affect memory
	\item parental guilt can affect recall of child or fetus exposure
	\end{itemize}
\item a possibility in any case-control study that relies on subject memory
\end{itemize}

\subsection{Correcting for Misclassification of Dichotomous Variable}

\begin{enumerate}
\item obtain sensitivity($\theta$) and specificity($\phi$)
	\begin{itemize}
	\item[-] usually by performing a validation study in a randomly chosen subset of your sample
	\item[-] in this sample exposure is measured with little or no misclassification
	\end{itemize}
\item obtain expected cell counts with misclassification:

\begin{table}[h!]
\begin{tabular}{l  l  c  c  l  c  c  l  c  c  l }
 &  \multicolumn{4}{c}{\textbf{True Exposure}} &  &  &  \multicolumn{4}{c}{\textbf{Misclassified Exposure}}   \\
 &  & $A=1$ & $A=0$ &  &  &  &  & $A^{*}=1$ & $A^{*}=0$ &  \\
\cline{3-4} \cline{9-10}                     
\multirow{2}{*}{\textbf{Disease}} & $Y=1$ & \multicolumn{1}{|c|}{$a$} & \multicolumn{1}{c|}{$b$} & $m_{1}$ & \multirow{2}{*}{$\xrightarrow{\theta , \ \phi}$} &  & $Y=1$ & \multicolumn{1}{|c|}{$a^{*}$} & \multicolumn{1}{c|}{$b^{*}$} & $m_{1}$ \\
\cline{3-4} \cline{9-10}                     
 & $Y=0$ & \multicolumn{1}{|c|}{$c$} & \multicolumn{1}{c|}{$d$} & $m_{0}$ &  &  & $Y=0$ & \multicolumn{1}{|c|}{$c^{*}$} & \multicolumn{1}{c|}{$d^{*}$} & $m_{0}$ \\
\cline{3-4} \cline{9-10} \\
\end{tabular}
\caption{True and Misclassified Exposure}
\label{misclass}
\end{table}


\begin{align*}
a^{*} &= \theta a+(1-\phi b) \\
b^{*} &= (1-\theta)a+\phi b\\
c^{*} &= \theta c + (1-\phi)d\\
d^{*} &= (1-\theta) c + \phi d
\end{align*}
\item calculate expected true cell counts without misclassification (\em see table ~\ref{misclass} \em)
\begin{align*}
a=\frac{\phi m_{1}-b^{*}}{\theta+\phi-1} \qquad & \qquad b=\frac{\theta m_{1}-a^{*}}{\theta+\phi-1} \\
\\
c=\frac{\phi m_{0}-d^{*}}{\theta+\phi-1} \qquad & \qquad d=\frac{\theta m_{0}-c^{*}}{\theta+\phi-1}
\end{align*}
\end{enumerate}

\subsection{Noncompliance}

	\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.5]{randomdag.png}
			\caption{Characteristics of a simple randomized trial. U: Unmeasured patient characteristics, Z: Treatment assignment, A: Treatment received, C: Censoring Indicator, Y: Outcome}
			\label{randomdag}
	\end{figure}

	\begin{itemize}
	\item in real randomized experiments, participants may not comply with the assigned treatment
	\item distinguish \em assigned \em treatment ($Z$) and \em received \em treatment ($A$)
	\item $Z$ is a misclassified version of $A$
	\item noncompliance (of $Z$) is a special case of \em treatment \em misclassification 
		\begin{itemize}
		\item in general, $A^{*}$ has NO causal effect on $Y$ (refers to figure ~\ref{Fig_9_1})
		\item however, $Z$ has causal effect on $Y$ (see figure ~\ref{noncompliance})
			\begin{enumerate}
			\item $Z \rightarrow A \rightarrow Y$ 
			\item $Z \rightarrow Y$ (not through A, e.g. change life style)
			\end{enumerate}
		\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.2]{noncompliance.pdf}
		\caption{DAG showing noncompliance. $Z$ is assigned treatment, $A$ is received treatment.}
		\label{noncompliance}
		\end{figure}
		\item causal effect of $Z$ on $Y$ combined both effects
		\item how to eliminate $Z \rightarrow Y$, i.e. only effects of $Z$ on $Y$ are $Z \rightarrow A \rightarrow Y$
			\begin{itemize}
			\item \em double-blind placebo-controlled randomized experiment \em
			\end{itemize}
		\item no $Z \rightarrow Y$: \em exclusion restriction \em holds
			\begin{itemize}
			\item $Y^{z=0,a}=Y^{z=1,a}$ for all subjects and all values $a$
			\end{itemize}
		\end{itemize}
	\end{itemize}

\subsection{Intention-to-treat Effect}
	\begin{itemize}
	\item as-treated effect (``efficacy'') \textbf{used for adverse effect}
		\begin{itemize}
		\item[\-] \textit{see figure ~\ref{noncompliance}}		
		\item \em nonrandom noncompliance \em results in $Y^{a} \not \amalg A$
		\item the reason why participants received treatment ($A=1$) is not random but rather associated with participants' prognosis $U$
		\item $A$ is associated with $U$ ($U \rightarrow A$)
		\item $A \leftarrow U \rightarrow Y$ 
		\item confounding for the effect of $A$ on $Y$ 
		\item does not have a causal interpretation unless adjust for confounding (by g-methods or instrumental variable estimation)
		\end{itemize}
	\item intention-to-treat effect (``effectiveness'') \textbf{used for treatment effect}
		\begin{itemize}
		\item $Y^{z} \amalg Z$ even when there is nonrandom noncompliance
		\item ``the effect of assigning participants to being treated with A''
		\item $\frac{Pr[Y=1|Z=1]}{Pr[Y=1|Z=0]}=\frac{Pr[Y^{z=1}]}{Pr[Y^{z=0}]}$
		\item the magnitude of ITT effect depends on:
			\begin{enumerate}
			\item the effect of $A$ on $Y$ ($A \rightarrow Y$)
			\item the effect of $Z$ on $A$ ($Z \rightarrow A$)
			\item the direct effect of $Z$ on $Y$ that is not mediated by $A$ ($Z \rightarrow Y$) (null in double-blind randomized experiment)
			\end{enumerate}
		\item ITT effect can only be computed when there is no loss to follow-up or any censoring
			\begin{itemize}
			\item (alternative) \em pseudo-ITT effect\em : $\frac{Pr[Y=1|Z=1,C=0]}{Pr[Y=1|Z=0,C=0]}$
			\end{itemize}
		\end{itemize}
	\item reasons to use ITT analysis
		\begin{enumerate}
		\item ITT effect is closer to the null than the effect of $A$ on $Y$ in a double-blind placebo-controlled randomized experiment
			\begin{itemize}
			\item ITT effect is the lower bound of the effect of $A$ on $Y$ (conservative)
			\end{itemize}
		\item \textbf{null preservation}: if the sharp causal null hypothesis holds for $A$, the ITT average effect would also be null 
		\end{enumerate}
	\item reporting only the ITT effect implies preference for misclassification bias over confounding, a preference needs to be justified in each application
	\item reporting ITT effect as primary findings from a randomized experiment is hard to justify for
		\begin{enumerate}
		\item experiments that are not double-blind placebo-controlled
		\item study for the effect of a treatment \em safety \em
		\end{enumerate}
	\item caveats for ITT analysis
		\begin{enumerate}
		\item not good for drug safety study (too conservative)
		\item not conservative when both treatments are ``active'' (no placebo was used in the study)
		\item the ITT effect measures the effect of assigned treatment under the adherence conditions observed in a particular experiment
		\item the above argument implies that we should refrain from conducting double-blind randomized clinical trial because , in real life, both doctors and patients are aware of the received treatment
		\end{enumerate}
	\end{itemize}
 
\begin{figure}
	\centering
		\includegraphics[scale=0.8]{BiasSum1.png}
		\caption{Summary of bias definitio, example, and remedy}
		\label{BiasSum1}
\end{figure}
\begin{figure}
	\centering
		\includegraphics[scale=0.8]{BiasSum2.png}
		\caption{Summary of bias definitio, example, and remedy}
		\label{BiasSum2}
\end{figure}
\begin{figure}
	\centering
		\includegraphics[scale=0.8]{BiasSum3.png}
		\caption{Summary of bias definitio, example, and remedy}
		\label{BiasSum3}
\end{figure}
\begin{figure}
	\centering
		\includegraphics[scale=0.8]{BiasSum4.png}
		\caption{Summary of bias definitio, example, and remedy}
		\label{BiasSum4}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Advanced Topics in Causal Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Directed Acyclic Graphs: theory}
\subsection{definition of \textbf{DAG}}
	\begin{itemize}
	\item use DAG to conceptualize problem 
	\item use counterfactual-based methods to analyze data
	\end{itemize}	
	\begin{enumerate}
	\item graphs have nodes with directed edges and no directed cycles
    \item nodes: random variables $V=(V_{1}, ...,V_{M})$
    \item $PA_{m} \ \mbox{are the \em parents \em of} \ V_{m}$: the \em set \em of nodes with arrows into $V_{m}$
    \item $V_{j}$ is a \em descendant \em of $V_{m}$ if there is a sequence of nodes connected by edges between $V_{j} \ \mbox{and}\ V_{m}$
	\end{enumerate}
	
\subsection{definition of \textbf{\em causal \em DAG}}
	\begin{itemize}
	\item causal DAG is a specific type of DAG
	\item a causal DAG is a DAG with following features, which give DAG a causal interpretation:
		\begin{enumerate}
		\item lack of arrow means ``no direct causal effect'' with respect to other variables in the DAG
        \item \em all \em common causes are on the DAG (even unmeasured)
		\end{enumerate}
	\item causal DAG is agnostic to counterfactuals  
	\end{itemize}

\subsection{\textbf{assumption to link DAGs to data}: causal Markov assumption}
	\begin{itemize}
	\item we need an assumption to link \textbf{causal DAG} to \textbf{statistical data}; otherwise, a causal DAG is just a graph of no parctical use
	\item to do so, we first need to define \textbf{DAGs} (all DAGs, not only causal DAGs) in terms of mathematical form
	\item \textbf{definition: a DAG represents the joint probability of all variables in the DAG ($f(\nu)$) if and only if $f(\nu)$ satisfies the Markov factorization}
	$$\displaystyle f(v) = \prod^{M}_{j=1} f(v_{j}|pa_{j})$$
	\item[\-] $f(\nu)$ can be factorized as the product of the probability of each variable given its parents
	\item[\-]= the non-descendents of a given variable is independent of that variable conditional on the variable's parents 
	\item Markov factorization impies additional stasittical indepemdence:
	\item[\-] \textbf{conditional independence} 
	$$Y \amalg A | L \ \mbox{if Y and A is \textbf{d-separated} given L in the DAG}$$
	\item[\-] \textbf{marginal independence} 
	$$Y \amalg A \ \mbox{if Y and A is \textbf{d-separated} without given any other variables in the DAG}$$


	\item d-separation and d-connection
		\begin{itemize}
		\item d-separation
			\begin{enumerate}
			\item d-separation means no open path between $A$ and  $Y$ in the DAG
			\item d-separation between $A$ and $Y$ implies \em independence \em between $A$ and $Y$
				\begin{description}
				\item[marginal independence] d-separated \em without \em conditioning on any $L$
				\item[conditional independence] d-separated \em after \em conditioning on $L$ 
				\end{description}
			\item \textbf{``graphical rules to \em block \em a path''}
				\begin{enumerate}
				\item a \em collider \em on the path $\rightarrow$ block
				\item a \em non-collider \em on the path is \em conditioned on \em $\rightarrow$ block
				\end{enumerate}
			\end{enumerate}
		\item d-connection
			\begin{enumerate}
			\item there is open path between $A$ and $Y$
			\item d-connection between $A$ and $Y$ does \textbf{NOT} always imply \em dependence \em between $A$ and $Y$
			\item need assumption of \textbf{faisthfulness} to say that two variables are dependent if they are d-connected
			\item \textbf{``graphical rules to \em open \em a path''}
				\begin{enumerate}
				\item a \em collider \em on the path is \em conditioned on \em $\rightarrow$ open
				\item a \em non-collider \em on the path $\rightarrow$ open
				\end{enumerate}
			\end{enumerate}
		\end{itemize} 
	\item \textbf{Now, if I want to link my causal DAG represents to my data, what should I do...}
	\item[\-] \textbf{causal Markov assumption}
	\item[\-] The joint distribution of variables in my causal DAG satisfies the Markov factorization
	\end{itemize}	 

\subsection{non parametric structural equation model (NPSEM)}
	\begin{itemize}
	\item NPSEM is a \em causal DAG model \em that includes counterfactuals random variables (e.g. $Y^{a=1}, Y^{a=0}, etc$)
	\item NPSEM combined both counterfactual theory and DAG
	\item notations of NPSEM
        \begin{itemize}
        \item $W$: random variable
        \item \em $\omega$ \em: the set of possible values of $W$ 
        \item $\varpi=(\omega_{1},...\omega_{m})$
        \item $R$ is any subset of $V$; $r$ is a value of $R$
        \item $V^{r}_{m}$ is a counterfactual value of $V_{m}$
        \end{itemize}
	\item{\textbf{for a NPSEM presented by a DAG $G$ with $V$}}
		\begin{itemize}
		\item \textbf{model assumption}
			\begin{enumerate}
			\item \textbf{$\epsilon_{m}$} is the unobserved \textbf{mutually independent} random error
			\item \textbf{$f_{m}=(pa_{m},\epsilon_{m})$} (deterministic unknown function)
				\begin{itemize}
				\item $V_{1}=f_{\epsilon_{1}}$
				\item $V^{\bar{v}_{m-1}}_{m} \equiv V^{pa_{m}}_{m}$
				\item (both given by the function)
				\end{itemize}
			\end{enumerate}		
		\item \textbf{model}
			\begin{itemize} 
			\item $V_{m}$ and $V^{r}_{m}$, for $m>1$ can both be obtained recursively
			\item e.g. $V^{v_{1}}_{3} = V^{v_{1},V^{v_{1}}_{2}}_{3}$ (counterfactual)
			\item e.g. $V^{3}=V^{V_{1},V^{V_{1}}_{2}}_{3}$
			\end{itemize}
		\end{itemize}
		\begin{figure}[H]
		\caption{a DAG represents NPSEM}
		\includegraphics[scale=0.2]{NPSEM.png}
		\vspace{-40pt}
		\label{NPSEM}		
		\end{figure}
	\item the model assumption of NPSEM ($\epsilon_{m}$) can be translated into model assumption of causal DAG (causal Markov assumption)
		\begin{itemize}
		\item \textbf{$\epsilon_{m}$} is the unobserved \textbf{mutually independent} random error which
			\begin{enumerate}
			\item implies causal Markov assumption holds
			\item is equivalent to that all common causes are on the causal DAG
			\end{enumerate}
		\end{itemize}
	\item causal DAG (DAG is agnostic to counterfactual) $>$ NPSEM (incooperates counterfactual)
	\item fully randomized causally interpreted structured tree graph (weaker assumption) $>$ NPSEM (stronger assumption)
	\end{itemize}

\subsection{identifying condition (counterfactual) in causal DAG}
	\begin{description}
	\item[positivity] $L \rightarrow A$; the arrow is not deterministic
	\item[consistency] $A \rightarrow Y$; the arrow is well-defined
	\item[exchangeability] the lack of paths between $A$ and $Y$, other than those originating from $A$, that would result in an association between $A$ and $Y$ (figure~\ref{fig:dag_exch_0})
	\end{description}
		\begin{figure}[h!]
		\centering
			\includegraphics[scale=0.65]{exchangeability_0.png}
			\caption{DAG integrating the counterfactual world  $A=a$}
		\label{fig:dag_exch_0}
		\end{figure}


\section{Direct and Indirect Effects}
\subsection{Non-time varying exposure}
\begin{itemize}
\item in the presence of exchangeability we can calculate the net effect of exposure A on Y
\item to separate direct (not though intermediate B) and indirect effects the assumption of partial exchangeability with respect to B is required for each exposure stratum and the assumption of no interaction (figure 36)\\
$E[Y^{a=1,b=0}|B=1]=E[Y^{a=1}|B=0]$ and $E[Y^{a=0,b=0}|B=1]=E[Y^{a=0}|B=0]$

\begin{itemize}
\item in a randomized trial to be able to seperate direct and indirect effects both the exposure and an intervention on the intermediate have to be randomized
\end{itemize}
\item if \textbf{no interaction} between A and B then sum of the direct and indirect effects should equal the total effect

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.5]{direct_indirect.jpg}
		\caption{Causal pathway of exposure A to outcome Y with intermediate B}
		\label{direct,indirect1}
\end{figure}
\item in the presence of covariates that affect the intermediate and the outcome regardless of asociation with exposure partial exchangeability does not hold
\begin{itemize}
\item\textit{Note:}confounding of the direct effect may be present even if the total effect is unbiased 
\end{itemize}
\item if the covariates are not affected by exposure and data are available then the direct and indirect effects can be estimated by using conventional methods such as stratification (figure 37)
\begin{figure} [H]
	\centering
		\includegraphics[scale=0.4]{direct_indirect2.jpg}
		\caption{Causal pathway of exposure A to outcome Y with intermediate B confounded by U}
		\label{direct,indirect2}
\end{figure}
\item if the covariates are affected by exposure or data are unvailable G-comutation methods must be used (figure 38)
\begin{figure} [H]
	\centering
		\includegraphics[scale=0.4]{direct_indirect3.jpg}
		\caption{Causal pathway of exposure A to outcome Y with intermediate B confounded by L and U}
		\label{direct,indirect3}	
\end{figure}
\item in the presence of interaction between exposure and intermediate the direct and indirect effects will not equal the total effects
\begin{figure} [h!]
	\centering
			\subfigure[No interaction]{
				\includegraphics[scale=.4]{direct_indirect4-1.jpg} 
				\label{direct_indirect4-1}
			} 
			\hspace{1 cm}
			\subfigure[Interaction]{
			\label{direct_indirect4-2}
			\includegraphics[scale=.4]{direct_indirect4-2.jpg} 
			}
		\caption{Without interaction only $B^{a=1}$ is involved in any effect of A(a).In the case of interaction $B^{a}$ interacts with A to cause Y as part of the direct effect of A(b)}
		\label{direct,indirect4}	
\end{figure}
\begin{itemize}
\item depending on the interaction direct and indirect effects will be super or subadditive
\item Indirect effects cannot be estimated but the effect that can be prevented by intervening on the intermediate can\\in the absence of an interaction this and the indirect effects were the same
\end{itemize}
\end{itemize}

\subsection{Time varying exposures}
\begin{itemize}
\item with time varying exposures the direct (controlled) effects of $A_{0}$ on Y are the ones not mediated through $A_{1}$ \\if we do not specify control for $A_{1}$ then effect of $A_{0}$ is total effect.
\begin{itemize} 
\item with dichotomous exposures there are two such exposures: \\the one with $A_{1}=0$ where the effect is the counterfactual difference\\
$E[Y_{\bar{a}={1,0}}] - E[Y_{\bar{a}={0,0}}]= E[Y_{\bar{a}={1,0}}-Y_{\bar{a}={0,0}}]$\\
and the one with $A_{1}=1$ where the effect is the counterfactual difference\\
$E[Y_{\bar{a}={1,1}}] - E[Y_{\bar{a}={0,1}}]= E[Y_{\bar{a}={1,1}}-Y_{\bar{a}={0,1}}]$\\
\textit{Note:}these effects are still joint effects of the time varying exposure $[A_{0},A_{1}]$
\end{itemize}
\item must use methods of causal inference for time varying exposures to calculate these direct effects (\textit{g-methods})

\begin{figure} [H]
 \centering
		\subfigure[]{
				\includegraphics[scale=.5]{varying_confounder.png} 
				\label{varying_confounder}
			} 
			\hspace{1 cm}
			\subfigure[]{
				\includegraphics[scale=.3]{varying_confounder1.png} 
				\label{varying_confounder1}
			} 
		\caption{Time varying exposure: Direct effect of A could be partially through $L_{1}$(b)}
		\label{direct,indirect5}	
\end{figure}

\item we cannot determine whether any direct effect of $A_{0}$ goes through $L_{1}$ without determining causal effect of $L_{1}$ on Y


\item {direct effects in MSMs}
\begin{itemize}
\item consider the marginal structural model $E[Y_{g=(a_{0},a_{1})}]= \theta_{0} + \theta_{1}a_{0} + \theta_{2}a_{1} + \theta_{3}a_{0}a_{1}$ 
\item no effect of $A_{0}$ on Y when $A_{1}=1$ then $\theta_{1} + \theta_{3}=0$
\item no effect of $A_{0}$ on Y when $A_{1}=0$ then $\theta_{1}=0$
\item no effect of $A_{0}$ on Y without intevening on $A_{1}$, then $\theta_{1}=\theta_{2}=\theta_{3}=0$\\ $\theta_{1}$ and $\theta_{3}$ represent direct effect of $A_{0}$, while $\theta_{2}$ direct
\end{itemize}
\item {direct effects in s}
\begin{itemize}
\item consider the structural nested model \\ $Y_{g=(a_{0},a_{1})}=Y_{g=(0,0)}+\beta_{1}a_{0}+\beta_{2}a_{1}+\beta_{3}a_{1}L_{1,g={a_{0}}}+\beta_{4}a_{0}a_{1}+\beta_{5}a_{0}a_{1}L_{1,g={a_{0}}}$
\item no effect of $A_{0}$ on Y when $A_{1}=1$ if $\beta_{1}+\beta_{3}+\beta_{4}+\beta_{5}=0$
\item no effect of $A_{0}$ on Y when $A_{1}=0$ if $\beta_{1}=0$\\
\item no effect of $A_{0}$ on Y without intervening on  $A_{1}$ if all $\beta{s}=0$\\ $\beta_{1}$,$\beta_{3-5}$ represent direct effect of $A_{0}$ while $\beta_{2}$ represents indirect effect
\end{itemize}
\end{itemize}
   

\section{Static \& Dynamic Observation Plans}
Notes from Hernan et al. Stat Meth Med Res. 2008;00:1-26

\subsubsection*{Assumptions}
	\begin{enumerate}
	\item treatment $A_{i,t}$ can only change right after measurement of $L_{i,t}$
	\item i.e. observation times are the only times of potential treatment change
	\end{enumerate}

\subsection{Types of Observation Plans}

\subsubsection*{Static}
	\begin{itemize}
	\item observation plan is pre-specified 
		\begin{itemize}
		\item e.g. Nurses Health Study
		\end{itemize}
	\item at regular intervals
	\end{itemize}

\subsubsection*{Dynamic}
	\begin{itemize}
	\item observation plan varies across individuals
	\item may depend on covariate and / or treatment history
		\begin{itemize}
		\item e.g. clinical history
		\item e.g. patients in pain medication study only returning to clinic when symptomatic
		\end{itemize}
	\item can occur in studies with static plans if individuals miss visits because of clinical history
	\item are generally expected in observational longitudinal studies
	\end{itemize}

\subsection{Static Observation Plans}
	\begin{itemize}
	\item if assigned at random at baseline, do not have to account for observation plan in analysis
	\item true whether one or many static plans	
	\end{itemize}

\subsubsection*{Inverse Probability Weighting in Static Regimes}
	\begin{figure}[h!]
		\begin{center}
		\subfigure[Longitudinal study with a single static observation plan]{\label{figobs1}\includegraphics[scale=0.5]{fig_obs_1.png}} \hspace{2 cm}
		\subfigure[The pseudo-population with a static treatment regime]{\label{figobs2}\includegraphics[scale=0.5]{fig_obs_2.png}} \hspace{2 cm}
		\end{center}
	\caption{IPW without specifying observation plan}
	\label{figobs12}
	\end{figure}

	\begin{itemize}
	\item we estimate numerator and denominator of $SW^A_t$ parametrically
	\item only for times when treatment changes
	
	\begin{align*}
	SW^{A}_t=\prod_{k=0}^t \frac{f(A_k|\bar{A}_{k-1})}{f(A_k|\bar{A}_{k-1}, \bar{L}_k)}
	\end{align*}
	
	\item numerator and denominator = 1 (thus $SW^A_t$ for times when treatment does not change)
	\item this estimator identifies the causal effect if the following asumptions hold
		\begin{enumerate}
		\item \textit{Consistency} $\rightarrow Y^{\bar{a}_t}=Y^{\bar{A}_t}=Y_t \ if \ \bar{A}_{t-1}=\bar{a}_{t-1}$ 
		\item \textit{Exchangeability} $\rightarrow Y^{\bar{a}_k} \amalg A_k|\bar{A}_{k-1},\bar{L}_{k-1} \text{ for all } \bar{a} \ and \ t>k$ 
		\item \textit{Positivity} $\rightarrow f(\bar{a}_{k-1},\bar{L}_k >0 \Longrightarrow f(a_k|\bar{a}_{k-1},\bar{l}_k)>0$
		\item[\-] \textbf{an implicit 4th assumption}
		\item $N_i,t=0 \Longrightarrow A_i,t = A_{i,t-1}$
			\begin{itemize}
			\item $N_{i,t}$ is a time-varying indicator function 
				\begin{align*}
				N_{i,t}=
					\begin{cases}
					0 \text{ if subject } i \text{ not observed at time } t \\
					1 \text{ if subject } i \text{ observed at time } t
					\end{cases}
				\end{align*}
			\item assumption that times of observation and treatment change coincide
			\item only true if subjects have no access to treatment except at study visits and if subjects could not discontinue treatment on their own
			\end{itemize}
		\end{enumerate}
	\end{itemize}

\subsubsection*{When 4th assumption does not hold}
	\begin{itemize}
	\item bias can result
	\item from the fact that a covariate determined treatment time
		\begin{itemize}
		\item usually unmeasured at the true time of change, thus acts as an unmeasured confounder
		\item solution of ''carrying last measurement forward	
			\begin{itemize} 
			\item using $L^*=L_{t-1}$ as proxy for $L$ does not remove bias
			\item IPW weights based on $L*$ do not create pseudo-population in which $L_t \rightarrow A-t$ is removed (as in \textit{figure ~\ref{figobs12}})
			\end{itemize}
			
			\begin{figure}[h!]
			\centering
			\includegraphics[scale=0.5]{fig_obs_3.png}
			\caption{longitudinal study with a single static plan and a carried forward covariate L*}
			\label{figobs3}
			\end{figure}

		\item the ''coarser" the measurement plan, the larger the unmeasured confounding is expected to be
		\end{itemize}
	\end{itemize}
			
\subsubsection*{Single Static Observation Plan}

	\begin{figure}[h!]
		\begin{center}
		\subfigure[Single static observation plan (explicit)]{\label{figobs4}\includegraphics[scale=0.5]{fig_obs_4.png}} \hspace{2 cm}
		\subfigure[Multiple static observation plans]{\label{figobs5}\includegraphics[scale=0.5]{fig_obs_5.png}} \hspace{2 cm}
		\end{center}
	\caption{}
	\label{figobs45}
	\end{figure}

	\begin{itemize}
	\item when 4th assumption holds there is no $\rightarrow N_{i,t}$
		\begin{itemize}
		\item also implicit: observation plan $N_{i,t}$ can only affect $Y_{t+1}$ thorugh treatment $A_t$
		\end{itemize}
	\item thus in reality we always estimate the effect of the joint regime of two components	
		\begin{enumerate}
		\item observation plan $\bar{n}$
		\item treatment regime $\bar{a}$
		\end{enumerate}
	\item IPW with $SW^A_t$ for static regime is implicitly conditional on the observation plan $\bar{n}$
	\item even though it is often ignored, we should \textbf{characterize causal effect according to observation plan}
		\begin{itemize}
		\item for example, for a treatment with a narrow etiological window
		\item[\-] \dots finer observation plan more likely to show causal effect
		\end{itemize}
	\end{itemize}
	
\subsubsection*{Multiple Static Observation Plans}
	\begin{itemize}
	\item subjects randomly assigned to one of several observation plans
	\item analysis conditional on those observed at time $t+1$ because they are the only ones with data on $Y_{t+1}$
		\begin{itemize}
		\item thus conditioning on $N_{t+1}$ is implicit in our analysis
		\item[\-] see \textit{figure ~\ref{figobs45}}
		\end{itemize}	
	\item we can express the joint effect of treatment and observation plan as \dots
		$$ E[Y^{\{ \bar{n},\bar{a=1} \}}_t+1]=E[Y^{\{ \bar{n},\bar{a=0} \}}_t+1] \Longrightarrow g(\bar{a}_t;\beta)$$
	\item if no direct effect of $N_{i,t}$ on $Y_{t+1}$ then $\beta$ is a causal paramater
	\item \textbf{causal effect transportable only to population with same mixture of observation plans}
	\item two solutions: artificial censoring vs. modelling effect of observation plan
	\end{itemize}
	
\subsubsection*{Artificial Censoring for Static Observation Plans}
	\begin{itemize}
	\item re-express our identifiability assumptions to include the observation plan
		\begin{enumerate}
		\item $Y^{\bar{n},\bar{a}}_t=Y^{\bar{N},\bar{A}}_t=Y_t \text{ if } \bar{N}_t=\bar{n}_t \text{ and } \bar{A}_t=\bar{A}_t$
		\item $Y^{\bar{n},\bar{a}}_t \amalg \{ \bar{N}_k,\bar{A}_k,\bar{L}_k\} \text{ for all } \bar{n}, \bar{a} \text{ and } t>k$
		\item $f(\bar{n}_k,\bar{a}_{k-1},\bar{l}_k>0 \Longrightarrow f(n_{k+1},a_k|\bar{n}_{k+1},\bar{a}_{k-1},\bar{l}_k)>0$
		\end{enumerate}
	\item create a psuedo-population that has the mixture of static observation plans we're interested in
	\item censor subjects as soon as they deviate from the plan of interest
	\item no selection bias because observation plans randomized at baseline
	\end{itemize}
	
\subsubsection*{Modelling Effect of Observation Plan}	
	\begin{itemize}
	\item $E[Y^{\bar{a},\bar{n}_{t+1}}|N_{t+1}=1]=g(\bar{a}_t,\bar{n}_t;\beta)$
	\item in the presence of a direct effect of $N_t$ on $Y_t+1$, the model helps 'transport' the effect in one population  with a certain mixture of static plans to another population with a particular observation plan
		\begin{itemize}
		\item more efficient than artifical censoring if few subjects following an observation plan
		\item however, requires parametric assumptions whereas artificial censoring does not
		\item could lead to extrapolation beyond the data
		\end{itemize}
	\item three cases\dots	
		\begin{enumerate}
		\item single or multiple static regimes randomized with equal probability at baseline
			\begin{itemize}
			\item normally would use $SW_t=SW^A \times SW^N$, but is unecessary because $SW^N =1$ for static regimes
				\begin{align*}
				SW^N_t=\prod_{k=1}^{t+1} \frac{f(N_k|\bar{N}_{k-1},\bar{A}_{k-1})}{f(N_k|\bar{N}_{k-1},\bar{A}_{k-1},\bar{L}_{k-1})}
				\end{align*}
			\item however numerator and denominator equal 1 for static regimes that are pre-specified
			\item thus $SW_t=SW^A_t \times SW^N_t = SW^A_t$ !!!
			\end{itemize}
		\item multiple static regimes randomized at baseline with probabilities dependent on baseline covariate $V$
			\begin{itemize}
			\item even though $SW^N \neq 1$ can add $V$ to structural model for outcome and the conditioning event of the numerator of the weights (e.g. $$f(N_k|N_{k-1},A_{k-1},V\dots)$$
			\item these weights for $SW^N$ do equal 1 
			\end{itemize}
		\item observation plans are dynamic (e.g. depend on former treatment but not covariates)
			\begin{itemize}
			\item $SW^N$ still equal to 1 
			\end{itemize}
		\end{enumerate}
	\end{itemize}
		
\subsection{Dynamic Observation Plans}

	\begin{figure}[h!]
	  \begin{center}
		\subfigure[Selection bias due to a dynamic observation plan]{\label{figobs6}\includegraphics[scale=0.5]{fig_obs_6.png}} \hspace{2 cm}
		\subfigure[Confounding due to a dynamic observation plan]{\label{figobs7}\includegraphics[scale=0.5]{fig_obs_7.png}} \hspace{2 cm}
	  \end{center}
	  \caption{}
	  \label{figobs67}
	\end{figure}

	\begin{itemize}
	\item exchangeability:at every time $t$, once can acheive conditional exchangeability between observed and unobserved subjects by using the \#A,L values measured at most recent time each suject was observed
	\item here we do need to take the observation plan $N$ into account in our analysis
	\end{itemize}
	
\subsubsection*{selection bias} (\textit{See figure ~\ref{figobs6}})
	\begin{itemize}
	\item a person's observation time is affected by prior treatment history, and analysis is conditional upon the last observation time
	\item need to use modified weights $SW_t=SW^A_t \times SW^N_t$ to remove $L_t \rightarrow N_{t+1}$ and $L_t \rightarrow A_t$ 
		\begin{itemize}
		\item note that $SW^N_1 \neq 1$
		\end{itemize}
	\item censoring can be seen as an extreme case of this structure
	\end{itemize}
	
\subsubsection*{Confounding} (\textit{See figure ~\ref{figobs7}})
	\begin{itemize}
	\item for this bias $N_i,t$ plays no role so we ignore it and all arrows into it
	\item here $L^*$ is the most recently recorded value of covariates (i.e. confounders) by time $t$
		\begin{itemize}
		\item $L^*$ and not $L$ is what clinicians and patients use to decide treatment intiation/discontinuation
			\begin{align*}
			L^*=
				\begin{cases}
				L^* \text{ if } N_{i,t}=0 \\
				L \text{ if } N_{i,t}=1
				\end{cases}
			\end{align*}
		\item thus $N_{i,t}\rightarrow L^*$ is\dots
			\begin{itemize} 
			\item \em deterministic \em if treatment can only change at observation times
			\item \em predictive \em if treatment can change at times other than observation times
			\end{itemize}
		\end{itemize}	
	\item when treatment can only change at observation times
		\begin{itemize}
		\item using weights $\frac{Pr[A_t|\bar{A}_{t-1}]}{Pr[A_t|\bar{A}_{t-1},\bar{L^*}_{t}]}$
		\item[\-] is implicitly $\frac{Pr[A_t|\bar{A}_{t-1}]}{Pr[A_t|\bar{A}_{t-1},\bar{L^*}_{t},\bar{N}_t]}$
		\item but we have a positivity violation becase $$Pr[A_t=1|A_{t-1}=0,N_{t}=0,\bar{N}_{t-1},\bar{A}_{t-2},L^*_t]=0$$
		\item to solve we must use weights $$SW^A_t \times SW^N_t$$
		\end{itemize}
	\item because $N$ is considered a 'treatment', we require it has no unmeasured confounders as well \\ that is...no $U\rightarrow N_t$ for all $t$
	\end{itemize}

\subsubsection*{Issues with Exchangeability}
	\begin{itemize}
	\item conditional exchangeability holds only if $L^*$ can suffice to block all back-door paths for treatment and observation at every time point
		\begin{itemize}
		\item would hold if at each observation time $N_{t,i}$, $N_{t+1,i}$ is completely determined by $\bar{A}_{k-1}$ and $\bar{L}^*_{k-1}$
		\item e.g. will not hold if patients decide when to be observed based on their covariate values between visits
			\begin{itemize}
			\item these would be unmeasured confounders
			\end{itemize}
		\end{itemize}
	\end{itemize}

\subsubsection*{Artificial Censoring for Dynamic Observation Plans}	
	\begin{itemize}
	\item alternative to using $SW^A \times SW^N$ that adjust for $L^*$
	\item estimate effect within subjects who are always observed (i.e. $\bar{n}=\{1,1,1,\dots1\}$)
		\begin{itemize}
		\item but in dynamic observation plans artificial censoring may be informative
		\end{itemize}
	\item to adjust for this informative censoring, we use weights $SW^{A,D}_t=SW^A_t \times SW^D_t$
		\begin{align*}
		SW^D_t&=\prod_{k=1}^{t+1} \frac{Pr[D_k=0|D_{k-1}=0,\bar{A}_{k-1}]}{Pr[D_{k=0}|D_{k-1}=0,\bar{N}_{k-1},\bar{L}^*_{k-1}]} \\
		where & \dots \\
		D_k &=
			\begin{cases}
			1 \text{ if censored at time } k \\
			0 \text{ otherwise}
			\end{cases}
		\end{align*}
		
		\begin{itemize}
		\item[\-] having $\bar{N}_{k-1}$ to denominator is uneccesary
		\item[\-] because $D_{k-1}=0$ implies $N_m=1$ for all observations $m>k$
		\end{itemize}
	\item thus we fit the MSM
		\begin{itemize}
		\item[\-]
			\begin{align*}
			E\big[ Y_{t+1}^{\bar{a},\bar{d}=0} |D_{t+1}=0 \big] &=g(\bar{a}_t;\beta) \\
			g(\bar{a}_t;\beta) &=\beta_0+\beta_1 \sum_{k=0}^t a_k+\beta_2t+\beta_3t^2
			\end{align*}
			\item[\-] by fitting
				\begin{align*}
				E\big[ Y_{t+1}|{\bar{A},\bar{D}=0} \big] &=g(\bar{a}_t;\gamma) \\
				g(\bar{A}_t;\gamma) &=\gamma_0+\gamma_1 \sum_{k=0}^t A_k+\gamma_2t+\gamma_3t^2 \\
				\end{align*}
			\item[\-] weighted by  $SW^{a,D}_t=SW^A_t \times SW^D_t$
			\end{itemize}
	\item if the observation plan is still dynamic in the artificially censored data, we use 
	$$SW^{A,N,D}_t=SW^A_t \times SW^N_t \times SW^D_t $$
	\item this approach can be generalized to include only periods of follow up \\ 
	where $\bar{n}_{t_k \rightarrow t_k+\delta}=\{1,1,1\dots\}$ \\
	and periods $t_k \rightarrow t_k+\delta$ occur at regular, specified, intervals over follow up
	\item e.g. every 3 months (as opposed to every week of follow up)
	\item see \em Hernan. Stat Methods Med Res 2008; 00:1-26 \em for further details 
	\end{itemize}

\section{Static \& Dynamic Treatment Regimes}

\subsection{Dynamic Regimes}
\begin{itemize}
\item treatment is dependent on $\bar L_k$ and $\bar A_{k-1}$, where $k=0,...,K$
\item can refer to both random and non-random dynamic regimes, but usually refers to non-random dynamic regimes
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{dynamic_regime.png}
	\caption{DAG of a dynamic regime where $A_k$ is determined by $A_{k-1}$ and $L_k$}
	\label{dynamic_regime}
\end{figure}

\subsubsection{Non-random Dynamic Regimes}
\begin{itemize}
\item treatment strategy $g$ where treatment may depend on the evolution of time-dependent covariates $\bar L_t$ and/or treatment $\bar A_{t-1}$ up to time $t$ in a \em deterministic \em manner 
\item simply put, regime looks at values of covariates $L$ and $A$ to determine whether to give current treatment
\item example: take treatment if RBC $>$ 1,000 and if treatment was not taken last week
\item mathematically, $g_k\{\bar a_{k-1},\bar l_k\}$ is a function of $\bar a_{k-1}$ and $\bar l_k$ that determines treatment at time $k$ ($a_k$)
\end{itemize}

\subsubsection{Random Dynamic Regimes}
\begin{itemize}
\item treatment strategy $g$ where treatment may depend on the evolution of time-dependent covariates $\bar L_t$ and/or treatment $\bar A_{t-1}$ up to time $t$ in a \em probabilistic \em manner 
\item example: assign treatment probability to 0.80 if RBC $>$ 1,000 and if treatment was not taken last week, otherwise treatment probability is 0.5
\item random dynamic regimes are \em sequentially randomized experiments \em because treatment $A_k$ depends only on $\bar a_{k-1}$ and $\bar l_k$ in a known probabilistic manner at each time point $k$, thus $Y^g \amalg A_t|\bar L_t=\bar l_t,A_k=g_k\{\bar A_{k-1},\bar L_k\}$
\end{itemize}

\subsubsection{Optimal Treatment Regime}
\begin{itemize}
\item if high values of $E[Y]$ are preferred, then the optimal treatment regime $g$ will maximize $E[Y^g]$
\item should consider static treatment regimes when determining optimal regime
\item should consider non-random dynamic regimes (likely optimal over static regime)
\item should not consider random dynamic regimes for optimal regime because a random strategy should not be preferred as an optimal treatment strategy
\item random dynamic regimes remain scientifically necessary for clinical trials when it is unknown which non-random dynamic regime is optimal (\em equipoise\em)
\end{itemize}

\subsubsection{``Strengthened" Identifiability Conditions}
\begin{itemize}
\item under these strengthened conditions, a non-randomized (deterministic) dynamic treatment regime can be analyzed as a sequentially randomized trial
\item that is ``full" consistency and ``full" exchangeability will be achieved when all parents of the treatment variables $A_m$ are measured (no $U \rightarrow A_m$)
\item ``full" consistency and exchangeability only refer to non-dynamic regimes, but are met when ``strengthened" consistency and exchangeability are achieved in a dynamic regime
\item $SW$ IPTW weights cannot be used with a dynamic regime, only $W$ may be used
\item the g-formula can be used if $\bar a_{k-1}$ is replaced by $\bar g\{\bar l_{k-1}\}$ and $\bar a$ is replaced by $\bar g\{\bar l_K\}$ (assuming only $\bar l_k$ determines $A_k$), however, g-formula suffers from the null paradox and is less robust
\item positivity condition remains unchanged
\end{itemize}

\subsubsection{``Strengthened" Consistency}
\begin{itemize}
\item in addition to $Y^g=Y$, $\bar L^g_k=\bar L_k$, 
\item $\bar L^g_k$ is the counterfactual history of L ($\bar L$) through time $k$ under treatment regime $g$
\item for treatment regime $g$ where $A_k=g_k\{\bar a_{k-1},\bar l_k\}$ (treatment depends on $\bar L$ and past $\bar A$), $Y^g=Y$ and $\bar L^g_k=\bar L_k$
\item for treatment regime $g$ where $A_k=g_k\{\bar l_k\}$ (treatment depends only on $\bar L$), $Y^g=Y$ and $\bar L^g_k=\bar L_k$
\end{itemize}

\subsubsection{``Strengthened" Conditional Exchangeability}
\begin{itemize}
\item in addition to $Y^g\amalg A_t|\bar L_t$, $Y^g\amalg A_t|\bar L_t,\bar A_k$
\item counterfactual outcome $Y^g$ is independent of treatment $A_t$ given both past covariate history and treatment history
\item for treatment regime $g$ where $A_k=g_k\{\bar a_{k-1},\bar l_k\}$,\\ $$Y^g \amalg A_t|\bar L_t=\bar l_t,A_k=g_k\{\bar A_{k-1},\bar L_k\}$$
\item for treatment regime $g$ where $A_k=g_k\{\bar l_k\}$,\\$$Y^g \amalg A_t|\bar L_t=\bar l_t,A_{t-1}=\bar g_{t-1}\{\bar l_{t-1}\}$$
\end{itemize}




\part{Inference without Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Statistical Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypothesis Testing}
	\begin{itemize}
	\item $H_{0}: \ x=a$ vs $H_{1}: x \neq a$
	$$Z^2 = \frac{(x-E[x|H_0])^2}{Var[x|H_0]} \sim \chi^2_1 $$
		\begin{itemize}
		\item $(x-E[x|H_0])^2$ is a function of the magnitude and direction of effect
		\item $Var[x|H_0]$ is a function of statistical power
			\begin{itemize}
			\item power determined by study design, including sample size and balance of numbers of subjects in exposure levels (or case and control groups)
			\item can always try to get more power by study design
			\item thus, statistical significance does not always imply biological significance
			\end{itemize}
		\end{itemize}
	\item p-value: probability that a result as extreme or more extreme than the result we observed would occur due to chance variation, if the null hypothesis were true
	\item \textbf{interpretation of p value:} The probability that these data or any result more extreme would have been obtained if the null hypothesis were true is (plug in the p-value you have)
		\begin{itemize}
		\item ``data'' in the previous interpretation means \em test statistics\em, not effect estimates
		\end{itemize}
	\item p-value gives no information on 
		\begin{itemize}
		\item \em effect magnitude \em 
		\item \em direction \em 
		\item \em range \em of the effect that are consistent with the observed data (confidence interval)
		\item \em statistical power of the study \em
		\end{itemize}
	\end{itemize}
	
\section{Confidence Interval} 
	\begin{itemize}
	\item $x \pm Z_{1-\frac{\alpha }{2}} \sqrt{\hat{Var(x)}}$
	\item if we repeat the experiment many times, $100(1-\alpha)\%$ of the time, it will include the true value of the parameter of interest 
	\item  \textbf{interpretation of 95\% confidence interval:} with 95\% confidence, the true risk ratio is between a 2-fold increase and a 3-fold increase
	\item  \textbf{interpretation of 95\% confidence interval:} These data are consistent with a 2-fold to 3-fold increase in risk ratio of exposure $A$ on outcome $Y$ with 95\% confidence, assuming there is no confounding, no selection bias, and no information bias
	\item confidence interval provides information on power by its range
	\begin{figure}
\centering
\includegraphics[scale=0.3]{pvaluefunc.pdf}
\caption{P-value function plot showing two rate ratios with different precision}
\label{pvaluefunc}
\end{figure}
	\item confidence interval can be used to do hypothesis testing (see figure ~\ref{pvaluefunc})
	\end{itemize}


\section{Sources of Random Variability}
	\begin{itemize}
	\item randomized trial 
		\begin{itemize}
		\item random treatment assignment
		\end{itemize}
	\item observational study
		\begin{itemize}
		\item sampling of subjects who validly represent the exposure-outcome association
		\item conditional randomization by nature
		\end{itemize}
	\item must achieve internal validity of study before statistical inference
	\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Traditional Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{table}[H]
\caption{Test of no exposure-disease association}
$$Z^2=\frac{\big{(}X-\hat{E}(X|H_0)\big{)}^2}{\hat{Var}(X|H_0)} \sim \chi_{1}^2 $$
\centering
	\begin{tabular}{rccl}
	\\
	&\multicolumn{2}{c}{\textbf{closed cohort}}\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{non-cases} & \multicolumn{1}{c|}{$N_1-a$} & \multicolumn{1}{c|}{$N_0-b$} & $M_0$ \\
	\cline{2-3}
	& $N_1$ & $N_0$ & $T$\\
	\\
	&\multicolumn{2}{c}{\textbf{open cohort}}\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{person-time} & \multicolumn{1}{c|}{$N_1$} & \multicolumn{1}{c|}{$N_0$} & $T$ \\
	\cline{2-3}
	\\
	&\multicolumn{2}{c}{\textbf{case-control}}\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{controls} & \multicolumn{1}{c|}{$c$} & \multicolumn{1}{c|}{$d$} & $M_0$ \\
	\cline{2-3}
	& $N_1$ & $N_0$ & $T$\\
	\end{tabular}
	
	\begin{tabular}{c c c c c}
	\\
	\textbf{$H_0$} & \textbf{$X$} & \textbf{$\hat{E}(X|H_0)$} & \textbf{$\hat{Var}(X|H_0)$} & stratified\\ [1ex]
	\hline
	\textbf{closed cohort}\\
	& $a$ & $\frac{N_1M_1}{T}$ & $\frac{M_1M_0N_1N_0}{T^3}$ & unstratified\\
	$C_1=C_0$\\
	& $\sum a_i$ & $\sum \frac{N_{1i}M_{1i}}{T_i}$ & $\sum \frac{M_{1i}M_{0i}N_{1i}N_{0i}}{T^3_i}$ & stratified\\
	\\
	\hline
	\textbf{open cohort }\\
	& $a$ & $\frac{N_1M_1}{T}$ & $\frac{N_1N_0M_1}{T^2}$ & unstratified\\
	$I_1=I_0$ \\
	& $\sum a_i$ & $\sum \frac{N_{1i}M_{1i}}{T_i}$ & $\sum \frac{N_{1i}N_{0i}M_{1i}}{T^2_i}$ & stratified\\
	\\
	\hline
	\textbf{case-control}\\
	& $a$ & $\frac{N_1M_1}{T}$ & $\frac{M_1M_0N_1N_0}{T^2(T-1)}$ & unstratified\\
	$OR=1$ \\
	& $\sum a_i$ & $\sum \frac{N_{1i}M_{1i}}{T_i}$ & $\sum \frac{M_{1i}M_{0i}N_{1i}N_{0i}}{T^2_i(T_i-1)}$ & stratified\\
	\\
	\hline
	\\
	\end{tabular}
\label{e-d_assoc}
\end{table}

\pagebreak

\begin{table}[H]
\caption{Confidence intervals for ratio and difference measures}
$$X \pm Z_{1-\frac{\alpha}{2}} \sqrt{\hat{Var}(X)}$$
\centering
	\begin{tabular}{lccc}
	\\
	& $X$ & $w_i$ & $\hat{Var}(X)$\\
	\hline
	\textbf{closed cohort}\\
	risk difference & $\frac{a}{N_1}-\frac{b}{N_0}$ & -- & $\frac{ac}{N^3_1}+\frac{bd}{N^3_0}$\\
	\\
	summary risk difference & $\frac{\sum w_i \left(\frac{a_i}{N_{1i}}-\frac{b_i}{N_{0i}} \right)}{\sum w_i}$ & $\frac{N^3_{1i}N^3_{0i}}{N^3_{0i}a_ic_i+N^3_{1i}b_id_i}$ & $\frac{1}{\sum w_i}$ \\
	\\
	risk ratio (ln) & ln$\left(\frac{\frac{a}{N_1}}{\frac{b}{N_0}}\right)$ & -- & $\frac{c}{aN_1}+\frac{d}{bN_0}$ \\
	\\
	summary risk ratio (ln) & ln$\left(\frac{\sum w_i \frac{\frac{a_i}{N_{1i}}}{\frac{b_i}{N_{0i}}}}{\sum w_i}\right)$ & $\frac{b_iN_{1i}}{T_i}$ & $\frac{\frac{\sum( M_{1i}N_{1i}N_{0i}-a_ib_iT_i)}{T^2_i}}{\left( \sum \frac{a_iN_{0i}}{T_i}\right) \left( \sum \frac{b_iN_{1i}}{T_i} \right)}$\\
	\\
	\hline
	\textbf{open cohort}\\
	rate difference & $\frac{a}{N_1}-\frac{b}{N_0}$ & -- & $\frac{a}{N^2_1}+\frac{b}{N^2_0}$\\
	\\
	summary rate difference & $\frac{\sum w_i \left(\frac{a_i}{N_{1i}}-\frac{b_i}{N_{0i}} \right)}{\sum w_i}$ & $\frac{N^2_{1i}N^2_{0i}}{a_iN^2_{0i}+b_iN^2_{1i}}$ & $\frac{1}{\sum w_i}$ \\
	\\
	rate ratio (ln) & ln$\left(\frac{\frac{a}{N_1}}{\frac{b}{N_0}}\right)$ & -- & $\frac{1}{a}+\frac{1}{b}$ \\
	\\
	summary rate ratio (ln) & ln$\left(\frac{\sum w_i \frac{\frac{a_i}{N_{1i}}}{\frac{b_i}{N_{0i}}}}{\sum w_i}\right)$ & $\frac{b_iN_{1i}}{T_i}$ & $\frac{\frac{\sum( M_{1i}N_{1i}N_{0i})}{T^2_i}}{\left( \sum \frac{a_iN_{0i}}{T_i}\right) \left( \sum \frac{b_iN_{1i}}{T_i} \right)}$\\
	\\
	\hline
	\textbf{case-control}\\
	odds ratio (ln) & ln$\left(\frac{ad}{bc}\right)$ & -- & $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}$ \\
	\\
	summary odds ratio (ln) & ln$\left(\frac{\sum w_i \frac{a_id_i}{b_ic_i}}{\sum w_i}\right)$ & $\frac{b_ic_i}{T_i}$ & RGB variance\\
	\\
	\hline
	
	\end{tabular}
\label{conf_int}
\end{table}
\pagebreak

\begin{table}[H]
\caption{Estimate of the RBG variance}

\begin{tabular}{c c c c}
\\
&\multicolumn{2}{c}{\textbf{case-control}}\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{controls} & \multicolumn{1}{c|}{$c$} & \multicolumn{1}{c|}{$d$} & $M_0$ \\
	\cline{2-3}
	& $N_1$ & $N_0$ & $T$\\
\end{tabular}

\begin{tabular}{rl}
\\[1em]
\multicolumn{2}{l}{For the variance of the natural log of the Mantel-Haenszel summary odds ratio we use}\\
\multicolumn{2}{l}{the RGB variance.  To compute the $\hat{var}\big(\ln(\hat{OR}_{MH})\big)$ the following formulas are used:}\\[1em]

$A_i=$ & $\frac{a_id_i}{T_i}$\\[1em]
$B_i=$ & $\frac{b_ic_i}{T_i}$\\[1em]
$C_i=$ & $\frac{a_i+d_i}{T_i}$\\[1em]
$D_i=$ & $\frac{b_i+c_i}{T_i}$\\[2em]

\multicolumn{2}{l}{The derived quantities $A_i, B_i, C_i$, and $D_i$ are combined and summed by stratum:}\\[1em]

$(AC)=$ & $\sum A_iC_i$\\[1em]
$(AD)=$ & $\sum A_iD_i$\\[1em]
$(BC)=$ & $\sum B_iC_i$\\[1em]
$(BD)=$ & $\sum B_iD_i$\\[2em]

\multicolumn{2}{l}{With this notation the $\hat{OR}_{MH}$ can be written as:}\\[1em]

	$\hat{OR}_{MH}=$ & $\frac{\sum A_i}{\sum B_i}$\\[1em]
	$=$ & $\frac{\sum \frac{a_id_i}{T_i}}{\sum \frac{b_ic_i}{T_i}}$\\[2.5em]

\multicolumn{2}{l}{And $\hat{var}\big(\ln(\hat{OR}_{MH})\big)$ can be written as:}\\[1em]

$\hat{var}\big(\ln(\hat{OR}_{MH})\big)=$ & $\frac{1}{2}\left(\frac{(AC)}{A^2}+\frac{(AD)+(BC)}{A\times B}+\frac{(BD)}{B^2} \right)$\\[1em]
$=$ & $\frac{1}{2} \left( \frac{\sum \left( \frac{a_id_i}{T_i} \right) \left( \frac{a_i+d_i}{T_i} \right)}{\left( \sum \frac{a_id_i}{T_i} \right)^2}+\frac{\sum \left( \frac{a_id_i}{T_i} \right) \left( \frac{b_i+c_i}{T_i}\right) + \sum \left( \frac{b_ic_i}{T_i} \right) \left( \frac{a_i+d_i}{T_i} \right)}{\left( \sum \frac{a_id_i}{T_i} \right) \left( \sum \frac{b_ic_i}{T_i} \right)}+\frac{\sum \left( \frac{b_ic_i}{T_i} \right) \left( \frac{b_i+c_i}{T_i} \right)}{\left( \sum \frac{b_ic_i}{T_i} \right)^2}\right)$ \\
\end{tabular}
\label{rgb_var}
\end{table}
\pagebreak

\section{Road Map}
\begin{itemize}
\item Mantel-Haenszel test statistics $\sim \chi^2_1$
	\begin{itemize}
	\item $H_{0}: \hat RR_{MH}=1$ or $\hat RD_{MH}=0$ or ``there is no association between exposure and outcome''
	\end{itemize}
\item Homogeneity test statistics $\sim \chi^2_{I-1}$ where I is the number of levels of stratification factor
	\begin{itemize}
	\item $H_{0}: \hat RR_{i}=RR_{j}$ for all $i,j$ or ``all stratum specific effect estimates are the same''
	\end{itemize}
\item stratum specific weights differ for absolute and relative measures of effect
	\begin{itemize}
	\item \em risk/rate difference\em : inverse variance weights are most efficient ($\frac{1}{\hat{var}(\hat{RD})}$)
	\item \em risk/rate/odds ratio\em : Mantel-Haenszel weights are most efficient ($\frac{b_iN_{1i}}{T_i}$ or $\frac{b_ic_i}{T_i}$)
	\end{itemize}
\end{itemize}
\subsection{inference about hypothesis testing}
\subsubsection{count data}
	\begin{itemize}
	\item count data refers to 
		\begin{enumerate}
		\item number of cases and non-cases collected in a close cohort study
		\item number of cases and controls collected in a case-control study
		\end{enumerate}	
	\item for count data, the 2 numbers in the $4$ cells of the contingency table can be random variables 
		\begin{itemize}
		\item $2$ of $a,(N_{1}-a),b,(N_{0}-b)$ for close cohort study 
		\item $2$ of $a,b,c,d$ for case-control study
		\item see closed cohort study and case-control study part of table \ref{e-d_assoc}
		\end{itemize}	
	\item however, it's difficult to make inference about $2$ random variables at the same time
	\item thus, we pretend that we know the margin numbers of the table before we even collect the data
		\begin{itemize}
		\item for close cohort study, we only know how many exposed subjects ($N_{1}$) and unexposed subjects ($N_{0}$) will be recruited into the study
		\item for case-control study, we only know how many cases ($M_{1}$) and controls ($M_{0}$) will be recruited into the study
		\end{itemize}
	\item that is, we fix the margins of the contingency table ($M_{0},M_{1},N_{0},N_{1}$)
	\item \textbf{key assumption:}
	\item[\-] by fixing the margins, only one random variable is left for inference \\
	$\Rightarrow$ this random variable follows \textbf{hypergeometric} distribution
	\item hypothesis test for random variable follows hypergeometric distribution
		\begin{itemize}
		\item random variable: $a$
		\item from hypergeometric distribution, we can get expected value and variance of $a$		
		\item[\-] $E[a|H_{0}]=\frac{N_1M_1}{T} $
		\item[\-] $Var[a|H_{0}]=\frac{M_1M_0N_1N_0}{T^2(T-1)}$ 
		\item construct test statistics:
			\begin{itemize}
			\item[\-] $Z=\frac{a-E[a|H_{0}]}{\sqrt{Var[a|H_{0}]}} \ \mbox{follows noraml distribution asymptotically}$
			\item[\-] $\frac{(a-E[a|H_{0}])^{2}}{Var[a|H_{0}]} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\item[\-] $\dfrac{(a-\frac{N_1M_1}{T})^{2}}{\frac{M_1M_0N_1N_0}{T^2(T-1)}} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\end{itemize}
		\item for closed cohort data, use $Var[a|H_{0}]=\frac{M_1M_0N_1N_0}{T^3}$ for continuity correction  
		\end{itemize}
	\end{itemize}
	
\subsubsection{person-time data}
	\begin{itemize}
	\item person-time data refers to 
		\begin{enumerate}
		\item number of cases (counts) and person-times collected in a close cohort study 
		\item or in a open cohort study
		\end{enumerate}	
	\item for person-time data, the exposed and unexposed person-times are only ``units'' used to standardize the counts
		\begin{itemize}
		\item we may arbitrarily change the person-time unit for a incidence rate 
		\item e.g. 1 per 1000 person-year $=$ 10 per 10,000 person-year
		\item however, we need the same unit to compare counts in person-time data 
		\item e.g. 1 case is not less than 10 cases in the sense that the former 1 case aroused from 1000 person-year exposed person-time and the later 10 cases aroused from 10,000 person-year exposed person-time
		\end{itemize}	
	\item because person-times are only ``units'' but not random variables, we do not use them in hypothesis test
	\item we only use counts as random variables in person-time data, which are the numbers of exposed cases ($a$) and unexposed cases ($b$)
		\begin{itemize}
		\item see open cohort study part of table \ref{e-d_assoc}
		\end{itemize}
	\item \textbf{key assumption}
	\item[\-] we pretend that we know the total number of cases in the study but we do not know how many exposed cases will be there
	\item that is, $a$ is a random variable follows \textbf{binomial} distribution with the total number of trials is $\textbf{(a+b)}$ 
		\begin{itemize}
		\item $Binomial(n,p)$: $n$ is the total number of trials and $p$ is the probability of successful trials
		\end{itemize}
	\item hypothesis test for random variable follows \textbf{binomial} distribution
		\begin{itemize}
		\item random variable: $a$
		\item from binomial distribution, we can get expected value and variance of $a$		
		\item[\-] $E[a|H_{0}]=\frac{N_1M_1}{T} $
		\item[\-] $Var[a|H_{0}]=\frac{M_1N_1}{T^{2}}$ 
		\item construct test statistics:
			\begin{itemize}
			\item[\-] $Z=\frac{a-E[a|H_{0}]}{\sqrt{Var[a|H_{0}]}} \ \mbox{follows noraml distribution asymptotically}$
			\item[\-] $\frac{(a-E[a|H_{0}])^{2}}{Var[a|H_{0}]} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\item[\-] $\dfrac{(a-\frac{N_1M_1}{T})^{2}}{\frac{M_1N_1}{T^2}} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\end{itemize}
		\end{itemize}
	\end{itemize}	

\subsubsection{Mantel-Haenszel test}
\begin{itemize}
\item once we have the two tests constructed above, it's easy to construct M-H test
\item essentially $\dots$
$$\frac{(\sum_{i}a_{i}-\sum_{i}E[a_{i}|H_{0}])^{2}}{\sum_{i}Var[a_{i}|H_{0}]} \sim \chi^{2}_{1} \ \mbox{asymptotically}$$
\item use the expected values and variances correspond to the data type
\end{itemize}

\subsubsection{use risk difference to perform hypothesis test for count data}
	\begin{itemize}
	\item we may also use a regular paired t-test 
	\item we compared risk among the exposed to the risk among the unexposed
	\item because risks are probabilities, we take the two risks as probabilities of success in two binomial distributed random variables and compare them
	\item that is $\dots$
		\begin{itemize}
		\item random variable $X=(\frac{a}{N_{1}}-\frac{b}{N_{0}})$
		\item $X$ has $E[X|H_{0}]=0$ and $Var[X|H_{0}]=\frac{M_{0}}{T} \frac{M_{1}}{T} (\frac{1}{N_{1}}+\frac{1}{N_{0}})$
		\item construct test statistics:
			\begin{itemize}
			\item[\-] $t=\frac{X-E[X|H_{0}]}{\sqrt{Var[X|H_{0}]}}$
			\item[\-] $t \ \ \mbox{follows noraml distribution asymptotically (when n is greater than 30...)}$
			\item[\-] $\frac{(X-E[X|H_{0}])^{2}}{Var[X|H_{0}]} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\item[\-] $\dfrac{(\frac{a}{N_{1}}-\frac{b}{N_{0}})^{2}}{\frac{M_{0}}{T} \frac{M_{1}}{T} (\frac{1}{N_{1}}+\frac{1}{N_{0}})} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\end{itemize}
		\end{itemize}
	\end{itemize}

\subsubsection{use rate difference to perform hypothesis test for person-time data}
	\begin{itemize}
	\item we may also use a regular paired t-test 
	\item we compared rate among the exposed to the rate among the unexposed
	\item because rates are parameters of Poisson distributed random variables, we take the two rates as parameters of two Poisson distributions and compare them
	\item that is $\dots$
		\begin{itemize}
		\item random variable $X=(\frac{a}{N_{1}}-\frac{b}{N_{0}})$
		\item $X$ has $E[X|H_{0}]=0$ and $Var[X|H_{0}]=\frac{M_{1}}{T} (\frac{1}{N_{1}}+\frac{1}{N_{0}})$
		\item construct test statistics:
			\begin{itemize}
			\item[\-] $t=\frac{X-E[X|H_{0}]}{\sqrt{Var[X|H_{0}]}}$
			\item[\-] $t \ \mbox{follows normal distribution asymptotically (when n is greater than 30...)}$
			\item[\-] $\frac{(X-E[X|H_{0}])^{2}}{Var[X|H_{0}]} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\item[\-] $\dfrac{(\frac{a}{N_{1}}-\frac{b}{N_{0}})^{2}}{\frac{M_{1}}{T} (\frac{1}{N_{1}}+\frac{1}{N_{0}})} \sim \chi^{2}_{1} \ \mbox{asymptotically}$
			\end{itemize}
		\end{itemize}
	\end{itemize}

\subsection{inference about $95\%$ confidence interval}
\begin{itemize}
\item $100 \times (1-\alpha)\%$ confidence interval for a variable follows normal distribution is $\hat{X} \pm Z_{1-\frac{\alpha}{2}}\sqrt{Var(\hat{X})}$
\item when $X$ is a parameter (e.g. rate ratio), X usually follows normal distribution
	\begin{itemize}
	\item Central limit theorem: the summation of values of a random variable following any distribution ($\sum_{i} X_{i}$) would follows normal distribution
	\end{itemize}
\item the formula for $95\%$ confidence interval of a a parameter estimate is $\hat{X} \pm 1.96 \times se(\hat{X})$
	\begin{itemize}
	\item $se(\hat{X})=\sqrt{Var(\hat{X})}$ given $\hat{X}$ is a parameter estimate
	\end{itemize}
\item therefore, the inference of confidence interval relies entirely on $Var(\hat{X})$
\item the following part is just to show how to get variance for different parameters
	\begin{itemize}
	\item plug-in  $se(\hat{X})$ into the formula given above to get $95\%$ confidence interval
	\end{itemize}
\item for ratios, we \textbf{log-transform} the ratio first and then exponentiate it back to original scale after getting the confidence interval for $log(ratio)$
	\begin{itemize}
	\item $e^{log(\hat{X}) \pm 1.96 \times se(log(\hat{X}))}$
	\item this is purely a technical issue (it is much easier to deal with $Var(log(x)-log(y))$ than $Var(\frac{x}{y})$)
	\end{itemize}
\end{itemize}

\subsubsection{Odds ratio (OR)}
\begin{itemize}
\item see table \ref{e-d_assoc}
\item $OR=\frac{ad}{bc}$
\item $log(OR)=log(a)+log(d)-log(b)-log(c)$
\item $Var(log(OR))=Var[log(a)+log(d)-log(b)-log(c)]$
\item[\-] $Var(X+Y)=Var(X)+Var(Y)$ and $Var(X-Y)=Var(X)+Var(Y)$ given $X$ and $Y$ are independent from each other
\item $Var(log(OR))=Var[log(a)]+Var[log(b)]+Var[log(c)]+Var[log(d)]$
\item[\-] $Var[X]=E[X]$ given $X$ is a count (Poisson distribution) 
\item[\-] $Var[log(X)]=(\frac{1}{X})^{2}Var(X)=(\frac{1}{X})^{2}(X)=\frac{1}{X}$ (delta method)
\item $Var(log(OR))=\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}$
\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 
\end{itemize}

\pagebreak
\begin{itemize}
\item[\-] \textbf{a different way to get the variance of odds ratio}
\item this method is based on assuming binomial distribution, which is closer to a actual data structure
\item the previous one is based assuming Poisson distribution for each count, which is not actually the case
\begin{eqnarray*}
Var(log(OR)) &=& Var(log(\frac{a/b}{c/d})) \\
&=& Var(log(\frac{a}{b})-log(\frac{c}{d})) \\
&=& Var(log(\frac{a}{b}))+Var(log(\frac{c}{d})) \\
&=& Var(log(\frac{a/M_{1}}{b/M_{1}})+Var(log(\frac{c/M_{0}}{d/M_{0}})) \\
&=& Var(log(\frac{\hat{P_{1}}}{1-\hat{P_{1}}}))+Var(log(\frac{\hat{P_{0}}}{1-\hat{P_{0}}})) \\
&&\mbox{set $\frac{a}{M_{1}}=P_{1}$ and $\frac{c}{M_{0}}=P_{0}$, both are parameters of binomial distribution}\\
&=& \big(\frac{1}{P_{1}}-\frac{-1}{1-P_{1}}\big)^{2}Var(\hat{P_{1}})  +\big(\frac{1}{P_{0}}-\frac{-1}{1-P_{0}}\big)^{2}Var(\hat{P_{0}})  \\
&& Var(log(\frac{\hat{P}}{1-\hat{P}}) = Var(log(P)-log(1-P)) = (\frac{1}{P}-\frac{1}{1-P})^{2}Var(\hat{P})\\
&=& \big(\frac{1}{P_{1}(1-P_{1})}\big)^{2}Var(\hat{P_{1}})  +\big(\frac{1}{P_{0}(1-P_{0})}\big)^{2}Var(\hat{P_{0}})  \\
&=& \big(\frac{1}{P_{1}(1-P_{1})}\big)^{2}\frac{\hat{P_{1}}(1-\hat{P_{1}})}{M_{1}}  + \big(\frac{1}{P_{0}(1-P_{0})}\big)^{2}\frac{\hat{P_{0}}(1-\hat{P_{0}})}{M_{0}}  \\
&=& \big(\frac{1}{P_{1}(1-P_{1})}\big)\frac{1}{M_{1}}  + \big(\frac{1}{P_{0}(1-P_{0})}\big)\frac{1}{M_{0}}  \\
&=&  \big(\frac{1}{\frac{a}{M_{1}}(1-\frac{a}{M_{1}})}\big)\frac{1}{M_{1}}  + \big(\frac{1}{\frac{c}{M_{0}}(1-\frac{c}{M_{0}})}\big)\frac{1}{M_{0}}  \\
&=&  \big(\frac{1}{\frac{a}{M_{1}}\frac{b}{M_{1}}}\big)\frac{1}{M_{1}}  + \big(\frac{1}{\frac{c}{M_{0}}\frac{d}{M_{0}}}\big)\frac{1}{M_{0}}  \\
&=&  \big(\frac{1}{\frac{ab}{M_{1}}}\big) + \big(\frac{1}{\frac{cd}{M_{0}}}\big)   \\
&=&  \frac{M_{1}}{ab}+ \frac{M_{0}}{cd}   \\
&=&  \frac{a+b}{ab}+ \frac{c+d}{cd}   \\
&=&  \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d} 
\end{eqnarray*}
\item this method requires more algebra and is confusing \dots
\end{itemize}

\subsubsection{rate ratio}
\begin{itemize}
\item see table \ref{e-d_assoc}
\item $IRR=\frac{a/N_{1}}{b/N_{0}}$
\item $log(IRR)=log(\frac{a}{N_{1}})-log(\frac{b}{N_{0}})$
\item $Var(log(IRR))=Var(log(\frac{a}{N_{1}})-log(\frac{b}{N_{0}}))=Var(log(\frac{a}{N_{1}}))+Var(log(\frac{b}{N_{0}}))$
\item $Var(log(IRR))=Var(log(\frac{a}{N_{1}}))+Var(log(\frac{b}{N_{0}}))=(\frac{N_{1}}{a})^{2}Var(\frac{a}{N_{1}})+(\frac{N_{0}}{b})^{2}Var(\frac{b}{N_{0}})$
\item[\-] by delta method

\item $Var(log(IRR))=(\frac{N_{1}}{a})^{2}Var(\frac{a}{N_{1}})+(\frac{N_{0}}{b})^{2}Var(\frac{b}{N_{0}})
=(\frac{N_{1}}{a})^{2}(\frac{1}{N_{1}})^{2}Var(a)+(\frac{N_{0}}{b})^{2}(\frac{1}{N_{0}})^{2}Var(b)$
\item[\-] $N_{1}$ and $N_{0}$ are person-time units used to standardize the study bases from which the counts are from; therefore, $N_{1}$ and $N_{0}$ are considered as constants, not random variables (random variables are $a$ and $b$)
\item[\-] $Var(kX)=k^{2}Var(X)$ given that $k$ is a constant
\item $Var(log(IRR))=(\frac{N_{1}}{a})^{2}(\frac{1}{N_{1}})^{2}Var(a)+(\frac{N_{0}}{b})^{2}(\frac{1}{N_{0}})^{2}Var(b)=(\frac{N_{1}}{a})^{2}(\frac{1}{N_{1}})^{2}(a)+(\frac{N_{0}}{b})^{2}(\frac{1}{N_{0}})^{2}(b)$
\item[\-] $Var(X)=X$ for a count
\item \textbf{$Var(log(IRR))=\frac{1}{a}+\frac{1}{b}$}
\end{itemize}

\subsubsection{risk ratio}
\begin{itemize}
\item see table \ref{e-d_assoc}
\item $RR=\frac{a/N_{1}}{b/N_{0}}$
\item $log(RR)=log(\frac{a}{N_{1}})-log(\frac{b}{N_{0}})$
\item $Var(log(RR))=Var(log(\frac{a}{N_{1}})-log(\frac{b}{N_{0}}))=Var(log(\frac{a}{N_{1}}))+Var(log(\frac{b}{N_{0}}))$
\item $Var(log(RR))=Var(log(\frac{a}{N_{1}}))+Var(log(\frac{b}{N_{0}}))=(\frac{N_{1}}{a})^{2}Var(\frac{a}{N_{1}})+(\frac{N_{0}}{b})^{2}Var(\frac{b}{N_{0}})$
\item[\-] by delta method
\item \begin{eqnarray*}
Var(log(RR))&=&(\frac{N_{1}}{a})^{2}Var(\frac{a}{N_{1}})+(\frac{N_{0}}{b})^{2}Var(\frac{b}{N_{0}})\\
&=&(\frac{N_{1}}{a})^{2}(\frac{a}{N_{1}})(1-\frac{a}{N_{1}})(\frac{1}{N_{1}})+(\frac{N_{0}}{b})^{2}(\frac{b}{N_{0}})(1-\frac{b}{N_{0}})(\frac{1}{N_{0}})\\
&=&(\frac{N_{1}}{a})^{2}(\frac{a}{N_{1}})(\frac{N_{1}-a}{N_{1}})(\frac{1}{N_{1}})+(\frac{N_{0}}{b})^{2}(\frac{b}{N_{0}})(\frac{N_{0}-b}{N_{0}})(\frac{1}{N_{0}})\\
&=&\frac{N_{1}-a}{aN_{1}}+\frac{N_{0}-b}{bN_{0}}\\
\end{eqnarray*}
\item[\-] $Var(\hat{p})=\frac{\hat{p}(1-\hat{p})}{n}$ for $\hat{p}$ is the parameter for a binomial distribution
\item $Var(log(RR))=\frac{N_{1}-a}{aN_{1}}+\frac{N_{0}-b}{bN_{0}}$
\end{itemize}

\subsubsection{rate difference}
\begin{itemize}
\item see table \ref{e-d_assoc}
\item $IRD=\frac{a}{N_{1}}-\frac{c}{N_{0}}$
\item $Var(IRD)=Var \big( \frac{a}{N_{1}}-\frac{c}{N_{0}} \big) = Var \big( \frac{a}{N_{1}}\big)+Var\big(\frac{c}{N_{0}} \big) = (\frac{1}{N_{1}})^{2}Var(a)+(\frac{1}{N_{0}})^{2}Var(c)$ 
\item[\-] $Var(kX)=k^{2}Var(X)$ given that $k$ is a constant
\item $Var(IRD)=(\frac{1}{N_{1}})^{2}Var(a)+(\frac{1}{N_{0}})^{2}Var(c)=(\frac{1}{N_{1}})^{2}(a)+(\frac{1}{N_{0}})^{2}(c)$ 
\item[\-] $Var[X]=E[X]$ given $X$ is a count (Poisson distribution) 
\item $Var(IRD)=\frac{a}{N_{1}^{2}}+\frac{c}{N_{0}^{2}}$ 
\end{itemize}

\subsubsection{risk difference}
\begin{itemize}
\item see table \ref{e-d_assoc}
\item $RD=\frac{a}{N_{1}}-\frac{c}{N_{0}}$
\item $Var(RD)=Var \big( \frac{a}{N_{1}}-\frac{c}{N_{0}} \big) = Var \big( \frac{a}{N_{1}}\big)+Var\big(\frac{c}{N_{0}} \big)$
\item $Var(RD)=Var \big( \frac{a}{N_{1}}\big)+Var\big(\frac{c}{N_{0}} \big)=\frac{a}{N_{1}}(1-\frac{a}{N_{1}})(\frac{1}{N_{1}})+\frac{c}{N_{0}}(1-\frac{c}{N_{0}})(\frac{1}{N_{0}})$
\item[\-] $\frac{a}{N_{1}}$ and $\frac{c}{N_{0}}$, both are parameters of binomial distribution
\item[\-] $Var(\hat{p})=\frac{\hat{p}(1-\hat{p})}{n}$ for $\hat{p}$ is the parameter for a binomial distribution
\item $Var(RD)=\frac{a}{N_{1}}\frac{b}{N_{1}}\frac{1}{N_{1}}+\frac{c}{N_{0}}\frac{d}{N_{0}}\frac{1}{N_{0}}$
\item $Var(RD)=\frac{ab}{N_{1}^{3}}+\frac{cd}{N_{0}^{3}}$
\end{itemize}

\section{Stratification}
\begin{itemize} 
\item marginal (unconditional) causal effect measures: causal effect for the entire population, $Pr[Y^{a=1}=1]$ vs $Pr[Y^{a=0}=1]$
\item conditional causal effect measures: causal effect for a subset of the population, $Pr[Y^{a=1}=1|M=m]$ vs $Pr[Y^{a=0}=1|M=m]$
    \begin{itemize}
    \item if the stratum-specific effect measures are all the same, the effect measure for a entire population would be the same as the 
stratum-specific effect measure
    \end{itemize}
\item collapsibility: population effect measure can be expressed as a weighted average of the stratum specific measures
    \begin{itemize}
    \item risk ratio and risk difference are collapsible
    \end{itemize}
\item non-collapsibility
    \begin{itemize}
    \item odds ratio and odds difference are non-collapsible
    \end{itemize}
\item when the stratum-defining variable is associated with outcome, the population odds ratio can be closer to the null value than any of 
the non-null stratum specific causal odds ratios
\item under the sharp null hypothesis, all effect measures are collapsible
\item restriction: form of stratification where investigator only looks at certain strata of $L$
    \begin{itemize}   
    \item can be used to preserve positivity
    \end{itemize}
\item can be used to compute average causal effects in a subset of the population, but not individual effects
\item pooled stratum-specific effect measures
    \begin{itemize}
    \item only when there is no effect measure modification 
    \item reduce variability of the estimate
    \end{itemize}	
\end{itemize}



\section{Matching}
\begin{itemize}
\item selection of a reference series that is identical to the index series \em with respect to the distribution of one or more potential confounder \em
\item chiefly applied in case-control studies
\end{itemize}

\subsection{The Matching Process}
    \begin{itemize}
    \item matching factor: factor $L$ by which exposed and unexposed in a prospective cohort study are matched
        \begin{itemize}
        \item mind that matching factor (and many other factors) will change over time
		\item desirable characteristics of matching factors
			\begin{enumerate}
			\item easily assessed in entire population
			\item not interested in its effect
			\item meet positivity assumption (strata with only treated, or untreated, individuals are excluded from the analysis)
			\item are associated with outcome
			\item not so tied up with exposure that all members of matched sets (both cases and controls) have the same value for exposure
			\end{enumerate}
        \end{itemize}
	\item subjects are matched $1:N$ (or $N:N$) to create a subset of \em matched subjects \em where the the variables $L$ are forced, \em by design \em, to follow the same distribution in both the treated and untreated
		\begin{enumerate}
		\item \em individual matching\em : more than 4 controls per case would result in little gain in power
			\begin{itemize}
			\item \em relative efficiency \em (RE) $=\frac{m}{m+1}$, where $m$ is the number of matched controls per case
			\end{itemize}
		\item \em frequency matching\em : may have residual confounding after frequency matching on continuous variable
			\begin{itemize}
			\item ex: deciding to have 50 \% males and 50 \% females in both the exposed and unexposed groups
			\item can be used to created a matched population that follows \em any \em arbitrary distribution of $L$ (i.e. 10/20, 50/50, 70/30, etc.)
			\end{itemize}
		\end{enumerate}		
	\end{itemize}	

\subsection*{Matching, Confounding, \& Efficiency}	
	\begin{itemize}
    \item in case-control study matching will only improve the statistical efficiency in the subsequent stratified analysis but will \em not 
\em remove confounding
    \item if to control for confounding only through matching
		\begin{itemize}
		\item must assume conditional exchangeability given matching factors $L$
		\item in the subset of matched pairs, the treated and untreated are marginally exchangeable
		\end{itemize}
	\item if matching for efficiency	
        \begin{itemize}
        \item not all covariates needed to achieve exchangeability are included
        \end{itemize} 
	\item usually always want to adjust for matching factor in analysis because it usually imposes a selection bias (except in the case of over-matching)
		\begin{itemize}
		\item match for efficiency only: $A\rightarrow[S]\leftarrow M\rightarrow Y$
		\item match for confounding: add arrow $M\rightarrow A$
		\item overmatching: $A\rightarrow[S]\leftarrow M \quad \quad Y$
		\end{itemize}	
   	\item matching can be used to create a matched population with \em any \em arbitrary distribution of $L$ in both the treated and the untreated
        \begin{itemize}
        \item matching can be seen as ``standardization by design"
        \end{itemize}
   \item the average causal effect in the matched population may not be the same as the average causal effect in the population
		\begin{itemize}
		 \item causal effects computed by matching are \em conditional \em because they apply to a subset of the population with a particular distribution of covariates $L$
		 \end{itemize} 
	\end{itemize} 
 
\subsection{Matching Strategies}
\begin{itemize} 
\item there are several different matching strategies
    \begin{itemize}
    \item effect in the untreated: match to make the study population look like (i.e. have the covariate distribution of $L$) the untreated 
group
    \item effect in the treated: match to make the study population look like (i.e. have the covariate distribution of $L$) the treated 
group
    \item in 1:1 matching, the treatment group with the smaller sample size determines the distribution of $L$ in the final matched 
population (i.e. strata of $L$ that have only treated or only treated are discarded) 
   \end{itemize}
\end{itemize}
	
	
	
\section{Matched Cohort study}
	\begin{itemize}
	\item prevents association between confounder and exposure of interest
	\item matching prevents confounding even in \em crude analysis \em
	\item stratified analysis improves efficiency
	\item \em can \em assess the effect of matching factor on the outcome
	\item \em can \em assess the effect modification by matching factor on the exposure of interest
	\end{itemize}

	
	
\section{Matched Case-Control study}
	\begin{itemize}
	\item eliminates the association between confounder and the outcome
	\item matching does not prevent confounding \em by itself \em
	\item matching only improves efficiency 
	\item matching for \em efficiency \em, NOT validity
		\begin{itemize}
		\item a way to deal with potential positivity violations
		\item ensure enough cases and controls within strata to allow for adjustment
		\item stratified analysis will be performed for confounding with or without matching on confounder
		\end{itemize}
	\item matching introduces \em selection bias \em $\Longrightarrow$ \em toward the null \em
		\begin{itemize}
		\item if the matching factor is associated with the exposure, matching would make the exposure distribution in controls similar to the exposure distribution in cases
		\item must perform matched analysis (stratified analysis) after matching in case-control studies
		\end{itemize}
	\item \em CANNOT \em assess the effect of matching factor on the outcome
	\item \em can \em assess effect modification by matching factor on the exposure of interest
	\item when the level of potential confounders is \em unique \em each case, matching is necessary to obtain controls for each case (individual matching)
	\item when each matched set represents a level of confounder that is observed \em repeatedly\em , all study subjects with the same level of confounders can be analyzed with in a single stratum (stratified analysis)
		\begin{itemize}
		\item e.g. for $1:1$ individually matching on gender only, may perform stratified analysis on gender instead of stratified analysis on mathcing set.
		\end{itemize}
	\end{itemize}

	\begin{table}[h!]
	\centering
	\caption{Efficiency and validity of matching under different analysis schemes}
	\begin{tabular}{l l c l c c }
	\hline \hline
	\multicolumn{6}{c}{\textbf{Effect of Matching in Case-control Studies}}
	\\
		&
		& 
		& 
		& \multicolumn{2}{c}{\textsc{Analysis}} 
	\\
		&
		&		
		& \textbf{Design} 
		& \textbf{Stratified} 
		& \textbf{Not Stratified} 
	\\
	\hline
		& & & & &
	\\
		  \multirow{2}{*}{$E \leftarrow M \rightarrow D$} 
		& \multirow{2}{*}{$\Bigg\}$} 
			& \textsl{Appropriate} 
			& Match 
			& V PPP 
			& \textsl{BIAS} 
	\\
		&	& \textsl{Matching}
			& Do not Match 
			& V P 
			& \textsl{BIAS} 
	\\
	& & & & & 
	\\
		  \multirow{2}{*}{$E \; \; M \rightarrow D$} 
		& \multirow{2}{*}{$\Bigg\}$} 
			& \textsl{Unneccesary} 
			& Match 
			& V PP 
			& V PPP
	\\
		&	& \textsl{Matching}
			& Do not Match 
			& V PP 
			& V PPP
	\\
	& & & & &
	\\
		  \multirow{2}{*}{$E \leftarrow M \; \; D$} 
		& \multirow{2}{*}{$\Bigg\}$} 
			& \textsl{Over-} 
			& Match 
			& V P  
			& \textsl{BIAS} 
	\\
		&	& \textsl{Matching}	
			& Do not Match 
			& V P 
			& V PPP
	\\
	& & & & &
	\\
		  \multirow{2}{*}{$E \rightarrow M \rightarrow D$} 
		& \multirow{2}{*}{$\Bigg\}$} 
			& \textsl{Match on} 
			& Match 
			& \textsl{BIAS} 
			& \textsl{BIAS} 
	\\
		&	& \textsl{Intermediate}	
			& Do not Match 
			& \textsl{BIAS}  
			& V PPP 
	\\
	& & & & & 
	\\
	\hline \hline
	\end{tabular}
	\end{table}
	
\section{Informative and non-informative risk sets}

	\begin{itemize}
	\item note that informativeness is relative to your estimand
	\item For ratio measures
		\begin{itemize}
		\item you need a case, exposed and unexposed denominator
		\end{itemize}
	\item however for the intercept, i.e. the absolute 'baseline' rate
		\begin{itemize}
		\item all you need is denominator in the unexposed
		\item for this reason, in poisson strata that have no cases and only unexposed person-time \em are \em informative for the estimation of the baseline rate, but no the effect estimate for exposure
		\item in cox however, such risk-sets are thrown out completely
		\end{itemize}
	\item also note, in a technical sense all you need is a case and variation in covariates to contribute to the ratio paramaters of the model in some way
	\item all of this translates to matched sets in matched case-control studies
	
	\begin{table}[h!]
	\caption{Notation for cohort and case-control data}
	\begin{tabular}{rccl}
	\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{non-cases} & \multicolumn{1}{c|}{$N_1-a$} & \multicolumn{1}{c|}{$N_0-b$} & $M_0$ \\
	\cline{2-3}
	& $N_1$ & $N_0$ & $T$\\
	\end{tabular}
	\end{table}

\item when stratifying on time scale and additional confounders 2x2 tables per stratum represent risk sets
\item using the Mantel-Haenszel estimator $RR_{MH}=\frac{\sum_{i}{\frac{a_{i}N{0i}}{T_{i}}}}{\sum_{i}\frac{b_{i}N{1i}}{T_{i}}}$ only risk sets with $M_{1i}>0$ are considered informative so when $a_{i}>0$ the risk set contributes to the numerator of the estimator and when $b_{i}>0$ it contributes to the denomenator if both $N_{1i}>0$ and $N_{0i}>0$
\item the same informative risk sets arethe ones contributhing to the $RR_{MH}$ variance $\hat{Var}[log(\hat{RR})]=\frac{A}{BC}$, where \\
$A=\sum_{i}\frac{M_{1i}N_{1i}N_{0i}}{T^{2}_{i}}$\\
$B=\sum_{i}\frac{a_{i}N_{0i}}{T_{i}}$\\
$C=\sum_{i}\frac{b_{i}N_{1i}}{T_{i}}$
\item matched data
\begin{itemize}
\item risk set sampling assures $M_{1i}>0$ as risk sets are difined at the time each case occurs
\begin{figure} [H]
\centering
	\includegraphics[scale=0.5]{matched_cohort_2x2.jpg}
\caption{2x2 Notation for matched pairs cohort data}
\label{matched_cohort_2x2}
\end{figure}
\item consider figure 2 where\\
T are pairs with an exposed and an unexposed case\\
U are pairs with an exposed case and unexposed non-case\\
V are pairs with an unxposed case and exposed non-case\\
W are pairs with an exposed and unexposed non-case
\item matched case control data assure $M_{1i}>0$ for each stratum of matching factors
\item for mathed cohort data the $RR_{MH}$ simplifies to $RR_{MH}=\frac{T+U}{T+V}$
\begin{figure} [H]
\centering
	\includegraphics[scale=0.5]{matched_cc_2x2.jpg}
\caption{2x2 Notation for matched pairs case-control data}
\label{matched_cc_2x2}
\end{figure}
\item consider figure 3 where\\
T are pairs with an exposed case and an exposed control\\
U are pairs with an exposed case and unexposed control\\
V are pairs with an unxposed case and exposed control\\
W are pairs with an unexposed case and unexposed control
\item in matched data analysis informative risk sets are dose with discordant case-control pairs with respect to exposure levels for OR estimates\\
$OR_{MH}=\frac{\sum_{i}\frac{a_{i}d_{i}}{T_{i}}}{\sum_{i}\frac{b_{i}c_{i}}{T_{i}}}$ simplifies to $OR_{MH}=\frac{U}{V}$
\end{itemize}
\end{itemize}
	
\section{Analysis of Matched Case-control Studies}
	\begin{itemize}
	\item matching should be followed by stratified analysis
	\item match set $=$ strata
	\item Paired data analysis for matched case-control studies
		\begin{enumerate}
		\item notation \\
			\begin{tabular}{rccl}
			& \multicolumn{2}{c}{\textbf{controls}}\\
	 		\multicolumn{1}{c}{\textbf{cases}} & \ exposed \ & unexposed \\
			\cline{2-3} 
			\multicolumn{1}{r|}{exposed} & 			\multicolumn{1}{c|}{$f_{11}$} & \multicolumn{1}{c|}{$f_{10}$}\\
			\cline{2-3}
			\multicolumn{1}{r|}{unexposed} & \multicolumn{1}{c|}{$f_{01}$} & \multicolumn{1}{c|}{$f_{00}$}\\
			\cline{2-3}
			\\
			\end{tabular}
			
			\begin{itemize}
			\item $f_{ij}$ is the number of \em pairs \em
			\item $i$ is the exposure status of the case; $j$ is the exposure status of the control
			\end{itemize}
		\item hypothesis testing (McNemar test)\\
		\\
		$H_0$: there is no association between the exposure and the outcome\\
		\\
		$H_1$ : there is an association between the exposure and the outcome
		$$Z^2 = \frac{[f_{10} - f_{01}]^2}{f_{10} + f_{01}} \sim \chi^2_1$$
		\item only discordant pairs provide information
		\item same as the transmission disequilibrium test (TDT) in genetics
		\item confidence interval
			\begin{itemize}
			\item $\hat OR_{MH} = \frac{f_{10}}{f_{01}}$
			\item $Var[log(\hat OR_{MH})] = \frac{1}{f_{10}} + \frac{1}{f_{01}}$
			\end{itemize}
		\end{enumerate}
	\item McNemar vs Mentel-Haeszel 
			\begin{itemize}
			\item for $1:1$ matching
			
			\begin{figure}[H]
			\centering
			\includegraphics[scale=0.4]{mcnemar.png}
			\caption{paired data in stratification layout}
			\label{mcnemar}
			\end{figure}
			
			$$\displaystyle \hat OR_{MH} = \frac{\sum_i \frac{a_i d_i}{T_i}}{\sum_i \frac{b_i c_i}{T_i}} = \frac{A[\frac{1 \times 0}{2}]+B[\frac{1 \times 1}{2}]+C[\frac{0 \times 0}{2}]+D[\frac{0 \times 1}{2}]}{A[\frac{0 \times 1}{2}]+B[\frac{0 \times 0}{2}]+C[\frac{1 \times 1}{2}]+D[\frac{1 \times 0}{2}]} = \frac{B}{C}$$
			
			\end{itemize}
	\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{G-Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Standardization}
\subsection{General Idea}
 $$\displaystyle E[Y^{a}] = \sum_{l}E[Y|A=a,L=l]Pr[L=l] $$
\begin{itemize}
\item the general idea is to simulate an occurrence measure for population A as if, \em counterfactually, \em population A had the covariate 
distribution (looked 
like) population B 
\item weights equal to the proportion of individuals in the reference population within each stratum (i.e. $ Pr[L=l] $)
\item causal risk ratio: $ \frac{\sum_{l}Pr[Y=1|A=1,L=l]Pr[L=l]}{\sum_{l}Pr[Y=1|A=0,L=l]Pr[L=l]} $
\item standardized occurrence measure: weighted average of the stratum-specific occurrence measures (i.e. $ Pr[Y=1|L=l] $) where the weights as 
the strata-specific probability of the covariate $L$ (i.e. $ Pr[L=l] $) 
    $$\sum_{l} Pr[Y=1|A=a,L=l] \times Pr[L=l]$$ 
    $$if \ Pr[A=a|L=l] > 0 \ for \ all \ l \ with \ Pr[L=l] \neq 0$$
\item standard population: reference population used to create the weight $ Pr[L=l] $
\item For a subset of the population ($A=a^{\prime}$)
$$\displaystyle \sum_{l}E[Y|A=a,L=l]Pr[L=l|A=a^ \prime]$$ 
\end{itemize}

\subsection{Traditional Notation}

\begin{table}[H]
\centering
\caption{traditional notation for case-control/cohort data}
	\begin{tabular}{rccl}
	\label{table}
	\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & \multicolumn{1}{c|}{$a$} & \multicolumn{1}{c|}{$b$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{non-cases} & \multicolumn{1}{c|}{$N_1-a$} & \multicolumn{1}{c|}{$N_0-b$} & $M_0$ \\
	\cline{2-3}
	& $N_1$ & $N_0$ & $T$
	\end{tabular}
\end{table}

\subsubsection*{Unified approach}
\begin{itemize}
\item Standardization can be expressed as a weighted average of stratum specific effect measures
	\begin{itemize}
	\item in the presence of effect modification we prefer standardization
		\begin{itemize}
		\item the weights meaningly refer to the structure of the population and provide a true average effect
		\end{itemize}
	\item in the absence of effect modification we prefer mantel-haenszel / inverse variance weights
		\begin{itemize}
		\item the weights are arbitrary, but if \textbf{no} effect modification they provide a true average effect
		\end{itemize}
	\item \em assuming identifiability conditions hold \em	
	\end{itemize}
\item \textbf{Direct standardization of the effect measure}
 \begin{itemize}
\item standardized rate(risk) ratio =$\sum_{I}w_{i}\hat{RR}_{i}=\dfrac{\sum_{I}w'_{i}\hat{RR}_{i}}{\sum_{I}w'_{i}}$
\item standardized rate(risk) difference =$\sum_{I}w_{i}\hat{RD}_{i}=\dfrac{\sum_{I}w'_{i}\hat{RD}_{i}}{\sum_{I}w'_{i}}$
\item $w_{i}=\dfrac{w'_{i}}{\sum_{I}w_{i}}$ and $\sum_{I}w_{i}=1$
\item strata $I$ defined by collection of covariates (L vector in counterfactual notation)
\end{itemize}
\item \textbf{Direct standardization}
	\begin{itemize}
	\item weighted average of stratum specific rate ratios using the distribution of unexposed cases as the weights ($\hat{I_{0i}}N_{0}$)
	\item unexposed is the standard in terms of stratum specific rates and data distribution\\ \\
$SRR=\sum_{I}w_{i}\hat{RR_{i}}=\sum_{I}\dfrac{\hat{I_{0i}}N_{0i}}{\sum_{I}\hat{I_{01}}N_{0i}}\times \dfrac{\dfrac{a_i}{N_{1i}}}{\dfrac{b_i}{N_{0i}}} =\sum_{I}\dfrac{b_i}{\sum_{I}b_i}\times \dfrac{\dfrac{a_i}{N_{1i}}}{\dfrac{b_i}{N_{0i}}}=\dfrac{\sum_{I}\frac{a_{i}}{N_{1i}}N_{0i}}{\sum_{I}b_{i}}$ \\ \\ $where$ \dots \\ \\
$w_{i}=\dfrac{\hat{I_{0i}}N_{0i}}{\sum_{I}\hat{I_{01}}N_{0i}}=\dfrac{\frac{b_{i}}{N_{0i}}N_{0i}}{\sum_{I}\frac{b_{i}}{N_{0i}}N_{0i}}=\dfrac{b_{i}}{\sum_{I}b_{i}}$\\ \\
$w'{i}=b_{i}$
	\end{itemize}
\item \textbf{Indirect standardization}
	\begin{itemize}
	\item weighted average of stratum specific rate ratios using the distribution of expsected exposed cases, which were calculated by the incidence rate among the controls $(\hat I_{0i})$ and the distribution of exposed subjects ($N_{1i}$), as the weights\\ \\
$SMR=\sum_{I}w_{i}\hat{RR_{i}}$ \\ \\ $where$ \dots \\ \\
$w_{i}=\frac{\hat{I_{01}}N_{1i}}{\sum_{I}\hat{I_{01}}N_{1i}}=\frac{\frac{b_{i}}{N_{0i}}N_{1i}}{\sum_{I}\frac{b_{i}}{N_{0i}}N_{1i}}=\frac{E_{i}}{\sum_{I}E_{i}}$\\ \\
$w'{i}=E_{i}$\\ \\
$SMR=\sum_{I}w_{i}\hat{RR_{i}}=\sum_{I}w_{i}\frac{\hat{I_{1i}}}{\hat{I_{1i}}}=\sum_{I}w_{i}\dfrac{\frac{a_{i}}{N_{1i}}}{\frac{b_{i}}{N_{0i}}} \\ \\=\dfrac{\sum_{I}\frac{b_{i}}{N_{01}}N_{1i}}{\sum_{I}\frac{b_{i}}{N_{01}}N_{1i}}\dfrac{\frac{a_{i}}{N_{1i}}}{{b_{i}}{N_{0i}}}=\dots=\dfrac{\sum_{I}a_{i}}{\sum_{I}\frac{b_{i}}{N_{0i}}N_{1i}}=\dfrac{O}{E}$\\ \\
$where \dots$
		\begin{itemize}
		\item O is observed cases in the exposed
		\item E is expected cases in the exposed under the null hypothesis for stratum specific rates
		\end{itemize}
	\item \textit{SMR is standardized ratio when the \textbf{exposed} are the standard in terms of data distribution}
	\end{itemize}
\end{itemize}
\subsubsection*{Traditional approach}
\begin{itemize}
\item individual standardization of rates(risks) in the exposed and unexposed
\item different weights but algebraically identical formulas
\item \textbf{Direct standardization}
\begin{itemize}
\item weight $w_{i}=\frac{N_{0i}}{\sum_{I}N_{0i}}$
	\begin{itemize}
	\item proportion of PT in stratum i in the unexposed
	\end{itemize} 
\item \textit{Standardized rate in the exposed}
	\begin{itemize}
	\item expected crude rate in the unexposed if the rates were the same as the exposed\\
$\hat{I_{1}}=\sum_{I}w_{i}\hat{I_{1}}=\sum_{I}w_i\dfrac{a_{i}}{N_{1i}}=\sum_{I}\dfrac{N_{0i}}{\sum_{I}N_{0i}}\frac{a_{i}}{N_{1i}}=\dfrac{\sum_{I}N_{0i}\frac{a_{i}}{N_{1i}}}{N_{0}}$\\where $N_{0}=\sum_{I}N_{0i}$
	\end{itemize}
\item \textit{Standardized rate in the unexposed}
	\begin{itemize}
	\item crude rate in the unexposed \\ \\
$\hat{I_{0}}=\sum_{I}w_{i}\hat{I_{0}}=\sum_{I}w_i\dfrac{b_{i}}{N_{0i}}=\sum_{I}\dfrac{N_{0i}}{\sum_{I}N_{0i}}\frac{b_{i}}{N_{0i}}=\dfrac{\sum_{I}b_{i}}{N_{0}}$ \\
	\end{itemize}
\item \textit{standardized rate ratio} \\ \\ $\hat{SRR}=\dfrac{\hat{I_{1}}}{\hat{I_{0}}}=\dfrac{\frac{\sum_{I}N_{0i}\dfrac{a_{i}}{N_{1i}}}{\sum_{I}N_{0i}}}{\frac{\sum_{I}b_{i}}{N_{0i}}}=\dfrac{\sum_{I}N_{0i}\frac{a_{i}}{N_{1i}}}{\sum_{I}b_{i}}$\\
	\begin{itemize}
	\item which is equivalent to the unified approach
	\end{itemize}
\end{itemize}
\item \textbf{Indirect standardization}
\begin{itemize}
\item weight $w_{i}=\dfrac{N_{1i}}{\sum_{I}N_{1i}}$
	\begin{itemize}
	\item proportion of PT in stratum i in the exposed
	\end{itemize}
\item \textit{Indirectly standardized rate in exposed}
	\begin{itemize}
	\item[\-] $\hat{I_{1}}=\sum_{I}w_{i}\hat{I_{1}}=\sum_{I}w_i\dfrac{a_{i}}{N_{1i}}=\sum_{I}\dfrac{N_{1i}}{\sum_{I}N_{1i}}\frac{a_{i}}{N_{1i}}=\dfrac{\sum_{I}a_{i}}{\sum_{I}N_{1i}}=\frac{O}{N_{1}}$\\ \\ $where \dots \ N_{1}=\sum_{I}N_{1i}$ and O is the observed cases in the exposed\\ \\ $O=\sum{O_{i}}=\sum{a_{i}}$
	\end{itemize}
\item \textit{Indirectly standardized rate in unexposed}
	\begin{itemize}
	\item[\-] $\hat{I_{0}}=\sum_{I}w_{i}\hat{I_{0}}=\sum_{I}w_i\dfrac{b_{i}}{N_{0i}}=\sum_{I}\dfrac{N_{1i}}{\sum_{I}N_{1i}}\frac{b_{i}}{N_{0i}}=\dfrac{\sum_{I}\frac{b_{i}}{N_{0i}}{N_{1i}}}{\sum_{I}N_{i1}}=\frac{E}{N_{1}}$\\ \\ where E is the expected cases in the exposed if the exposed had the rates of the unexposed
	\end{itemize}
\item \textit{Indirectly standardized rate ratio}
	\begin{itemize}
	\item[\-] $SMR=\dfrac{\hat{I_{1}}}{\hat{I_{0}}}=\dfrac{\sum_{I}a_{i}}{\sum_{I}\frac{b_{i}}{N_{0i}}{N_{1i}}}=\dfrac{O}{E}$ just like the unified approach.
	\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Interpretations of standardized measures}
\begin{itemize}
\item \textit{directly} standardized measures are interpreted as the measures which would be observed in any population \dots
	\begin{itemize}
	\item with similar distribution of stratification factors as the unexposed
	\item when rates are similar to those in the exposed
	\end{itemize}
\item \textit{indirectly} standardized measures are interpreted as the measures which would be observed in any population
	\begin{itemize}
	\item with a similar distribution of stratification factors as the exposed
	\item rates similar to those observed in the exposed
	\end{itemize}
\item \textit{Comparability}
	\begin{itemize}
	\item \textit{directly} standardized measures are comparable from study to study if the same standard population is used
	\item \textit{indirectly} standardized measures are comparable only in the absence of effect modification
	\end{itemize}
\end{itemize}
\subsection{Relation between Causal and Traditional Notation}
\begin{itemize}
\item when using the entire population
$\frac{\sum Pr[Y=1|A=1,L=l]Pr[L=l]}{\sum Pr[Y=1|A=0,L=l]Pr[L=l]}$ is equivalent to $\frac{\sum_{I}w_{i}\frac{a_{i}}{N_{1i}}}{\sum_{I}w_{i}\frac{b_{i}}{N_{0i}}}$\\ if $w_{i}=\frac{N_{i}}{\sum_{I}N_{i}}\hspace{1cm}(\frac{N_{i}} {\sum_{I}N_{i}}=Pr[L=l])$
\item when using weights based on the unexposed (direct) we have the effect among the untreated\\ \\
$\frac{\sum{\frac{a_{i}}{N_{1i}}N_{01}}}{\sum{b_{i}}}$ same as $\frac{\sum_{l}Pr[Y=1|A=1,L=l]Pr[L=l|A=0]}{Pr[Y=1|A=0]}$
\item similarly the SMR is the effect among the treated\\ \\
$\frac{\sum_{I}a_{i}}{\sum_{I}\frac{b_{i}}{N_{0i}}{N_{1i}}}$ is the same as $\frac{Pr[Y=1|A=1]}{\sum_{l}Pr[Y=1|A=0,L=l]Pr[L=l|A=1]}$
\end{itemize}

\subsection{Variances}
\begin{itemize}
\item SRR\\
$$Var[\hat{SRR}]=\sum_{I}w_{i}^2Var[\hat{RR_{i}}]=\sum_{I}w_{i}^2\hat{Var}[e^{\log{\hat{RR_{i}}}}]=\sum_{I}w_{i}^2\hat{RR_{i}^2}\hat{Var}[log\hat{RR_{i}}]$$
$(\hat{Var}[log\hat{RR_{i}}]=\dfrac{1}{\hat{RR_{i}^2}}\hat{Var}[{\hat{RR_{i}}}])$
\item SRD\\
$\hat{Var}[\hat{SRD}]=\sum_{I}w_{i}^2\hat{Var}[\hat{RD_{i}}]$
\end{itemize}

\subsection{Reference Populations and Exchangeability Assumptions}

\subsubsection*{\textbf{Standardization in the \underline{entire population}}} 
$$E[Y^{a}] = \displaystyle \sum_{l}E[Y|A=a,L=l]Pr[L=l] $$
\begin{itemize}
\item requires \emph{full} conditional exchangeability $Y^{a=1,a=0} \amalg A|L$ 
\item Standardized Risk Ratio 
    \begin{itemize}
    \item $\frac{\sum expected \; deaths \; among \; population \; if \; all \; exposed}{\sum expected \; deaths \; among \; population \; 
if \; all \; unexposed}$
    \item $\frac{Pr[Y^{a=1}]}{Pr[Y^{a=0}]}$
    \item expected number of cases in the entire population had everyone been exposed divided by the number of cases had everyone been 
unexposed
    \end{itemize}
\end{itemize}
\begin{align*}
SRR_{T} &= \frac{Pr[Y^{a=1}]}{Pr[Y^{a=0}]} \\ 
&= \frac{\displaystyle \sum_{l} Pr[Y^{a=1}=1,L=l]}{\displaystyle \sum_{l} Pr[Y^{a=0}=1,L=l]} \ \mbox{(introduce L)} \\
&= \frac{\sum Pr[Y^{a=1}=1|L=l] \times Pr[L=l]}{\sum Pr[Y^{a=0}=1|L=l] \times Pr[L=l]} \\ 
&= \frac{\sum Pr[Y^{a=1}=1|A=1,L=l] \times Pr[L=l] }{\sum Pr[Y^{a=0}=1|A=1,L=l] \times Pr[L=l] } \ \mbox{(conditional exchangeability)} \\ 
&= \frac{\sum Pr[Y=1|A=1,L=l] \times Pr[L=l] }{\sum Pr[Y=1|A=0,L=l] \times Pr[L=l] } \ \mbox{(consistency)} \\
\end{align*}

\subsubsection*{\textbf{Standardization in the \underline{exposed}}} 
\begin{itemize}
\item requires \em partial \em conditional exchangeability $Y^{a=0} \amalg A|L$
$$E[Y^{a}] = \displaystyle \sum_{l}E[Y|A=a,L=l]Pr[L=l|A=1] $$
\item Standardized Mortality Ratio - $SMR$
    \begin{itemize}
    \item $\frac{\sum observed \; deaths \; among \; exposed}{\sum expected \; deaths \; among \; exposed} $
    \item $\frac{Pr[Y^{a=1}|A=1]}{Pr[Y^{a=0}|A=1]}$
    \item observed number of cases in the exposed divided by the number of cases had the exposed been unexposed
\begin{align*}
SMR &= \frac{Pr[Y^{a=1}=1|A=1]}{Pr[Y^{a=0}=1|A=1]}\\
		&= \frac{Pr[Y^{a=1}=1|A=1]}{\frac{Pr[Y^{a=0}=1,A=1]}{Pr[A=1]}}\\
		&= \frac{Pr[Y^{a=1}=1|A=1]}{\displaystyle \sum_{l}{\frac{Pr[Y^{a=0}=1,A=1,L=l]}{Pr[A=1,L=l]}} \times {\frac{Pr[A=1,L=l]}{Pr[A=1]}}} \ \mbox{(introduce L)}\\
		&= \frac{Pr[Y=1|A=1]}{\sum Pr[Y^{a=0}=1|A=1,L=l]Pr[L=l|A=1]} \\
		&= \frac{Pr[Y=1|A=1]}{\sum Pr[Y^{a=0}=1|A=0,L=l]Pr[L=l|A=1]} \ \mbox{(conditional exchangeability)} \\
		&= \frac{Pr[Y=1|A=1]}{\sum Pr[Y=1|A=0,L=l]Pr[L=l|A=1]} \ \mbox{(consistency)}\\
\end{align*}
     \item the denominator simulates the number of cases in the unexposed as if they had the covariate distribution of the exposed     
    \end{itemize}
\end{itemize}

\subsubsection*{\textbf{Standardization in the \underline{unexposed}}} 
$$E[Y^{a}] = \displaystyle \sum_{l}E[Y|A=a,L=l]Pr[L=l|A=0] $$
\begin{itemize}
\item requires \em partial \em conditional exchangeability $Y^{a=1} \amalg A|L$
\item Standardized Risk Ratio - $SRR_{U}$
    \begin{itemize}
    \item $\frac{\sum expected \; deaths \; among \; unexposed}{\sum observed \; deaths \; among \; unexposed} $
    \item $\frac{Pr[Y^{a=1}|A=0]}{Pr[Y^{a=0}|A=0]}$
    \item the number of cases had the unexposed been exposed divided by the observed number of cases in the unexposed
$$ \frac{\sum_{l} Pr[Y=1|A=1,L=l]Pr[L=l|A=0]}{Pr[Y=1|A=0]} $$
    \item the numerator simulates the number of cases in the exposed as if they had the covariate distribution of the unexposed     
    \end{itemize}
\end{itemize}

\subsubsection*{\textbf{Standardization in a \underline{subset} of the entire population}}
$$E[Y^{a}|M=m^{\prime}] = \displaystyle \sum_{l}E[Y|A=a,L=l]Pr[L=l|A=0,M=m^{\prime}] $$
\begin{align*}
& Pr[Y^{a=1}=1|M=m^{\prime}] \\
&= \displaystyle \sum_{l} Pr[Y^{a=1}=1,L=l|M=m^{\prime}]  \\
&= \displaystyle \sum_{l} \frac{Pr[Y^{a=1}=1,L=l,M=m^{\prime}]}{Pr[M=m']}     \\
&= \displaystyle \sum_{l} \frac{Pr[Y^{a=1}=1|L=l,M=m^{\prime}] \times Pr[L=l|M=m^{\prime}] \times Pr[M=m']}{Pr[M=m']}  \\
&= \displaystyle \sum_{l} Pr[Y^{a=1}=1|L=l,M=m^{\prime}] \times Pr[L=l|M=m^{\prime}] \\
&= \displaystyle \sum_{l} Pr[Y^{a=1}=1|A=1,L=l,M=m^{\prime}] \times Pr[L=l|M=m^{\prime}]  \ \mbox{(conditional exchangeability)} \\
&= \displaystyle \sum_{l} Pr[Y=1|A=1,L=l,M=m^{\prime}] \times Pr[L=l|M=m'] \  \mbox{(consistency)} \\
\end{align*}



\section{G-formula}
\subsection{definitions for estimating the causal effect of  time-varing exposure}
	\begin{itemize}
	\item for real situation, use the DAG with assumptions on associations between variables
	\item assuming a closed cohort with no right censoring
	\item using ``time since baseline'' as time scale 
	\item only consider exposures occurring at or after baseline
	\item fixed exposure
		\begin{enumerate}
		\item only occur at the baseline
		\item do not change over time 
		\item evolve over time with a deterministic way
		\end{enumerate}
	\item time-varying exposure
	\item time-dependent confounder: an independent predictor of both subsequent exposure and the outcome within strata jointly determined by baseline covariates and prior exposure ($A_{t+1} \leftarrow L_{t} \rightarrow Y $)
	\item the conventional methods would fail if \dots
		\begin{enumerate}
		\item time-dependent confounders presents
		\item within strata of the baseline covariates, baseline exposure predicts the subsequent time-dependent confounders
		\end{enumerate}
	\item three g-methods will be used to deal with time-varying exposures and time-dependent confounders:
		\begin{enumerate}
		\item g-computation algorithm formula (g-formula)
		\item inverse probability of treatment weighting (IPTW) of marginal structural models (MSMs)
		\item g-estimation of structural nested models (SNMs)
		\end{enumerate}
	\item[\-] all 3 methods will give identical results \textbf{non-parametrically}
	\item[\-] the method of choice depends on the causal contrast of interest and the robustness of the method to model misspecification
	\end{itemize}

\subsection{g-computation algorithm formula (g-formula)}
	\begin{itemize}
	\item we may get the probability of $Y^{a}$ from a causal DAGs with causal Markov assumption following the \emph{g-computation algorithm formula}
	\item step by step \dots
		\begin{enumerate}
		\item draw the temporally ordered complete DAG
		\begin{figure}[H]
		\centering
		\caption{temporally ordered complete causal DAG}
		\includegraphics[scale=0.2]{gformula1.png}
		\vspace{-60pt}
		\label{gformula1}
		\end{figure}
		\item write out the \textbf{joint probability} of all variables in the DAG
		$$f(Y,A,L)=f(Y|A,L)f(A|L)f(L)$$
		\item draw the reduced, intervention DAG
		\begin{figure}[H]
		\centering
		\caption{intervention DAG}
		\includegraphics[scale=0.2]{gformula2.png}
		\vspace{-60pt}
		\label{gformula2}
		\end{figure}
		\item reduce the \textbf{joint probability} of all variables in the DAG according to the intervention DAG
		$$f^{a}(Y,a,L)=f(Y|a,L)f(L)$$
		\item sum over the nuisance parameter (usually $L_{(t)}$) and get $f(Y^{a}=y)$
		$$f(Y^{a}=y)=f^{a}(Y=y)=\sum_{l}f(Y=y|a,l)f(l)$$
		\end{enumerate}
	\item Theorem
	$$\mbox{if} \ Y^{a} \amalg A|L \ \mbox{then} \ f^{a}(Y=y)=f(Y^{a}=y)$$
	\begin{eqnarray*}
	f^{a}(Y=y) &=& \sum_{l}f(Y=y|a,l)f(l) \\
			   &=& \sum_{l}f(Y^{a}=y|a,l)f(l) \ \mbox{(consistency)}\\
			   &=& \sum_{l}f(Y^{a}=y|a,l)f(l) \ \mbox{(conditional exchangeability)}\\
			   &=& \sum_{l}f(Y^{a}=y) \\
	\end{eqnarray*}
\item for time-varying exposure \dots
	\begin{enumerate}
	\item draw the temporally ordered complete DAG
		\begin{figure}[H]
		\centering
		\caption{temporally ordered complete causal DAG}
		\includegraphics[scale=0.2]{gformula3.png}
		\vspace{-20pt}
		\label{gformula3}
		\end{figure}
	\item write out the \textbf{joint probability} of all variables in the DAG
		$$f(Y,A_{1},L_{1},A_{0},L_{0})=f(Y|A_{1},L_{1},A_{0},L_{0})f(A_{1}|L_{1},A_{0},L_{0})f(L_{1}|A_{0},L_{0})f(A_{0}|L_{0})f(L_{0})$$
	\item draw the reduced, intervention DAG
		\begin{figure}[H]
		\centering
		\caption{intervention DAG}
		\includegraphics[scale=0.2]{gformula4.png}
		\vspace{-20pt}
		\label{gformula4}
		\end{figure}
	\item reduce the \textbf{joint probability} of all variables in the DAG according to the intervention DAG
		$$f^{a_{0},a_{1}}(Y,a_{1},L_{1},a_{0},L_{0})=f(Y|a_{1},L_{1},a_{0},L_{0})f(L_{1}|a_{0},L_{0})f(L_{0})$$
	\item sum over the nuisance parameter and get $f(Y^{a_{0},a_{1}}=y)$
		$$f(Y^{a_{0},a_{1}}=y)=f^{a_{0},a_{1}}(Y=y)=\sum_{l_{0},l_{1}}f(Y|a_{1},l_{1},a_{0},l_{0})f(l_{1}|a_{0},l_{0})f(l_{0})$$
	\end{enumerate}
	\end{itemize}



\section{Inverse Probability Weighting (IPW)}
\subsection{Inverse Probability Weights ($W$)}
$$\frac{1}{Pr[A=a|L=l]} \hspace{.5cm} or \hspace{.5cm} \frac{1}{f[A|L]} \hspace{.5cm} or \hspace{.5cm} E[Y^{a}] = E \left[ \frac{I(A=a)Y}{f[a | 
L]} \right] $$
\begin{itemize}
\item each person in the original population is weighted by the inverse probability of receiving the treatment \em they actually received \em given the covariates needed to identify the causal effect (i.e $1/(propensity \; score)$ for treated subjects and $1/(1-propensity \; score)$ for untreated subjects) to create conditional exchangeability in a new pseudo-population
	\begin{itemize}
	\item assumes conditional exchangeability can be obtained via the \em measured \em covariates $L$
	\item IPW cannot create conditional exchangeability in the pseudo-population when there is confounding by unmeasured variables $U$ ($A \leftarrow U \rightarrow Y$)
	\end{itemize}
\item simulation of what would have happened had all subjects in the population been treated and untreated \em and \em had the same distribution of L as the standard population
    \begin{itemize}
    \item pseudo-population: hypothetical population created by applying IP weights to the observed population
            	\begin{itemize}
    		\item size of the pseudo-population is the size of the original population times the number of treatment categories (for stabilized weights size is the same as original population)
        	\item the treated and untreated are unconditionally exchangeable because they are the same individuals under different treatment levels
        	\item associational RR in pseudo-population equals causal RR in the original population
        	\end{itemize}
    \end{itemize}
\item analytic control: adjust for $L$, linguistically different from physically controlling for $L$ in design phase
	\begin{itemize}
	\item removes the arrow from $L$ into $A$
	\end{itemize}
	
\begin{figure}
	\centering
		\includegraphics[scale=0.4]{unstablized_IPW.pdf}
		\caption{unstablized IP weighting creates a psuedo-population which is twice as large as the original population for dichotomous exposures}
		\label{unstablized_IPW}
\end{figure}




\item Non-parametric IPW and standardization are equivalent \\
Let's assume that A, L, and Y are all binary variables.
The IP weighted risk of Y for treatment group $A=a$ is $ E \left[ \frac{I(A=a)Y}{f[a | L]} \right] $. The standardized risk of Y for 
treatment group $A=a$ is $ 
\sum_{l}Pr[Y=1|A=a,L=l]Pr[L=l] $. To show that these two things are equal, first we need to put that $ E \left[ \frac{I(A=a)Y}{f[a | L]} 
\right] = E \left[ \frac{I
(A=a)Y}{f[A| L]} \right] $. 
Then, we may write down explicitly how they are equivalent: 

\begin{eqnarray*}
E \left[ \frac{I(A=a)Y}{f[A| L]} \right] & = & E \left[ \frac{I(A=a)}{f[A| L]} \times Y \right] \\
& = & \frac{I(A=1)}{f[A| L=1]} \times f[Y, A, L=1] \times Y + \frac{I(A=1)}{f[A| L=0]} \times f[Y, A, L=0] \times Y \\
& = & \frac{I(A=1)}{f[A| L=1]} \times f[Y=1, A, L=1] \times 1 + \frac{I(A=1)}{f[A| L=0]} \times f[Y=1, A, L=0] \times 1 \\
& + & \frac{I(A=1)}{f[A| L=1]} \times f[Y=0, A, L=1] \times 0 + \frac{I(A=1)}{f[A| L=0]} \times f[Y=0, A, L=0] \times 0 \\
& = & \frac{I(A=1)}{f[A| L=1]} \times f[Y=1, A, L=1] + \frac{I(A=1)}{f[A| L=0]} \times f[Y=1, A, L=0] \\
& = & \frac{I(A=1)}{f[A| L=1]} \times \frac{f[Y=1, A, L=1]}{f[A, L=1]} \times \frac{f[A, L=1]}{Pr[L=1]} \times Pr[L=1] \\
& + & \frac{I(A=1)}{f[A| L=0]} \times \frac{f[Y=1, A, L=0]}{f[A, L=0]} \times \frac{f[A, L=0]}{Pr[L=0]} \times Pr[L=0] \\ 
& = & \frac{I(A=1)}{f[A| L=1]} \times f[Y=1| A, L=1] \times {f[A| L=1]} \times Pr[L=1]\\
& + & \frac{I(A=1)}{f[A| L=0]} \times f[Y=1| A, L=0] \times {f[A| L=0]} \times Pr[L=0]\\
& = & f[Y=1| A, L=1] \times Pr[L=1] + f[Y=1| A, L=0] \times Pr[L=0]\\
& = & Pr[Y=1| A=1, L=1] \times Pr[L=1] + Pr[Y=1| A=1, L=0] \times Pr[L=0]\\
& + & Pr[Y=1| A=0, L=1] \times Pr[L=1] + Pr[Y=1| A=0, L=0] \times Pr[L=0]\\
& = & \sum_{l}Pr[Y=1|A=a,L=l]Pr[L=l] 
\end{eqnarray*}

Thus, we can show that, at least under this simplified situation, the IP weighted risk and standardized risk are equivalent.
\end{itemize}

\begin{figure}[h!]
\begin{align*}
f[Y,A,L] &= \underbrace{f[Y|A,L]  \times f[A|L] \times f[L]}_{Markov Factorization}\\
\frac{f[Y,A,L]}{f[A|L]} &= f[Y|A,L] \times f[L]\\
\underbrace{\displaystyle{\sum_{l} (\frac{f[Y,A,L]}{f[A|L]})}}_{IPW \; Estimator} &=
\underbrace{\displaystyle{\sum_{l} (f[Y|A,L] \times f[L])}}_{G-formula/Standardization}\\
\frac{f[Y,A]}{f[A]} &= f[Y|A]\\
f[Y|A] &= f[Y|A]\\
\end{align*}
\caption{The mathematical equivalence of IPW and standardization}
\label{stand_ipw}
\end{figure}

\begin{itemize}
\item in \em figure ~\ref{stand_ipw} \em note that each method perterbs the joint distribution to make the term $f[A\L]$ equal $f[A]=1$
\item that is, they simulate the data observed if $A=1$ for all persons \\ by making $A$ independent of $L$ while  preserving all other relationships in the data
\end{itemize}


\subsection{Stabilized Weights ($SW$)}
$$SW=\frac{f[A]}{f[A|L]}$$
\begin{itemize}
\item more efficient than unstabalized weights ($W$)
\item $SW$ can be used wherever $W$ is used, except with dynamic treatment regimes in which case $W$ must be used
\item for a static regime $\bar a=\{a_0,a_1\}$
$$SW=\frac{f[A_0]f[A_1|A_0]}{f[A_0]f[A_1|A_0,L_1]}=\frac{f[A_1|A_0]}{f[A_1|A_0,L_1]}$$
\item it is important to include $f[A_0]$ in IPW weight $W$ for dynamic regimes
\item \textit{fine point 1}:
	\begin{itemize}
	\item in \em unstabilized weighting \em the entire population (in its original size) is passed through all treatment regimes, such that the size of the pseudo-population equals the size of the oritingal population times the number of treatment regimes
	\item in \em stabilized weighting \em the entire population is again passed through all treatment regimes, however it is down-weighted by the probability in the numerator. The numerator is usually some function of treatment, so the different pseudo-populations reflect the size of the original treatment arms. 
	\item in the case where $f[A]$ is the numerator, the size of the population 'copy' passing through the treatment regime equals the size of the treatment group in the original population.
	\end{itemize}
\item \textit{fine point 2}:
	\begin{itemize}
	\item the denominator of IPW weight removes the association between $A$ and whatever is in the conditioning event (i.e. $1/f[A|L]$ removes $L \rightarrow A$)
	\item the numerator of stabilized weight restores the association between $A$ and whatever is in the conditioning event (i.e. $f[A|L_0]/f[A|L_0,L_1]$ removes $L_1 \rightarrow A$ but not $L_0
 \rightarrow A$)
	\item variables should be in conditioning event of numerator \textbf{only if} they are present in the conditioning event of the denominator
	\end{itemize}
\end{itemize}
\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{stabilized_IPW.pdf}
		\caption{Stabilized IP weighting creates a psuedo-population which has the same size as the original population}
		\label{stabilized_IPW}		
\end{figure}

\subsection{Censoring Weights ($CW$)}
$$CW=\frac{1}{f[C=0|A,L]}$$
$$SCW=\frac{f[C=0|A]}{f[C=0|A,L]}$$
\begin{itemize}
\item the main idea is to eliminate the path (conditioned collider) that was unblocked by censoring (figure ~\ref{censoring_IPW})

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{censoring_IPW.png}
	\caption{a) Study with selection bias due to censoring, b) Study population after censoring IP weights are applied}
		\label{censoring_IPW}
\end{figure}

\item we want to simulate what would have happened had everyone's outcome been observed ($Y^{a,c=1}$)
\item underlying assumption: $Y^c \amalg C|A,L$
	\begin{itemize}
	\item censoring is random within levels of $A$ and $L$
	\item no unmeasured variables $U$ affecting censoring
	\item within levels of $A$ and $L$, the $Pr[C=1]=Pr[C=0]$ 
	\end{itemize}
\item censored individuals have $w=0$ and non-censored individuals have $w=CW$
\item IP weights can be created that account for \em both \em confounding and censoring
	\begin{itemize}
	\item simply multiply the standardized weight for confounding by the standardized weight for censoring
	$$ \frac{f[A]}{f[A|L]} \times \frac{f[C=0|A]}{f[C=0|A,L]} $$
	\item $L$ is not necessarily the same for the confounding and censoring weights and is determined by the structure of the causal diagram
	\end{itemize}
\end{itemize}

\subsection{SMR}
$$SMW=\frac{f[A=a^ \prime|L]}{f[A|L]}$$
\begin{itemize}
\item for estimation of causal RR within a subset of population (i.e. SMR for exposed group) must use modified weights
\end{itemize}

\subsection{Effect modification}

\section{G-estimation}
\begin{itemize}
\item Null Hypothesis
  \begin{itemize}
  \item null hypothesis of no average causal effect: $E[Y^{a=1}] = E[Y^{a=0}]$
  \item sharp causal null hypothesis: $Y_{i}^{a=1} = Y_{i}^{a=0}=Y$ for all subjects $i$.
  \end{itemize}
\item G-null test
  \begin{itemize}
  \item A test of the sharp null hypothesis under the assumption that conditional exchangeability holds
  \item Conditional exchangeability  $ Y^{a} \amalg A|L=l $ can  be equivalently expressed as $A \amalg Y^{a} |L=l $ 
  \item We can model exposure on outcome  $logit\{Pr[A=1|L, Y^{a=0}]\}=\alpha_{0}+\alpha_{1}Y^{a=0}+\alpha_{2}L $
  \item Then we test whether $\alpha_{1} =0 $ in the model to see if sharp null hypothesis is true
  \end{itemize}
\item Structural mean model 
  \begin{itemize}
  \item $E[Y^{a}] =\beta_{0}+\beta_{1}a$
  \item This model is saturated marginal structural mean model
  \item This model can also be referred to as a structural nested mean model
  \end{itemize}
\item Structural nested model
  \begin{itemize}
  \item Structural nested model is composed of two models
  \item Structural model  $E[Y^{a}] =\beta_{0}+\beta_{1}a$
  \item Exposure model $logit\{Pr[A=1|L, Y^{a=0}]\}=\alpha_{0}+\alpha_{1}Y^{a=0}+\alpha_{2}L $
  \item Called nested model becuse structural model can be nested in the exposure model
  \end{itemize}
\item G-estimation
  \begin{itemize}
  \item  A method to estimate the parameters of structural nested models
  \item Let $H(\psi)$ be the couterfactual outcome, then the relation between $H(\psi)$ and treatment $(A)$ can be written as $H(\psi) = Y-\psi A$
  \item  One could then test that the coefficient of $H(\psi)$ to be zero in a model for the regression of A on $H(\psi)$ (exposure model). 
  \item  Grid search intervals of value of psi
    \begin{itemize}
    \item The coefficient that has a P-value equals 1 is the valid parameter estimate , and the set of all coefficients that have a p-value greater than $\alpha$ form a  $1-\alpha$   confidence interval.
     \end{itemize}
  \item model:
 	 \item $logit\{Pr[A=1|L, H(\psi)]\}=\alpha_{0}+\alpha_{1}H(\psi)+\alpha_{2}L $
	  \item $H(\psi) =  Y^{a=0} if \psi = \psi^{*}$ where $ \psi^{*}$ is the true value of $ \psi$
  \item  $\psi^{*}$ with 95 percent confidence interval has the causal interpretation for the effect of A on the counterfactual outcome
  \item $H(\psi)$ can be transformed to several function forms such as log or square root to increase the efficiency of g-estimation process
	\end{itemize}
\end{itemize}

\section{Instrumental Variables}
\subsection{Instrumental Variables}
\begin{itemize}
\item intrumental variables can be used to estimate causal effects without exchangeability
\item an instrumental variable Z
\begin{itemize}
\item has a causal effect on exposure A, or shares a common cause with A
\item affects outcome Y only through A 
\item there is no confounding on the effect of Z on Y
\end{itemize}
\begin{figure}[H]
  \begin{center}
    \subfigure[causal effect of Z on A]{\label{IV1} \includegraphics[scale=0.4]{IV.jpg}} \hspace{2 cm}
    \subfigure[common cause $U^{*}$ of Z and A]{\label{IV2}\includegraphics[scale=0.4]{IV2.jpg}} \hspace{2 cm}
  \end{center}
  \caption{Instrumental Variables}
  \label{IV}
\end{figure}
\item examples of instrumental variables
\begin{itemize}
\item cigarette prices is an instrumental variable Z, for the exposure of cigarette smoking A, and various health outcomes Y (\textit{figure 1a})
\item self reported lactose intolerance (Z) and dietary calcium intake (A) have a common cause in the lactose intolerance gene ($U^{*}$) with respect to osteoporosis (Y) (\textit{figure 1b})
\end{itemize}
\item standard IV estimator $E[Y^{\alpha=1}]-E[Y^{\alpha=0}]=\frac{E[Y|Z=1]-E[Y|Z=0]}{E[A|Z=1]-E[A|Z=0]}$
\item may use IP weights for censoring before applying IV estimator
\end{itemize}
\subsection{Limitations  of IV}
\subsubsection{Unverifiable assumptions}
\begin{itemize}
\item an instrument cannot be verified as one
\begin{itemize}
\item non-instruments introduce bias
\end{itemize}
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.4]{IV3.jpg}
  \end{center}
  \caption{Instrumental Variables}
  \label{IV3}
\end{figure}
\item weak instruments blow up bias (weak association of Z and A leads to small denomenator in the IV estimator)
\item standard IV methods do not deal well with time-varying exposures
\item need additional unverifiable assumptions to estimate causal effects
\\the IV estimator is a valid point estimate of causal effect under additional assutpions\\ without these assumptions an IV can only be used to calculate bounds for a causal effect 
\item additional assumptions are
\begin{itemize}
\item constant treatment effect across all subjects
\item no interaction between instrument and exposure (on either the additive or multiplicative scale)
\end{itemize}
\item neither of these assumptions can be verifiable
\item can be replaced by the assumption of monotonicity
\end{itemize}

\subsubsection{Non-compliance in randomized experiments}
\begin{itemize}
\item randomized experiments with full compliance assure the 3 identifiability assumptions of exchangeability, consistency and positivity
\item if noncompliance is not random, then exchangebility achieved with effective randomization is lost
\item random noncompliance leads to decreased power
\item noncompliance can also lead to violations of positivity when randomization is done taking into account covariates L
\end{itemize}
\subsubsection*{intention to treat and IV}
\begin{itemize}
\item treatment assignment is instrumental variable Z
\item causal effect of treatment A on outcome Y given by\\ $E[Y^{a=1}]-E[Y^{a=0}]=\frac{E[Y|Z=1]-E[Y|Z=0]}{E[A|Z=1]-E[A|Z=0]}$\\ the denominator equals 1 if full compliance\\ with non-compliance the intention to treat effect in the numerator is inflated
\end {itemize}
\subsubsection{Monotonicity}
\begin{itemize}
\item monotonicity is an alternative assumptions for use of instrumental variables to those of constant effect of treatment and no effect modification
\item 4 types of people in trial with randomized treatment (A) assignment (Z)
\begin {itemize}
\item always takers, $A^{Z=0}=1$,$A^{Z=1}=1$
\item never takers, $A^{Z=0}=0$,$A^{Z=1}=0$
\item compliers, $A^{Z=0}=0$,$A^{Z=1}=1$
\item defiers, $A^{Z=0}=1$,$A^{Z=0}=1$
\end{itemize}
\item monotonicity if no defiers
\item instrumental variable of treatment assignment measures effect in compliers
\item problems 
\begin{itemize}
\item we cannot identify the compliers
\item in continuous exposures everyone is a 'complier'
\end{itemize}
\end{itemize}
\subsection{IV as special case of g-estimation}
\begin{itemize}
\item standard IV does not work for time-varying exposures
\item have to use g-estimation of SNMs
\item using the simple $H(\psi)$ model we can do IV analysis by fitting the model \\
$logitPr[Z=1|H(\psi)]=\alpha_{0} + \alpha_{1}H(\psi)$
\item advantages of g-estimation IV analysis
\begin{itemize}
\item can use baseline covariates increasing efficiency and allow richer dose response function
\item use multiplicative model and assumption of no multiplicative scale interaction
\item 95\% CIs
\item extension to time-varying exposures
\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Inference with Models: Concepts \& Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Modeling Concepts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Modeling Concepts in Epidemiology}
\subsection{Estimators}
\begin{itemize}
\item \textit{Estimand}: the unknown population parameter, what we are trying to find out ($E$)
\item \textit{Estimate}: the result of applying the estimator to a particular data set ($\hat E$)
\item \textit{Estimator}: an algorithm or rule that when applied to the data estimates a parameter of interest
	\begin{itemize}
	\item \textit{Consistent}: $\displaystyle \lim_{n \rightarrow \infty} \hat{\theta} = \theta$
	\item \textit{Unbiased}: in repetitions of the study the 95\% ci contain the true value 95\% of the time 
	\item \textit{Biased}:  in repetitions of the study the 95\% CI does not contain the true value 95\% of the time
		\begin{itemize}
		\item systematically deviate from the true value of the effect measure
		\end{itemize}
	\end{itemize}
\item \textit{Bias of Estimator}: the average deviation of the estimator from the true value of the effect measure over study repetitions  $$E[\hat{\theta}-\theta]$$
\item \textit{Statistical Model}: a mathematical expression for a set of assumed restrictions on the possible state of nature
\item \textit{Nuisance Parameter}: any parameter (not of interest) that must be estimated in order to estimate the parameter of interest
	\begin{itemize}
	\item estimating these results in a loss of power for estimating the parameter of interest
	\end{itemize}
\end{itemize}

\subsection{Model Classes \& Types}
\subsubsection*{Broad Model Classes}
\begin{itemize}
\item \textit{Conditional}: parameters refer to subgroups of the population with particular covariate values 
\item \textit{Marginal}: parameters refer to the entire population under different covariate values
\item \textit{Structural}: parameters have a causal interpretation
\item \textit{Associational}: parameters only describe statistical relationships and may not have a causal interpretation
\item \textit{Nested}: a model can be expressed as a simplification of a larger, more complicated model,
	\begin{itemize}
	\item the result of placing constraints on the larger model
	\end{itemize}
\end{itemize}

\subsubsection*{Inference vs. Prediction}
\begin{itemize}
\item \textit{Prediction}
	\begin{itemize}
	\item only worry about validity of predicted value
		\begin{itemize}
		\item no requirement for conditional exchangeability, positivity, or consistency
		\end{itemize}
	\item concerned more about 
		\begin{itemize}
		\item overall model efficiency
		\item transportability (external validity)
		\end{itemize}
	\item concerned less about validity of paramater estimates
	\item covariate inclusion driven by efficiency, transportability, and clinical utility
		\begin{itemize}
		\item often use algorithms (stepwise or data-mining)
		\end{itemize}
	\end{itemize}
\item \textit{Causation} (Inference)
	\begin{itemize}
	\item only worry about validity of parameter estimate
		\begin{itemize}
		\item require conditional exchangeability, positivity, and consistency
		\end{itemize}
	\item concerned less about efficiency of overall model
	\item may or may not be concerned about transportability (external validity)
	\item covariate inclusion driven by subject-matter knowledge
	\item \textbf{only the parameter for \underline{exposure} has a causal interpretation}
		\begin{itemize}
		\item modelling decisions made to meet identifiability conditions for that parameter only
		\item other parameters may only have associational interpretation
		\item separate modeling process, covariate choice, etc. for \em each \em exposure of interest
		\end{itemize}
	\item model should \textit{not} be evaluated with goodness of fit tests
		\begin{enumerate}
		\item the number of good fitting models will be large
		\item these models will have a wide range for the effect estimate
		\item the range of effect-estimates from well fitting models may have a wider range than the model-specific standard errors
		\end{enumerate}
	\end{itemize}
\end{itemize}

\subsubsection*{Incorporating a priori information}	
\begin{itemize}
\item \textit{Non-Parametric}: 
	\begin{itemize}
	\item a function of the data only (no distributional assumptions)
	\item allows the data to ''speak for itself"
	\item synonym: \em saturated \em
	\item non-parametric estimators can be expressed in a model form if \dots
		\begin{itemize}
		\item \# paramaters = \# possible values for dependent variable
			\begin{itemize}
			\item i.e. the \# of quantities we want to estimate
			\end{itemize}
		\item includes all main effects of covariates and all possible interactions between them
		\end{itemize}
	\end{itemize}
\item \textit{Semi-Parametric}: 
	\begin{itemize}
	\item distributional assumptions for \em some \em of the paramaters
	\item the unspecified paramaters are allowed to follow any distribution
		\begin{itemize}
		\item subtle point: while the paramater itself may be unspecified, its estimation still requires the use of an estimator or algorithim
		\end{itemize}
	\item ex. cox-proportional hazards, splines, etc.
	\end{itemize}
\item \textit{Parametric}: 
	\begin{itemize}
	\item all paramaters have distributional assumptions
	\item fucntion of data and a priori information
		\begin{itemize}
		\item if assumptions correct, avoids estimating nuisance parameters and increases power for estimating paramater of interest
		\end{itemize} 
	\item synonym: unsatruated
	\end{itemize}
\end{itemize}
	
\subsubsection*{General Classes of Models}	
\begin{itemize}
\item \textit{Generalized Additive Model}: 
	\begin{itemize}
	\item example: $E[Y|X_1,X_2,\dotsc,X_p] = \alpha + f_1(X_1)+f_2(X_2)+\dots+f_p(X_p)$
	\item response variable is a sum of functions of the covariates
	\item not necessarily linear
	\item any functions suitable (linear, non-parametric, moving avearge, locally weighted mean, etc)
		\begin{itemize}
		\item example: smoothing functions (e.g. kernel, spline, etc.)
		\end{itemize}
	\end{itemize}
\item \textit{Generalized Linear Models}: 
	\begin{itemize}
	\item example: $E[Y|X_1,X_2,\dotsc,X_p] = \exp(\beta_1X_1+\beta_1X_2^2+\dots+\beta_pX_p)$
	\item the response variable is a function of a linear combination of the covariates via their \em parameters \em
	\item linearity means \dots
		\begin{itemize}
		\item linear (additive) function of the \em parameters \em on a particular scale (absolute, logarithmic, etc.)
		\item the covariates themselves do not have to be linear functions
			\begin{itemize}
			\item e.g. $x^2 \rightarrow$ still a linear model
			\item e.g. $\exp(\beta) \rightarrow$ still a linear model 
			\item e.g. $\beta^2 \rightarrow $ not a linear model
			\end{itemize}
		\end{itemize}
	\item Need to also specify
		\begin{enumerate}
		\item functional form (\textit{Link function}: logit, log-linear, etc.)
		\item statistical distribution of the residuals (normal, binomial, poisson, etc.)
		\item[-] includes linear, logistic, poisson, etc.
		\item[-] these assumptions can be relaxed (GEE \& random effects)
		\end{enumerate}
	\end{itemize}
\item \textit{Random-Effects Models}
		\begin{itemize}
		\item synonyms: heriarchical, multilevel, growth-curve
		\end{itemize}
\item \textit{Models for failure-time data}
	\begin{itemize}
	\item i.e. survival analysis
	\item parametric approaches: exponential, weibull
	\item semiparametric approaches: Cox proportional hazards model, Accelerated failure time model
	\end{itemize}
\end{itemize}

	
\section{Why we need models}

\subsection{Model Specification}
\begin{itemize}
\item the assumptions made in modelling strategy should reflect one's prior beliefs
\item causal inference requires the condition of \em no model misspecification \em
\end{itemize}

\subsection{The curse of dimensionality}
\begin{enumerate}
\item occurs when we need to control for many variables in order to satisfy identifiability conditions
	\begin{itemize}
	\item ex. for binary exposure, outcome and 10 binary covariates, the saturated model has $2^{11}=2048$ parameters
	\item even if assume no effect modification $\rightarrow$ $2^{10}+1=1025$ parameters
	\item \# parameters equivalent to \# 2x2 tables in stratified analysis
	\item we only care about the parameter for exposure, the rest are nuisance parameters
		\begin{itemize}
		\item very few strata likely to be informative (i.e. exposed and unexposed subjects)
		\item without assumptions one could learn very little about the effect of exposure in any schedule
		\end{itemize}
	\end{itemize}
\item in extreme cases the effects of exposure cannot be separated from the effects of covariates
	\begin{itemize}
	\item stratum level and exposure status are almost perfectly associated in the data
	\item i.e. lack of positivity prohibits identifiability of causal effect
	\end{itemize}
\end{enumerate}

\subsection{Supplementing the Data with A priori information}
\begin{itemize}
\item \textit{Sparse Data}: highly stratified data, but can obtain useful concidence intervals (e.g. matched pair analysis)
\item \textit{Weak Data}: no unbiased estimator for the paramater of interest has a variance small enough to construct useful confidence intervals (even with no unmeasured confounding)
	\begin{itemize}
	\item weak data \underline{requires} modelling assumptions
		\begin{itemize}
		\item weak data = sparse data always
		\item sparse data may or may not = weak data
		\end{itemize} 
	\end{itemize}
\item \textbf{modelling assumptions supplant the data with} a \em priori \em \textbf{information} $\rightarrow$ 
	\begin{itemize}
	\item restricts some or most of the nuisance parameters to equal zero
		\begin{itemize}
		\item hence reduces number of strata \& number of parameters to estimate
		\end{itemize}
	\item \em increases efficiency \em because more strata with larger numbers of exposed and unexposed subjects
	\item paramater for exposure will have narrower confidence interval
	\end{itemize}
\item usually prior beleifs for risk factors of disease are more ''sharp" than beleifs for predictors of expsoure
\item recall that a confounder has a structural definition based on subject matter knowledge
	\begin{itemize}
	\item its inclusion into the model should not be subject to a statistical test
	\end{itemize}
\end{itemize}	
	
\subsection{bias and efficiency trade-off}	
\begin{itemize}
\item saturated vs. reduced model
	\begin{enumerate}
	\item a highly stratified model will have small bias (provided no unmeasured confounding) but effect estimate may have larger variance
	\item a reduced model is more likely to be misspecified (and thus biased) but effect estimate has smaller variance
	\item the savings in variance afforded by modelling assumptions should offset the increase in bias that would result if those assumptions were incorrect
		\begin{itemize}
		\item true magnitude of such bias is unknown
		\end{itemize}
\item if one's assumptions are badly contradicted by the data one should be willing to give them up
\item if statistical test are used to verify assumptions, the $\alpha$ level should depend in part on the strength of one's beleifs
		\begin{itemize}
		\item stronger beliefs: more liberal rejection criteria $\rightarrow$ higher $\alpha$
		\item stronger beliefs: more restrictive rejection criteria $\rightarrow$ lower $\alpha$
		\end{itemize}	
	\end{enumerate}
	\item weak data refers to the kind of data that we cannot gain any information on the parameter of interest because the variance of the estimate was too large
	\item mean square error is the sum of variance of the estimate and the square of the bias
	\item saturated model $\Rightarrow$ less bias, larger variance (less efficient)
	\item reduced model $\Rightarrow$ more bias, smaller variance (more efficient)
	\item do not know whether saturated or reduced model has smaller mean square error
\end{itemize}	

\section{Stratification-based models}
\begin{itemize}
\item most common adjustment method in practice (ex:linear and logistic regression)
\item exposure and confounders are included as covariates in the model for outcome
\item stratification (standard regression) vs. g-methods:
	\begin{itemize}
	\item identifying assumptions (exchangeability, positivity, consistency) the same
	\item no model for exposure in standard regression
	\item model for the outcome is \em conditional \em on all covariates in standard regression
	\item standard regression cannot estimate the average causal effect in the whole population, unless the assumption of no effect modification is made
	\item standard regression has to adjust for effect modification because of conditional nature of the model (otherwise the model is misspecified)
	\item g-methods only incorporate effect modification if it is relevant to the research question
	\end{itemize}

\begin{figure}[h!]
	\includegraphics[scale=.5]{fig_ci_8_9.png}
	\caption{Fireman example where $A$=physical activity, $Y$=CHD, $C$=being a firefighter, $L$=parental SES, and $U$=attraction toward activities requiring physical activity}
	\label{fireman}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{hazardratio.png}
\caption{cannot distinguish between the two structures using hazard ratio as effect estimate}
\label{hazardratio}
\end{figure}

\item advantages of structural classification of bias:
	\begin{enumerate}
	\item structure of the problem guides choice of analytical methods
	\item structure may aid in study design, even when structure has no implications for data analysis, ex: firefighter study (figure~\ref{fireman})
	\item selection bias from conditioning on pre-exposure variables may help explain why covariates behave as confounders in one study and not another (m-bias)
	\item causal DAGs aid in communication among investigators
	\end{enumerate}

\end{itemize}	

\subsection{Biases in Stratified  Analysis}

\begin{figure}
	\includegraphics[scale=.65]{hazard_ratio.png}
	\caption{DAG depicting hazard ratio where $A=$ surgery, $Y_1=$ death at $t=1$, $Y_2=$ death at $t=2$, and $U=$ protective haplotype}
	\label{hazard}
\end{figure}

\begin{itemize}
\item a variable that affects survival at $t=1$ always affects survival at $t=2$
	\begin{itemize}
	\item effect of $A$ on the risk of $Y$ on the \em risk ratio scale\em:
		\begin{itemize}
		\item $t=1$: $Pr[Y_1=1|A=1]/Pr[Y_1=1|A=0]>1$, assuming A is causative
		\item $t=2$: $Pr[Y_2=1|A=1]/Pr[Y_2=1|A=0]>1$, even though $A$ does not have a direct effect on $Y_2$
		\item association of $A$ and $Y_2$ arises from indirect effect of $A$ through $Y_1$ (figure~\ref{hazard})
		\item risk ratio is a measure of risk of $A$ on total mortality through time $t$ (cumulative risk)
		\end{itemize}
	\item effect of $A$ on the risk of $Y$ on the \em hazard ratio scale\em:
		\begin{itemize}
		\item hazard ratio
			\begin{itemize}
			\item hazard: probability of dying between time $t$ and $t+1$, given you were alive at time $t$, $Pr[Y_{t+1}=1|Y_t=0]$
			\item hazard at t=1: $Pr[Y_1=1]$
			\item hazard at t=2: $Pr[Y_2=1|Y_1=0]$
			\item as opposed to hazards, risk is cumulative
			\end{itemize}
		\item $t=1$: $Pr[Y_1=1|A=1]/Pr[Y_1=1|A=0]>1$
		\item $t=2$: $Pr[Y_2=1|Y_1=0,A=1]/Pr[Y_2=1|Y_1=0,A=0]\neq1$
		\item association between $A$ and $Y_2$ is opened by conditioning on a collider ($Y_1$), figure
		\item hazard ratio neither measures the direct effect of $A$ on $Y_2$ or the effect of $A$ on total mortality through time $t=2$
		\item crossing hazards:
			\begin{itemize}
			\item $t=1$: $Pr[Y_1=1|A=1]/Pr[Y_1=1|A=0]>1$
			\item $t=2$: $Pr[Y_2=1|Y_1=0,A=1]/Pr[Y_2=1|Y_1=0,A=0]<1$
			\item after each successive time of follow-up, individuals more resistant against developing the outcome are selected for (\em survival bias \em where there is depletion of the susceptible individuals in the population)
			\end{itemize}
		\end{itemize}
	\end{itemize}

\begin{figure}[h!]
	\includegraphics[scale=.6]{varying_confounder.png}
	\caption{DAG depicting time varying confounder affected by treatment where $A_t=$ ART at $T=t$, $Y=$ viral load, $L_t=$ CD4 count at $T=t$, and $U=$immunosupression level}
	\label{time_confound}
\end{figure}

\item time varying confounders $L$ affected by exposure $A$
	\begin{itemize}
	\item ex: the effect of a time varying treatment ($A=A_0+A_1=0,1,$ or $2$) on outcome (figure~\ref{time_confound})
	\item necessary to block all back-door paths for $A$, that is for $A_1, A_2$, ...
	\item eliminates confounding for one component, but creates selection bias for another
	\item to stratify, or not to stratify
		\begin{itemize}
		\item not stratifying results in bias due to confounding
		\item stratifying introduces selection bias
		\end{itemize}
	\end{itemize}
\item need g-methods to \textbf{remove arrows}, rather than stratification methods that put a \textbf{box around} (i.e. \em condition \em on) the covariate
\end{itemize}
      \subsection{Parameter interpretation for associational models}
      \subsection{Parameter interpretation for structural models}

	  
\section{Model Specification}



\section{Hypothesis Testing \& Estimation}	  
	\subsection{hypothesis testing for $\beta$}
		\begin{itemize}
		\item Wald test for $H_{0}:\beta=0 \ vs \ H_{1}:\beta\ne0$ ($H_{0}:\mbox{rate ratio}=1 \ vs \ H_{1}:\mbox{rate ratio}\ne1$)
		$$\frac{\hat\beta^{2}}{\widehat{Var}(\hat\beta)} \sim {\chi^{2}_{1}}$$
		\item likelihood ratio test: $-2\log \bigg[ \frac{L(\beta)_{red}}{L(\beta_{full})} \bigg] \sim \chi^{2}$ \\ 
			\begin{itemize}
			\item use the same data set
			\item use nested model 
			\item the degree of freedom of the test is the difference between number of variables in two models
			\end{itemize}
		\item test statistics is a function of sample size, study design, and the evidence in the data supporting the hypothesis under test
		\end{itemize}
	
	\subsection{$95\%$ confidence interval for $\beta$}
		$$e^{log(\hat{RR})\pm z_{1-\alpha/2}\sqrt{\widehat{Var}[log(\hat{RR})]}} = e^{\Delta\times\{\hat\beta\pm z_{1-\alpha/2}\widehat{SE}(\hat\beta)\}}$$		
		\begin{itemize}
		\item[\-] where $\Delta$ is the the increment to be used for comparison with a continuous variable ($\Delta=1$ for binary variable)
		\item e.g. $e^{10\times{\hat\beta}}$ is the log rate ratio of death for each 10 -year increase in age
			\begin{itemize}
			\item we make the assumption that the rate vary linearly on the log scale with age (log-linear)
			\end{itemize}
		\end{itemize}
	

	
	  
	  
	  
	  
	  
\section{Interaction in Regression Models}

\subsection{Main Points}
	\begin{itemize}
	\item effect modification can be modeled in two ways
		\begin{itemize}
		\item by modeling a variable that defines all strata formed by the interacting variables
		\item by adding product terms of two or more variables in the model
		\end{itemize}
	\item the interaction term of continuous variable implies linearity assumption about the effect modification on the scale of the model (log scale for Cox model and Poisson model, log scale of odds for logistic model)
	\item the presence of effect modification can be tested 
		\begin{itemize}
		\item by testing the regression coefficient of the product term in the model using Wald test or likelihood ratio test 
		\item likelihood ratio test has better finite sample property than Wald test
		\item one could also do a test of homogeneity (i.e. $\beta_i=\beta_j$)
		\end{itemize}
	\item the magnitude can be informally examined by comparing the relative size of the parameters on the effect measure's scale
		\begin{itemize}
		\item $\exp(\beta_1)$ vs. $\exp(\beta_1+\beta_3)$
		\end{itemize}
	\end{itemize}

\subsection{Interpretation}
	\begin{itemize}
	\item logistic, poisson, cox, and conditional logistic are all multiplicative models
		\begin{itemize}
		\item in such models the baseline, or absolute risk, odds, or rate for the reference group is multiplied by the \em relative \em effect of being in the comparison group
		\end{itemize}
		
		\begin{align*}
		\text{let } g&= \text{ outcome or time to outcome} \\
		\alpha &= \text{ some arbitrary baseline function} \\
		g(x) &= \alpha \times \exp(\beta_1X_1+\beta_2X_2+\dots+\beta_pX_p) \\
		&= \alpha \times e^{\beta_1X_1} \times e^{\beta_2X_2} \times \dots \times e^{\beta_pX_p}
		\end{align*}

	\item thus an interaction represents a departure from (equivalent)
		\begin{itemize}
		\item additive effects on the log scale
		\item departure from purely multiplicative effect of \em only \em exposure
		\end{itemize}
			
		\begin{align*}
		g(x) &= \alpha \times \exp(\beta_1X_1+\beta_2X_2+\beta_3X_1X_2) \\
		g(x) &= \alpha \times e^{\beta_1X_1} \times e^{\beta_2X_2} \times e^{\beta_3X_1X_2}
		\end{align*}
	\item inferring interaction requires identifiability conditions to hold for both variables, otherwise we only infer effect-modification	
	\item with multiplicative models, inferring interaction on the additive scale or sufficient cause interaction requires special conditions and assumptions
		\begin{itemize}
		\item you can infer effect modification on the additive scale if it is qualitative (chane in sign)			
		\item if you can infer effect modification on the additive scale, you can further infer sufficient cause interaction if you assume monotonicity
		\end{itemize}
	\item studies often underpowered to detect effect (measure) modification or interaction
	\item unclear whether observed heterogeneity is real or only due to chance
		\begin{itemize}
		\item should be considered on the basis of plausibility, magnitude of effect, and random variation
		\item likelihood ratio tests often better for assessing effect modification
		\end{itemize}
	\item unlike stratification, using regression models allows you to assume that there is either
		\begin{itemize}
		\item interaction between all variables in model (saturated model)
		\item interaction between only some variables
		\end{itemize}
	\item the proportional hazards assumption can be assessed by examining if the hazard ratio for exposure varies over time. Tested by\dots
		\begin{itemize}
		\item reclassifying exposure according to time periods and see if period specific parameters are different
		\item assessing whether the product term between exposure and the time-scale is significant
		\end{itemize}
	\item assessing effect modification in conditional logistic regression follows same logic
		\begin{itemize}
		\item reclassifying exposure according to matching factor level and see if period specific parameters are different
		\item assessing whether the product term between exposure and the matching factor is significant
		\end{itemize}	
	\end{itemize}

\subsection{Review of Interaction terms}
	\begin{itemize}
	\item fitting separate models for each level of some external variable $Z$ is analagous to assuming interaction by $Z$ in a unified model
	\item for continuous variables, the effect of $X$ depends on the \em level \em of $Z$
	\item to interact categorical variables without assuming trend, need to interact the 'dummy' variables
		\begin{align*}
		g(x,z)= \alpha+\beta \sum_{i=1}^I I(X=i) + \beta \sum_{j=1}^J I(Z=j) + \beta \sum_{i=1}^I \sum_{j=1}^J I(X=i) \times I(Z=j)
		\end{align*}
	\end{itemize}
	
\subsection{Assessment of Interaction using Estimation}

	\begin{itemize}
	\item For all models let us assume
		\begin{itemize}
		\item $X_1$ and $Z_2$ are two binary covariates and $Y$ is outcome of interest
		\item $ln(\alpha)$ is the outcome for the baseline reference group
		\item $I(\text{argument})=1$ if true, and $=0$ if false 
		\end{itemize}
	\end{itemize}

\subsubsection*{Wald Statistic}

	\begin{itemize}
	\item if $\beta$ is the OR comparing $X=1$ to $X=0$ for a given level of $Z$
	\item[\-] and if $\beta_{z=1}$ and $\beta_{z=0}$ are uncorrelated then\dots
		
	\begin{align*}
	\text{Wald Statistic} &= \frac{(\beta_{z=1}-\beta_{z=0})^2}{\text{Var}(\beta_{z=1})+\text{Var}(\beta_{z=0})} \sim \chi^2_1 
	\quad \text{where}\dots \\ \\
	\text{Var}(\beta)&=\frac{(\log e^{\beta}-\log(\text{lower bound})}{(Z_{1-\alpha/2})^2}
	\end{align*}

	\item this is analagous to a test-based confidence interval
		\begin{itemize}
		\item the variance is under $\text{H}_0$ but should be under $\text{H}_A$
		\end{itemize}
	\end{itemize}
	
\subsection{Assessment of Interaction using Hypothesis Testing}
	
\subsubsection*{Heterogeneity}

	\begin{itemize}
	\item conceptually analagous to  \em effect modification \em
	\item $RR(X)_{z=1} \neq RR(X)_{z=0}$
	\item in regression\dots
		\begin{multline*}
		\\
		\text{Logit}Pr[Y=1|X=x,Z=z] =\alpha \exp(\beta_1I(X=1)\\+\beta_2I(Z=1) +\beta_3[I(X=1) \times I(Z=1)]
		\end{multline*}
		
		\begin{align*}
		&X=0,\ Z=0 \quad \text{Logit}Pr[Y=1|X=0,Z=0] =\alpha \\
		&X=1,\ Z=0 \quad \text{Logit}Pr[Y=1|X=1,Z=0] =\alpha \exp(\beta_1) \\
		&X=0,\ Z=1 \quad \text{Logit}Pr[Y=1|X=0,Z=1] =\alpha \exp(\beta_2) \\
		&X=1,\ Z=1 \quad \text{Logit}Pr[Y=1|X=1,Z=1] =\alpha \exp(\beta_1+\beta_2+\beta_3)
		\end{align*}
		
	\item[\-] Under null hypothesis of no effect modification $\{e^{\beta_1}=e^{\beta_1}+e^{\beta_3}\}$ and $\{e^{\beta_2}=e^{\beta_2}+e^{\beta_3}\}$
	\item[\-] $\rightarrow H_0$: $\beta_3=0$ or $e^{\beta_3}=1$
	\end{itemize}
	
\subsubsection*{Observed vs. Expected Joint Effects}

	\begin{itemize}
	\item conceptually analagous to \em interaction \em 
		\begin{align*}
		\underbrace{\frac{\text{Risk}_{X=1,Z=1}}{\text{Risk}_{X=0,Z=0}}}_\text{Joint effect of X \& Z} 
		\neq 
		\underbrace{\frac{\text{Risk}_{X=1,Z=0}}{\text{Risk}_{X=0,Z=0}}}_\text{Main effect of X} 
		\times 
		\underbrace{\frac{\text{Risk}_{X=0,Z=1}}{\text{Risk}_{X=0,Z=0}}}_\text{Main effect of Z} 
		\end{align*}
	\item in regression, create new variable defining subgroups\dots
		\begin{multline*}
		\\
		\text{Logit}Pr[Y=1|X=x,Z=z] =\alpha \exp(\beta_1[I(X=1)\times I(Z=0)]\\+ \beta_2[I(X=0) \times I(Z=1)] +\beta_3[I(X=1) \times I(Z=1)]
		\end{multline*}

		\begin{align*}
		&X=0, \ Z=0 & \quad \text{Logit}Pr[Y=1|X=0,Z=0] &=\alpha  \\
		&X=1, \ Z=0 & \quad \text{Logit}Pr[Y=1|X=1,Z=0] &=\alpha \exp(\beta_1) \\
		&X=0, \ Z=1 & \quad \text{Logit}Pr[Y=1|X=0,Z=1] &=\alpha \exp(\beta_2) \\
		&X=1, \ Z=1 & \quad \text{Logit}Pr[Y=1|X=x,Z=z] &=\alpha \exp(\beta_3) 
		\end{align*}
	\item $e^(\beta_1+\beta_2)$ and $e^{\beta_3}$ are, respectively, the expected and observed joint effect of $X$ and $Z$
	\item under null hypothesis of no interaction $e^{\beta_3}=e^{\beta_1} \times e^{\beta_2}$
	\item $\rightarrow H_0$: $\{\beta_3-(\beta_1+\beta_2)=0\}$ or $\{e^{\beta_3} \div (e^{\beta_1} \times e^{\beta_2})=1\}$
	\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Time}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Person-Time Analysis}

\subsubsection*{Definition}
\begin{itemize}
\item is the amount of time an individual contributes to a group's observation, usually in a homogeneous state of risk 
\item has units of $person \times times$
\end{itemize}

\subsubsection*{Estimation}
		
	\begin{itemize}
	\item There are 3 equaivalent ways of calculating person-time
	 \item[\-] in a study of $n$ subjects $i$, total person-time ($PT$) of observation = 
		\begin{align*}
		\textbf{(a)} \; \displaystyle \sum_{i=1}^{n} [time]_{i} \qquad \qquad \textbf{(b)} \; E\big[(time)_i\big] \times \displaystyle \sum_{i=1}^{n} i 
		\end{align*}
		
		\begin{align*}
		\textbf{(c)} \  \sum time \times E\bigg[\sum_{i=1}^{n} i\bigg]
		\end{align*}

	\item[\textbf{(a)}] for each person, identify amount of person time contributed to group's observation, then sum the times of individual persons to get total person time for the group
	\item[\textbf{(b)}] multiply the number of persons under observation by the average duration of observation per person
	\item[\textbf{(c)}] multiply the length of the period of observation by the average number of perons under observation during the period
	\end{itemize}
	
\subsubsection*{Assumptions}	
	\begin{itemize}
	\item classification by time intervals and covariates assumes that within strata
		\begin{itemize}
		\item i.e. risk of event is approximately constant during interval of interest
		\item the incidence rate is applicable to time point in the interval (i.e. it is constant)
		\end{itemize}
	\item this assumption means that the following are equivalent
		\begin{itemize}
		\item $n$ persons followed during $t$ units of time
		\item $t$ persons observed during $n$ units of time
		\end{itemize}
	\item strong cumulative effects and latency periods make assumption implausible
	\item solution is to divide follow-up period into very small intervals of homogeneous risk
		\begin{itemize}
		\item can use multiple time scales
		\end{itemize}
	\item assumes an induction period of zero
		\begin{itemize}
		\item analagous to crossover randomized trials
		\item no \em accumulation \em of risk
		\item effect of exposure is instantaneous
			\begin{itemize}
			\item resolved by complex exposure definitions:
				\begin{itemize}
				\item total pack-years
				\item lag or latency times
				\end{itemize}
			\end{itemize}	
		\end{itemize}
	\end{itemize}	

\subsubsection*{Group vs. Individual Data}

	\begin{itemize}
	\item in a dynamic population, when withdrawls and events occur uniformly\dots
		\begin{itemize}
		\item the incidence rate is constant
		\item average incidence rate = incidence density
		\item group data (average) or individual (density) level data will suffice for estimation of hazard
		\end{itemize}
	\item corresponds to actuarial life table methods (uniformity of events over interval)
	\item allows the comparison of \em average rate \em, based on an \em average population \em, to an incidence density based on person-years 
		\begin{itemize}
		\item in vital statistics $\rightarrow$ mid-point estimate at July 1
		\end{itemize}
	\end{itemize}	
		
\subsubsection*{Advantages when person-time is unit of analysis}		
	\begin{itemize}
	\item accounts for changes in individual experience in multiple categories of exposure over time
	\item cumulative effects of exposure can be taken into account with more complex definitions
	\end{itemize}
		
\section{Age-Period-Cohort Effects}
	\begin{itemize}
	\item age-period-cohort analyses can be applied to prevalence, incidence, and mortality data
	\item age, period, and cohort effects often distort the perception of how disease progresses over time or as persons age in a \em cross-sectional \em analysis
	\item cross-sectional data
		\begin{itemize}
		\item cannot be used to make \em future \em projections about the burden of disease
		\item can be used to assess the \em current \em burden of disease
		\end{itemize}
	\item can be difficult to tease apart, usually requires complex modeling (APC models) 
	\item note that \em age \em = \em birth cohort \em + \em age \em
	\end{itemize}
		
\subsection{Definitions}

\subsubsection*{Age Effect}
	\begin{itemize}
	\item change in the rate of a condition according to age, irrespective of birth cohort and calendar time
	\item a change in disease incidence that is due to a biological concomitant of aging
	\item Age is usually a confounder
		\begin{enumerate}
		\item associated with many exposure
		\item a strong risk factor for many health outcomes
		\end{enumerate}
	\item is usually the time-scale of choice in \em observational \em studies
	\end{itemize}
	
\subsubsection*{Period Effect}
	\begin{itemize}
	\item \textsl{Definition}
		\begin{itemize}
		\item change in disease frequency that are specific to a calendar time
		\item change in the rate of a condition affecting an entire population at some point, irrespective of age and birth cohort	
		\end{itemize}
	\item \textsl{cccurs when\dots}
		\begin{itemize}
		\item war, new treatment, massive migration, etc.
		\item introductions of new medications or preventative interventions
		\item changes in prevalence of exposures that have short induction periods
		\end{itemize}
	\item period effects for incidence rates are more prominent for diseases where cumulative effect of exposure is unimportant
		\begin{itemize}
		\item e.g. infectious disease, injury
		\end{itemize}
	\end{itemize}

\subsubsection*{Cohort Effect}
	\begin{itemize}
	\item \textsl{Definition}
		\begin{itemize}
		\item change in disease frequency shared by all members of a group who entered follow-up at a common time
		\item change in the rate of a condition according to year of birth, irrespective of age and calendar time		
		\end{itemize}
	\item \textsl{occurs when\dots}	
		\begin{itemize}
		\item lifetime experience of individuals born at a given point in time influence the disease or outcome of interest
			\begin{itemize}
			\item may \textbf{or may not} be related to circumstances at the time of birth of a given cohort
			\end{itemize}
		\item also viewed as an interaction between \em age \em and \em caldendar time \em
			\begin{itemize}
			\item or rather as a combination of age and period effect
			\end{itemize}
		\item usually driven by factors established in childhood or adolescence (always prior to disease)	
		\item cohort effects for incidence rates are more prominent for diseases where cumulative effects of exposure are important
			\begin{itemize}
			\item e.g. chronic diseases
			\end{itemize}
		\item in the absence of a cohort effect, the same age patterns for rates are found in cross-sectional and cohort curves	
		\item cohort effects can affect associations between disease outcomes and variables other than age
			\begin{itemize}
			\item e.g. case-control study matched on age
				\begin{itemize}
				\item if cases and controls identified from different birth cohorts
				\item where birth cohort is associated with exposure of interest
				\item birth cohort would be a confounder even though age is not
				\end{itemize}
			\end{itemize}	
		\end{itemize}	
	\end{itemize}
	
\section{Person-Time Data Structure}
\begin{itemize}
\item Typically, cohort  study  data  is  constructed  with  one  record  per  person

\begin{table}[h!]
\centering
\begin{tabular}{c c c c c}
	\hline \hline
	ID & Birth Year & Baseline Age & Baseline Year & Years of Follow-up \\
	\hline
	1 & 1985 & 19 & 2004 & 4 \\
	2 & 1984 & 18 & 2002 & 6 \\
	3 & 1984 & 20 & 2004 & 4 \\
	4 & 1985 & 18 & 2003 & 5 \\
	\hline \hline\\
\end{tabular}
\caption{Example dataset for an open cohort}
\label{time_table_wide}
\end{table}

\item Person-time data structure disaggreagate individual's follow-up period to single person-time unit and transformed the short-form (``wide") data to long-form data.
\item Person-time data strucure also called Anderson-Gill data structure or counting process data structure

\begin{table}[h!]
\centering
\begin{tabular}{c c c c c}
	\hline \hline
	ID & Birth Year & Age & Calendar Year & Year of Follow-up \\
	\hline
	1 & 1985 & 19 & 2004 & 1 \\
	1 & 1985 & 20 & 2005 & 2 \\
	1 & 1985 & 21 & 2006 & 3 \\
	1 & 1985 & 22 & 2007 & 4 \\
	\hdashline
	2 & 1984 & 18 & 2002 & 1 \\
	2 & 1984 & 19 & 2003 & 2 \\
	2 & 1984 & 20 & 2004 & 3 \\
	2 & 1984 & 21 & 2005 & 4 \\
	2 & 1984 & 22 & 2006 & 5 \\
	2 & 1984 & 23 & 2007 & 6 \\
	\hdashline
	3 & 1984 & 20 & 2004 & 1 \\
	3 & 1984 & 21 & 2005 & 2 \\
	3 & 1984 & 22 & 2006 & 3 \\
	3 & 1984 & 23 & 2007 & 4 \\
	\hdashline
	4 & 1985 & 18 & 2003 & 1 \\
	4 & 1985 & 19 & 2004 & 2 \\
	4 & 1985 & 20 & 2005 & 3 \\
	4 & 1985 & 21 & 2006 & 4 \\
	4 & 1985 & 22 & 2007 & 5 \\
	\hline \hline
\end{tabular}
\caption{Example of Anderson-Gill data structure}
\label{time_table_long}
\end{table}

\item Advantage of person-time data structure for data analysis
	\begin{itemize}
	\item Easily allows for time-varying exposures and confounders, with full ability to check programming
	\item Easily allows for exploration of alternate time scales
    \item Easily allows to switch from one to the other of the four methods presented for the analysis of cohort studies (Mantel-Haenszel, Cox, Poisson and pooled logistic regression)
 	\item Easily allows for discontinuous follow-up time; that is, it is a straightforward way to handle the situation where the same participants move in and out of follow-up 
	\item Easily allows for multiple events per person
	\item Allows for left-truncation and variable duration of follow-up (though this can also be done with one-record per person in Cox analyses)
	\end{itemize}
\item Scenarios that the counting process data structure is particularly well suited for
	\begin{itemize}
	\item \textbf{Time dependent covariates}: For example, the Framingham Heart Study, in which self-reported blood pressure is repeatedly assessed.
	\item \textbf{Alternate time scales}: For example, the uranium miners study, in which we can use age, years since starting to work, or calendar year as the time scale. 
	\item \textbf{Multiple events per subject}: For example, time to recurrent infections in patients treated with immunosupressor agents after organ transplantation in whcih the same subject can experience recurrent or multiple events.
	\item \textbf{Discontinuous intervals of risk}: For example, in a health insurance claim database, individuals may be on the health plan for an interval of years, then be off the health plan, then back on again.
	\item \textbf{Time dependent strata}: For example, in organ transplantation studies we can assess the predictive value of covariates in the patient outcome both before and after transplantation, in which transplant status (yes vs. no) is a time dependent stratification variable.
	\end{itemize}
\end{itemize}

\section{Time Scales \& Cohorts}
\begin{itemize}
\item time scales can be arranged in various manners based on what is optimal for the causal question of interest
\item practically speaking, variables are only suitable for use as a time scale if they do not stop accruing
\item allow for better control of confounding by time or periods of varying baseline risk (seasonality, secular trends, etc.) 
	\begin{itemize}
	\item however, only control for seasonality if it relates to the time scale chosen, otherwise need to model it explicitly
	\end{itemize}

	
\begin{figure}[h!]
\centering
	\includegraphics[scale=0.4]{scale_fuyear.png}\\
	\caption{Example of using cohort as time scale}
\label{time_frames1}
\end{figure}


\item using the cohort as the defining time scale (Figure ~\ref{time_frames1}) is often the time scale of choice in a randomized trial since exposure groups are exchangeable at baseline
\item year of birth (birth cohort) is often used to assess exposures that may be different in birth cohorts
	\begin{itemize}
	\item example: In 1899, maternal exposure to rubella during early pregnancy caused an increase in deafness in 10 year olds in 1909, 20 year olds in 1919, and 30 year olds in 1929
	\end{itemize}
\item \textbf{inception cohort}: individuals observed at the beginning of an exposure that defines a cohort, $t_0$
	\begin{itemize}
	\item the cohort as it exists at some initial point of observation
	\end{itemize}
\item \textbf{survivor cohort}: individuals who remain under observation at some time $ t_j $ after the initial point of observation $t_0$
	\begin{itemize}
	\item the cohort as it exists at some further removed point of observation
	\item all cohorts defined later than birth are to some extent a survivor cohort
	\item may result from selection bias due to loss of follow-up, competing risks, or depletion of susceptibles
	\end{itemize}

\begin{figure}[H]
\centering
	\includegraphics[scale=0.4]{scale_age.png}\\
	\caption{Example of using age as time scale}
\label{time_frames2}
\end{figure}

\item age (Figure ~\ref{time_frames2}) is often the timescale of use in observational studies because it stratifies risk sets based on age to rule out confounding due to age (age is often associated with both exposure and outcome)
	\begin{itemize}
	\item can further account for cohort effects by including birth (or entry) cohort into model
	\item \textit{note}: when age used as time scale, persons older than the case are excluded $\rightarrow$
	\end{itemize}

\begin{figure}[H]
\centering
	\includegraphics[scale=0.4]{scale_date.png}
\caption{Example of using period as time scale}
\label{time_frames3}
\end{figure}

\item risk sets can be used to investigate period effects that change over calendar time (Figure ~\ref{time_frames3})
	\begin{itemize}
	\item example: the way a disease is defined and diagnosed changes over time
	\item example: a change in a manufacturing process that alters exposure levels to an industrial toxin
	\item because risk sets match on time, the effect of the time scale cannot be estimated
		\begin{itemize}
		\item so the time scale should be something where you're not interested in estimating its effect
		\end{itemize}
	\end{itemize}
\item remember that you can always estimate effect-modification by the time scale which is essentially
testing the PH assuption	
\end{itemize}

\section{Time-Based Definitions of Exposure}

\begin{figure}
	\centering
		\includegraphics[scale=0.5]{exposuretime.pdf}
		\caption{Type of Exposure During Follow Up} 
		\label{exposuretime}
\end{figure}

\subsection{Concepts}
	\begin{itemize}
	\item person time classified based on exposure status according to study hypothesis
	\item any assumptions about induction and latent periods should be incorporated to study hypothesis	
	\item the aetiological window should be incorporated as well
	\item the time when exposure occurs and the time at risk of its effect are not necessarily the same	
	\end{itemize}

	\begin{itemize}	
	\item \textsl{Induction time} 
		\begin{itemize}
		\item time from exposure to disease onset 
		\item disease-free period after exposure, during which ''pathogenic" mechanisms work toward production of manifest disease
		\item \textsl{Residual effect} \\subsequent changes in dease incidience attributable to exposure	
		\item characteristic of a delayed rise in incidence rate after exposure
		\item minimum induction period: time elapsed until summed residual effects from all prior exposures become high enough to result in observable disease
		\end{itemize}
	\item \textsl{Latent period}
		\begin{itemize}
		\item time from disease onset to detection (exposure has no effect during it)
		\item disease is present but not manifest
		\end{itemize}
	\item \textsl{Etiological Window}
		\begin{itemize}
		\item the interval of time in which exposure is most relevant to cause of disease
		\item determines intervals of susceptibility
		\item person-time outside the etiological window should not be counted as exposed $\rightarrow$ bias towards null
		\item Examples:
			\begin{itemize}
			\item in utero, infancy, childhood, adolescence, etc.
			\item time in relation to menarche, pregnancy, childbirth, menopause
			\item calendar years
			\item \textbf{can be examined statistically through two-way interaction terms with age}
			\end{itemize}
		\end{itemize}
	\item \textsl{Chronic vs Point exposure}
		\begin{itemize}
		\item in \em chronic exposures \em, if we assume a long induction period,  we assume that exposure must accumulate to a certain level before having an effect
		\item same rationale for latent periods
			\begin{itemize}
			\item in this situation the time when exposure occurs $\neq$ time at risk of its effect
			\item classifying person-time as exposed during the induction period has the effect of independent non-differential misclassification of exposure (expected bias towards null)
			\end{itemize}
		\item in \em point exposures \em we assume negligible induction period.
			\begin{itemize}
			\item also known as \em acute \em exposure
			\item exposure occurs instantaneously
			\item in this situation time of exposure = time at risk of its effect are
			\end{itemize}
		\end{itemize}
	\item \textsl{Immortal person pime}
		\begin{itemize}
		\item entry criteria into cohort is dependent upon survival or meeting eligibility criteria
		\item should be excluded from analyses because it will downwardly bias estimated disease \underline{rate} for that group
		\item similarly for exposure categories, follow-up time should exclude time during which the exposure-category criteria are being met
		\end{itemize}
	\item \textsl{Post-exposure events}
		\begin{itemize}
		\item allocation of follow-up time should not depend on events that occur after it has accrued
		\item ''future should not determine the past"
		\item consider the randomized experiment you'd like to perform and classify person-time in your observational study accordingly
		\end{itemize}
	\end{itemize}

\subsection{Defining exposed and unexposed person-time}
	\begin{itemize}
	\item[\-] a choice between exposure definitions can be based on\dots
		\begin{enumerate}
		\item \em a priori \em considerations (subject matter knowledge)
		\item empirically (data driven) via comparison log-likelihood of non-nested models
			\begin{itemize}
			\item often not much power to determine which is exosure definition is more biologically relevant
			\item because they are related quantities
			\end{itemize}
		\end{enumerate}
	\end{itemize}
	
\subsubsection*{Exposure Definitions}
	\begin{itemize}
	\item \textsl{Categorical}
		\begin{itemize}
		\item events and person time \em must \em be in the same category
		\end{itemize}
	\item \textsl{Continuous}
		\begin{itemize}
		\item unlike cumulative, may increase or decrease over time
		\end{itemize}
	\item \textsl{Cumulative} 
		\begin{itemize}
		\item the summation of all exposures endured from $t_0$ until $t_1$
		\item e.g. time since\dots
		\item makes sense for ''one-hit" disease, where multiple hits are proportional to risk
		\item makes sense for disease where accumulated effect of exposure must meet a threshold for disease to occur
		\item for cumulative dose need: (1) age began/ended (2) frequency (3) intensity
		\end{itemize}
	\item \textsl{Composite measures}
		\begin{itemize}
		\item e.g. pack-years
		\item are necessary when exposure is a function of frequency, duration, intensity, and dose
		\item often components are best modelled as separate variables
			\begin{itemize}
			\item allows simultaneous interpretion of components and the composite measure
			\end{itemize}
		\end{itemize}
	\item \textsl{Intensity}
		\begin{itemize}
		\item average, maximum, median, minimum etc.
		\item makes sense for disease where \em current \em exposure must meet a threshold for disease to occur
		\item minimum $\rightarrow$ preventative (or beneficial) treatment (e.g. essential nutrients)
		\item maximum $\rightarrow$ causative (or harmful) treatment
		\item \textsl{Average lifetime exposure intensity}
			\begin{itemize}
			\item average exposure received by an individual where the average is taken over some specified time-frame
			\end{itemize}
		\end{itemize}
	\item \textsl{Dose}
		\begin{itemize}
		\item available dose, active dose, absorbed dose etc.
		\item be sure to account for method of exposure, route of administration, et.c
		\item should be comparable between exposed and unexposed
		\end{itemize}
	\item \textsl{Lagging exposures}
		\begin{itemize}
		\item exposures at or up to a specified time before current time
		\item to avoid confounding by subclinical disease during induction period
			\begin{itemize}
			\item avoidance or use of treatment/exposure $\leftarrow$ symptoms  $\rightarrow$ disease onset
			\end{itemize}
		\item to avoid reverse causation bias during latent period
		\end{itemize}
	\end{itemize}

\subsubsection*{Acitve Agent}
	
	\begin{itemize}
	\item often the active agent in an exposure depends on disease outcome and hypothesis
	\item i.e. in coffee $\rightarrow$ caffeine-parkinsons vs. nutrient-diabetes
	\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Continuous Exposures \& Confounders}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Indicator Method}

\begin{figure}
	\centering
		\includegraphics[scale=0.5]{drcurve.png}
		\caption{Comparison of Dose-Response Curves}
		\label{drcurve}
\end{figure}

\subsection{Reference coding}
	\begin{itemize}
	\item take a continuous variable $V$ with range [0,$\infty$]
		\begin{align*}
		\text{let } C_k&=\text{ threshold } k=\{1,2,\dots,k\}  \\
		\text{define } X&=
			\begin{cases} 
			1 \text{ if $V$ in } \big[0,C_1\big) \\
			2 \text{ if $V$ in } \big[C_1,C_2\big) \\
			\vdots	\\
			k \text{ if $V$ in } \big[C_k,\infty\big) \\
			\end{cases}
		\end{align*}
	\item defining an indicator function $I(X)$ to equal 1 if true, 0 if false\dots	
		\begin{align*}
		E(Y) &= \beta_0 + \beta \sum_{k=1}^{k-1} I(X=k) \\
			 &= \beta_0 + \beta_1I(X=1)+ \beta_2I(X=2)\dots \beta_{k-1}I(k-1)
		\end{align*}
		
		\begin{itemize}
		\item the category $k$ is the reference group to which all other categories are compared
		\item[\-] e.g. if logistic regression\dots
			\begin{align*}
			\beta_1 &= \bigg( \frac{Pr[Y=1|X=1]/Pr[Y=0|X=1]}{Pr[Y=1|X=k]/Pr[Y=0|X=k]} \bigg) \\
			\beta_2 &= \bigg( \frac{Pr[Y=1|X=2]/Pr[Y=0|X=2]}{Pr[Y=1|X=k]/Pr[Y=0|X=k]} \bigg) 
			\end{align*}
		\item when $X=k$, logit[$Pr[Y|X]=\beta_0$, so $\beta_0$ equals the log odds when $X=k$ and all other covariates have value $L=0$. So $I(X=k)$ is not included in the model
		\end{itemize}
	\item note that any category left out of the model is absorbed into the reference category
		\begin{itemize}
		\item if have categories $X=\{1,2,3,4\}$ but only include $I(X=1)$ and $I(X=4)$ in model, the refrence category becomes $I(X=2\cup3)$
		\end{itemize}
	\end{itemize}
	
\subsection{Assumptions}
	\begin{itemize}
	\item the dose response curve is a step function with abrupt changes in risk at the chosen thresholds $C_k$
		\begin{itemize}
		\item i.e. line with zero slope in each category
		\end{itemize}
	\item the paramater $\beta_k$ describing the relationship between outcome $Y$ and exposure $V$ is \textbf{homogeneous} over the range $[C_{k-1},C_k]$
	\item choice of thresholds and reference group are both biologically meaningful
	\item[$-$] for exposure $\rightarrow$ analagous to no misclassification
	\item[$-$] for confounder $\rightarrow$ analagous to no residual confounding
	\end{itemize}

\subsection{Practical issues}

\subsubsection*{Implementation}
	\begin{itemize}
	\item choosing the category with the largest sample size to be the reference group is the most efficient (powerful) use of data
	\item when interacting a categorized variable $X$ with a variable $Z$, must model \em all \em product terms between $Z$ and all included $I(X)$
		\begin{align*}
		E(Y|X,Z) = \beta_0&+\beta_1I(X=1)+\beta_2I(X=2)+\beta_3Z \\ &+\beta_4[Z\times I(X=1)]+\beta_5[Z\times I(X=2)]
		\end{align*}
	\end{itemize}

\subsubsection*{Strengths}
	\begin{itemize}
	\item the step function characterizing the dose-response curve can assume any arbitrary pattern
		\begin{itemize}
		\item i.e. does not assume a monotonic relationship
		\end{itemize}
	\item very interpretable, providing effect measures for each category
	\item More powerful to identify non-linear or threshold effect than linear trend test
	\item Less sensitive to outlier
   	\item Less influenced by extreme value in the end category where data is usually sparse
	\end{itemize}

\subsubsection*{Weaknesses}
	\begin{itemize}
	\item choice of (a) reference group (b) \# and location of thresholds is \em arbitrary \em\dots either use subject matter knowledge or evenly spaced intervals (quantiles)	
		\begin{itemize}
		\item the comparison of all categories to a common category may not be biologically relevant
		\item the thresholds may have no biological meaning
		\item may mask important trends in dose-response curve
		\end{itemize}
	\item using narrower (and hence, more) categories can decrease the potential bias from above issues, but will also decrease power
	\item Equal-distance categories may not indentify effects at extreme ends
	\item Unrealistic step-risk function (sudden change of risk once across the category boundary)
	\item Less statistical power and precision, throwing out of intra-category information and increasing degree of freedom
  	\item Potential data dredging due to manipulation of binning
	\item May sometimes violate monotonic increase or decrease assumption, which is biologically implausible
	\end{itemize}

\section{Score-Test}
\begin{itemize}
\item consider the simple regression model $logitPr[Y=1]=\beta_{0}+\beta_{1}A$, where A is a continuous exposure
\begin{itemize}
\item the Score-Test is a test used to test the hypothesis of $H_{0}:\beta_{1}=0$ vs $H_{a}:\beta_{1}\neq0$
\item the Score-Test is based on the score statistic $U(\beta_{1})=\frac{\partial(log(L(\beta_{0},\beta_{1})))}{\partial\beta_{1}}=\sum(Y-P)A$ where $P=P[Y|A]$\\
 and the information matrix $I(\beta_{1})=-\frac{\partial^{2}(log(L(\beta_{0},\beta_{1})))}{\partial\beta_{1}^{2}}=\sum{P(1-P)A}$ 
\item follows a $\chi^{2}_{(1)}$ distribution under the null
\item a score test that fails to reject the null $H_{0}:\beta_{1}=0$ does not mean no association
\item the exposure A can be replaced by any ordinal score vector S that is assigned to diffrent exposure levels of categorical variable
\item simple likelihood ratio test for the null hypothesis for categorical predictor of no association $H_{0}:\beta_{1}=...=\beta_{k}=0$ is $H_{a}=$not all $\beta{s}$ are equal
\item using scores S an alternative hypothesis with a natural ordering of a categorical predictors the alternatives $H_{a}:\beta_{1}<...<\beta_{k}$ or $H_{a}:\beta_{1}>...>\beta_{k}$
\end{itemize}
\item we can use a LRT test with the continuous exposure model nested within a model with categorical variables for the exposure to test if the linear model fits the data
\end{itemize}

\subsection{Linear trend test}
\begin{itemize}
\item Advantages
	\begin{itemize}
	\item More power and efficiency if the trend curve fit the linear function form
	\item Immune to data dredging due or manipulation of binning
	\item Avoid unreasonable assumption of sudden change of risk between categories
	\end{itemize}
\item Disadvantages
	\begin{itemize}
	\item Make a restrictive function form assumption of the trend curve (log-linear or logistic-linear)
	\item More sensitive to outlier
	\item Not robust to non-linear relationship
	\end{itemize}
\end{itemize}

\section{Fractional Polynomials}
\subsection{polynomial model}
\begin{itemize}
\item polynomial models were often used to examine the dose-response relationship without over-simplification of the dose-response relationship
\item in theory, one can approximate any smooth curve with enough polynomial terms
\item in reality, the number of terms required may be large and result in numerically unstable estimates
	\begin{itemize}
	\item models with polynomials greater than quadratic tend to be unstable and tend to produce artificial patterns
	\item on contrary, polynomial model was less flexible if only quadratic terms were included in the model
	\end{itemize}
\end{itemize}

\subsection{Fractional Polynomial model}
\begin{itemize}
\item fractions and inverse powers of the variable of interest ($X$) were suggested in addition to linear and quadratic terms
\item e.g. $E[Y]=\beta_{0} + \beta_{1}x^{1/2} + \beta_{2}x + \beta_{3}x^{-3/2} + \beta_{4}x^{2}$ (see figure \ref{drcurve})
	\begin{itemize}
	\item exponential and log terms ($e^{X}$ and $log(X)$) can also be used
	\end{itemize}
\item advantages of fractional polynomial model
	\begin{itemize}
	\item a simple qualitative dose-response or trend analysis can be done by adding power terms
	\item three different powers of $X$ between $x^{-2}$ and $X^{2}$ can give the model a wide range of shapes
	\item for logistic and log-linear model, include $ln(X)$ term in the model
		\begin{itemize}
		\item use $ln(x)$, the rate ratio,risk ratio, and odds ratio can be $e^{\beta ln(X)} = X^{\beta}$
		\item $X^{\beta}$ can increase slower than exponentially ($e^{\beta X}=[e^{X}]^{\beta}$) if $\beta \ge 1$
		\item $X^{\beta}$ can increase slower than linearly if $ 1 > \beta >0$
		\end{itemize}
	\item Allow change of slope and make use of all information 
	\item Increase power and precision to identify non-linear relationship
	\item More powerful to identify non-linear or threshold effect than linear trend test
	\item No assumption of step-risk function
   	\item More parsimonious model than categorical or spline regression
	\end{itemize}
\item disadvantages of fractional polynomial model 
	\begin{enumerate}
	\item covariate $X$ cannot be negative due to the fact that $X^{1/2}$ is not a real number if $X<0$. 
		\begin{itemize}
		\item it is better for x to have an absolute zero level which is coded as zero and all other levels of x are greater than zero 
		\item may add a value to make all $X$ are positive (however, need to decide what value to add)
		\end{itemize}	
	\item it is difficult to decide what terms to put in the model
		\begin{itemize}
		\item it is better that the shape of the curve was specified prior to model fitting
		\item otherwise, it's fishing
		\item however, it requires one to know each power term's effect on the shape of the curve to pre-specify the shape of the curve
		\item add quadratic term to increase the slope 
		\item add square-root term to decrease the slope
		\item add more extreme power terms to obtain more rapid change in shape		
		\end{itemize}
	\item outliers and influential points may affect the model fitting a lot
		\begin{itemize}
		\item regression diagnostics (e.g. residual plot, Cook's distance, goodness-of-fit, etc.) and $95\%$ confidence interval (CI) may be helpful to assess the model fitting
		\end{itemize}
	\item Unstable function forms, many FP functions fit equally well
   	\item Still parametric and less flexible than non-parametric (eg. categorical) methods
	\item Poor fit of end effects (compared to spline)
   	\item Parameter interpretation is difficult
	\end{enumerate} 
\end{itemize}

\section{Splines}
\begin{itemize}
\item splines are a sequence of joined segments that produces a piecewise pattern, also known as the ``broken-stick" model
\item provides a more flexible way to accommodate non-linear trends that cannot be approximated by simple polynomials
\item the basic idea is to divide the independent variable ($X$, $L$, etc) into a series of segments and fit piecewise trends having different functions but joined at fixed intervals
\item \textbf{knots}: are locations where segments are joined together, predetermined values $x^*$
\item \textbf{spline}: resulting piecewise linear curve
\item the graph of a spline looks like a series of connected line segments, it is piecewise linear but continuous
\item various spline functions can be considered
\item restrictions can also be placed on the splines (ex: linearity at extremes of the distribution, constraints to produce monotonic curves)
\end{itemize}

\subsection{Linear Splines}
\begin{itemize}
\item the simplest possible spline model is a linear spline with only one knot:
$$E(Y_{ij})=\beta_0+\beta_1X+\beta_2(X-x^*)_+$$
\item $x^*$ is equal to a predetermined value of $X$ where a knot is placed
\item $(X-x^*)_+$ is a function $g(X)$ defined as $max(0,\ X-x^*)$
	\begin{itemize}
	\item $g(X)$ equals 0 if $g$ is negative, and $X-x^*$ otherwise
	\item once $X>x^*$, the function $g$ always equals $X-x^*$
	\end{itemize}
\item more complex models may have multiple knots:
$$E(Y_{ij})=\beta_0+\beta_1X+\beta_2(X-x^*_1)_++\beta_3(X-x^*_2)_++\dots+\beta_{k+1}(X-x^*_K)_+$$
\item where $x^*_1$,  $x^*_2$, $\dots$,  $x^*_K$ are predetermined knots along the distribution of $X$ for the linear spline model
\item $\beta_k$ is the change in the slope of the dose-response going from category $k-1$ to category $k$
\item linear splines improve on indicator variables, which correspond to a step function, by guaranteeing the function is connected at knots
\item kinks in the function sometimes occur at knots, which are not biologically possible
\item linear splines can also suffer from instabilities that are sensitive to the choice of knots
\end{itemize}

\subsection{Quadratic and Cubic Splines}
\begin{itemize}
\item offer even greater flexibility over linear splines by allowing a curvilinear function for each of the splines
\item address the problem of kinks at the knots by creating a curve with no sharp bends
\end{itemize}
\subsubsection{Quadratic Splines}
\begin{itemize}
\item quadratic splines can be fit using the model:
$$E(Y_{ij})=\beta_0+\beta_1X+\beta_2X^2+\beta_3(X-x^*_1)^2_++\beta_4(X-x^*_2)^2_++\dots+\beta_{K+2}(X-x^*_K)^2_+$$
\item $\beta_k$ represents the change in the quadratic term (departure from linearity) of the dose-response function from category $k-1$ to category $k$
\item one additional parameter ($\beta_2X^2$) is needed to fit the quadratic spline
\item quadratic splines usually requires fewer knots for accuracy, thus the added parameter ($\beta_2X^2$) is offset by the fact that fewer parameters are needed
\item the quadratic spline has a smooth appearance because the same slope is shared on either side of the knots
\item like higher order polynomials, quadratic spline models can exhibit odd behavior at the extremes of the exposure distribution
	\begin{itemize}
	\item behavior of the lower tail can be improved by dropping $X^2$ from the model, that is restricting the fitted curve to be linear at this spline
	\item behavior of the upper tail can be improved by dropping $(X-x^*_K)^2_+$ and replacing all $(X-x^*_k)^2_+$ with  $\left((X-x^*_k)^2_+-(X-x^*_K)^2_+\right)$
	\end{itemize}
\end{itemize}

\subsubsection{Cubic Splines}
\begin{itemize}
\item splines can be further generalized to have cubic terms
\item cubic splines can be fit using the model:
$$E(Y_{ij})=\beta_0+\beta_1X+\beta_2X^2+\beta_3X^3+\beta_4(X-x^*_1)^3_++\beta_5(X-x^*_2)^3_++\dots+\beta_{K+3}(X-x^*_K)^3_+$$
\item $\beta_k$ represents the change in the cubic term of the dose-response function from category $k-1$ to category $k$
\item two additional parameters ($\beta_2X^2$ and $\beta_3X^3$) are required over linear spline models, and one additional parameter ($\beta_3X^3$) is required over quadratic spline models to fit the quadratic spline
\item cubic spline models are preferred by most statisticians
\end{itemize}

\subsection{Practical Considerations}
\begin{itemize}
\item Advantages
	\begin{itemize}
	\item Combine the advantages of categorical and power model
	\item Semi-parametric, flexible to capture dose response relationships of almost any shape
	\item require little specification of the dose-response relationship
	\item Smooth connection between knots no sudden change in risk across category boundary
	\item can get a p-value for a test of non-linearity
	\end{itemize}
\item Disadvantages
	\begin{itemize}
	\item Overfitting is likely and may affect generalizability
	\item may be strongly influenced by outliers
	\item can be sensitive to the choice of smoothing parameter (ex: linear, quadratic, cubic)
	\item difficult to get a summary relative risk for the dose-response relationship (best to look at a graph of the function)
 	\item Curves in the end categories (tails) may become very unstable for high power spline (can improve with restrictive spline)
            \item Effects depends on the number and value of knots while the selection of knots is usually visual or empirical
	\item not easily implemented in standard software packages, requires extra data manipulation
	\item can be mathematically complex and computationally intensive
	\end{itemize}
\end{itemize}
\section{Comparisons}
  \begin{figure}[H]
	\centering
		\includegraphics[scale=0.6]{drcompare.jpg}
		\caption{Comparison of different dose-response analytical strategies}
  \end{figure}
  
  \begin{table}
\centering
\begin{tabular}{ p{0.8in} p{1.7in} p{1.7in} p{1.7in}}
\hline \hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Assumptions}} & \multicolumn{1}{c}{\textbf{Strengths}} & \multicolumn{1}{c}{\textbf{Limitations}}\\
\hline
\textbf{\textsl{Single Measure}} & \textsf{one measurement accurately represents individual exposure, exposure remains constant} & \textsf{inexpensive, simple analysis, easily interpretable} & \textsf{susceptible to fluctuations in exposure, more susceptible to measurement error}\\
\hline
\textbf{\textsl{Multiple Measure}} & \textsf{exposure changes over time, one exposure measurement is insufficient} & \textsf{multiple assessments of exposure over time, can detect exposure trends over time} & \textsf{more resource intensive, requires complicated longitudinal analysis techniques}\\
\hline
\textbf{\textsl{Cum. Dose}} & \textsf{exposure bioaccumulates over time and is related to the outcome in a cumulative manner} & \textsf{able to investigate cumulative effect of an exposure on outcome} & \textsf{requires multiple measurements, lose information by collapsing measurements}\\
\hline
\textbf{\textsl{Average Dose}} & \textsf{effects of exposure over time are transient or trying to adjust for cumulative exposure over time} & \textsf{easy to analyze and interpret, compare individuals with different exposure times} & \textsf{requires multiple measurements, lose potential information by collapsing measurements into an average}\\
\hline
\textbf{\textsl{Min/Max Dose}} & \textsf{threshold level needed for exposure to initiate/prevent  outcome, duration not important} & \textsf{aids in identifying biological threshold, easy analysis, easily interpretable} & \textsf{requires multiple measurements, only uses data from one measurement point}\\
\hline
\textbf{\textsl{Exposure Duration}} & \textsf{assumes length of exposure is an important determinant of outcome} & \textsf{only necessary to measure if exposed and not exposure intensity, easy to analyze and interpret} & \textsf{does not factor in the intensity of the exposure}\\
\hline
\textbf{\textsl{Exposure Window}} & \textsf{assumes certain periods of exposure may be more important for outcome} & \textsf{can detect important windows of exposure effect, powerful analysis to detect time effect} & \textsf{need to determine what time scale is important, need to define time periods of interest}\\
\hline
\textbf{\textsl{Lag Exposure}} & \textsf{assumes an induction / latent period defined by the investigator} & \textsf{avoids misclassification of exposure by adjusting for induction/latent period} & \textsf{ignores etiological effects of exposure during induction time, throws away data}\\
\hline \hline \\
\end{tabular}
\caption{Comparison of Exposure Assessments}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Missing Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Types of Missing Data}

	\begin{enumerate}
	\item[\-] \em Define\dots \em $M$=\textit{missing data}, $Y_{miss}$=\textit{unobseverd data},  $Y_{obs}$= \textit{observed data}
	\item \textbf{MCAR}: \em Missing Completely at Random \em
		\begin{itemize}
		\item $M \amalg Y_{obs},Y_{miss}$
			\begin{itemize}
			\item[\-] $Pr(M|Y_{obs},Y_{miss})=Pr(M)$
			\end{itemize}
		\item the missingness process does not depend on collected or uncollected information
		\item to obtain a valid analysis there is \em no need \em to model the missingness process but it may be more efficient to do so
		\item example
			\begin{itemize}
			\item random data entry / processing errors
			\end{itemize}
		\item methods that are unbiased:
			\begin{itemize}
			\item complete case, IPW, single/multiple imputation, maximum likelihood
			\end{itemize}
		\end{itemize}
	\item \textbf{MAR}: \em Missing at Random \em
		\begin{itemize}
		\item $M \amalg Y_{miss}|Y_{obs}$	
			\begin{itemize}
			\item[\-] $Pr(M|Y_{obs},Y_{miss})=Pr(M|Y_{obs})$
			\end{itemize}
		\item the missingness process depends only on information you collected
		\item to obtain a valid analysis you must model the missingness process
		\item example:
			\begin{itemize}
			\item genetic epi study: choose to genotype only a subset of SNPs because the rest can be inferred from the linkage disequilibrium pattern in the genotyped SNPs
			\end{itemize}
		\item methods that are unbiased:
			\begin{itemize}
			\item IPW, single/multiple imputation, maximum likelihood
			\item complete case is multiplicative
			\end{itemize}
		\end{itemize}
	\item \textbf{MNAR}: \em Missing Not at Random \em
		\begin{itemize}
		\item $M \not \amalg Y_{miss}|Y_{obs}$
		\item[\-] $Pr(M|Y_{obs},Y_{miss}) = Pr(M|Y_{obs},Y_{miss})$
		\item example:
			\begin{itemize}
			\item study of substance abuse: persons with a criminal record are more likely to refuse questions about illict drug use. Unfortunately you did not collect information on criminal record, and the data you have don't give you information about the possibility of a criminal record.
			\end{itemize}
		\item methods that are unbiased:
			\begin{itemize}
			\item maximum likelihood
			\end{itemize}
		\end{itemize}
	\item[$-$] \textit{missing indicator is always biased}
	\end{enumerate}

	\begin{figure}
	\centering
		\includegraphics[scale=0.5]{miss_sum.png}
		\caption{Approaches for handling missing data}
		\label{miss}
	\end{figure}

\section{Complete Case}
\begin{itemize}
\item  Only use persons without missing data on any variable in model
\item  Unbiased in MCAR
  \begin{itemize}
   \item Example case-control study, MCAR 
   \item  $Pr((M=0|D,E, X_{1},..,X_{p})=Pr(M=0)=f$ for all participants, where $M=1$ means missing confouder
       \begin{figure}[H]
	\centering
		\includegraphics[scale=0.6]{completeCase1.png}
		\caption{Case-control data stratum i}
      \end{figure}
   \item Use complete data in stratum i $OR_{i}=\frac{fa_{i}fd_{i}}{fb_{i}fc_{i}}=\frac{a_{i}d_{i}}{b_{i}c_{i}}$ is a valid estimate
   \item Efficiency decraesed because sample size is reduced by $(1-f)\%$
   \end{itemize}
\item Unbiased in MAR if missingness if multiplicative
  \begin{itemize}
  \item MAR [$-$] \textit{multiplicatively}
  \item $Pr((M=0|D,E, X_{1},..,X_{p})=Pr((M=0|D,X_{1},..,X_{p})Pr((M=0|E, X_{1},..,X_{p})=f_{di}g_{ei}$
  \begin{figure}[H]
	\centering
		\includegraphics[scale=0.6]{completeCase2.png}
		\caption{Case-control data stratum i}
      \end{figure}
  \item $OR_{i}=\frac{f_{1i}g_{1i}a_{i}f_{0i}g_{0i}d_{i}}{f_{1i}g_{0i}b_{i}f_{0i}g_{1i}c_{i}}=\frac{a_{i}d_{i}}{b_{i}c_{i}}$ is a valid estimate
  \item In general, if each cell has a fraction of complete case $f_{11i},f_{10i},f_{01i},f_{00i}$, then complet case method is unbiased if $\frac{f_{11i}f_{00i}}{f_{10i},f_{01i}}=1$
  \item Efficiency reduced compared to unobserved full data
  \item For continuoius outcome, even though the missing process is multiplicative,  the complete case method is biased
  \end{itemize}
\end{itemize}

\section{Missing Indicator}
\begin{itemize}
\item  Recode missing data as zero, create indicator variable for missing then used it for stratified analysis or controll in regression model
\item  Not biased if not a confounder
\item  Guaranteed to be biased if a confounder (effect estimates are weighted avg. of nonmissing data (unbiased) and a missing data estimates(biased)). 
\item For missing data on many covariates, each missing data level must have variiation in the outcome $ \rightarrow$ at least 1 case and control
\item  Underestimates variability in dataset
\item Very efficient b/c get to use all of your data
   \begin{itemize}
   \item  A trade off between bias and efficiency, increase efficiency at a price of small bias if the proportion of missing data is small
\item[$-$] \textit{missing indicator is always biased}
   \end{itemize}
\end{itemize}

\section{Inverse Probability Weighting}
    \subsection{Single variable}
\begin{itemize}
\item observed data weighted to recreate the full (including unobserved) data
\item fit logistic regression for missingness
\begin{itemize}
\item simple example with outcome Y, exposure A, confounder L, $M=I(L missing)$
\item model $logit[Pr(M|A,Y)]=\beta_{0}+\beta_{1}Y+\beta_{2}X+\beta_{3}YA$
\item set weights $w(A,Y)=\frac{1}{Pr(M|A,Y)}$
\item then run weight regression of of Y on A,L (must use robust variance)
\end{itemize}
\item assumptions
\begin{itemize}
\item data missing at random (distribution of L the same in those where it is observed and those where it is unobserved)
\item missingness model is correctly specified
\item no missing data on A and Y
\end{itemize}
\end{itemize}
\subsection{Missingness Pattern}
\begin{itemize}
\item with multiple variables with misssing data use 'missingness pattern' 
\item covariates $L_{1},...,L_{k}$, $M_{i}=I(L_{i} missing)$
\item $M^{*}$ is a categorical variable with $2^{k}$ levels where k is the number of covariates with missing data
\item If $M_{1}=M_{K}=0$, then M*=1; if $M_{1}=1$ and $M_{2}=..=M_{k}=0$, $M^{*}=2$; etc.
\item fit polytomus logistic regression $logit[Pr(M^{*}=i|A,Y)]=\beta_{i0}+\beta_{i1}Y+\beta_{i2}A+\beta_{i3}AY$
\begin{itemize}
\item to minimize the number of parameters and strata for the polytomus logistic model we could make further assumtions
\item e.g $Pr(L|M^{*}=i)=Pr(L|M^{*}=j)$ for all $i,j>1$
\end{itemize}
\item fit weighted regression of Y on A,$L_{1},...,Z_{k}$, with weights $w(A,Y)=\frac{1}{Pr(M^{*}|A,Y)}$ (robust variance)
\item same assumtpion as with single variable
\end{itemize}

\section{Imputation}

	\subsection{Concepts}
		\begin{itemize}
		\item conceptually, imputation requires that the distribution of missing values, given the observed data, be specified
			\begin{align*}
			Pr(\mathbf{X}^{miss}|\mathbf{M},\mathbf{X}^{obs}]=
			\dfrac
				{
				Pr(\mathbf{M}|\mathbf{X}^{miss},\mathbf{X}^{obs},Y) 
					\times Pr(Y|\mathbf{X}^{miss},\mathbf{X}^{obs}) 
					\times Pr(\mathbf{X}^{miss},\mathbf{X}^{obs})
				}
				{
				\sum_{\mathbf{X}^{miss}} Pr(\mathbf{M}|\mathbf{X}^{miss},\mathbf{X}^{obs},Y) 
					\times Pr(Y|\mathbf{X}^{miss},\mathbf{X}^{obs}) 
					\times Pr(\mathbf{X}^{miss},\mathbf{X}^{obs})
				}
			\end{align*}
		\item[\-] where $\mathbf{M}=\{M_1,\dots,M_p\}$ is a vector of missing indicators \\ ($M_i=1$ if $X_i$ is missing, $0$ otherwise); $\mathbf{X}=\{X_1,\dots,X_p\}$ is a vector of covariates  
		\item to estimate this distribution we need three components:
			\begin{enumerate}
			\item a model for outcome $Y$, conditional on the complete set of covariates 
				\begin{itemize}
				\item $\mathbf{X}=(\mathbf{X}^{miss},\mathbf{X}^{obs})$
				\end{itemize}
			\item a model for the missingness process $\mathbf{M}$
				\begin{itemize}
				\item $Pr(Y=\mathbf{M}|\mathbf{X},Y)$
				\item if data is not MCAR does not have to specified \\ \dots $Pr(Y=\mathbf{M}|\mathbf{X},Y)=Pr(\mathbf{M})$ and it cancels out
				\end{itemize}
			\item a model for the joint distribution of the covariates $\mathbf{X}$
				\begin{itemize}
				\item $Pr(\mathbf{X})=Pr(\mathbf{X}^{miss},\mathbf{X}^{obs})$
				\item this is the part that makes imputation difficult
				\end{itemize}
			\end{enumerate}
		\item this involves more assumptions than we typically make in multivariate analysis
		\end{itemize}

	\subsection{Single Imputation}

		\subsubsection{Unconditional mean}
			\begin{itemize}
			\item replace missing values with crude mean of observed values
			$$ X_{ij}^{imputed}=\bar{X}_j=\frac{1}{\sum (1-M_{ij})} \sum(1-M_{ij})X_{ij}$$
			\item \textbf{biased} (in general) and not recommended
				\begin{itemize}
				\item paramaters are biased
				\item underestimates variability in $X_j$ and weakens
				\item weakens any associations with other $X$'s and outcome $Y$
				\item bias towards the null
				\end{itemize}
			\end{itemize}	
		\subsubsection{Unconditional draw}
			\begin{itemize}
			\item replace missing values with realizations from a probability distributions based on the observed values
			\item[\-] $X_{ij}^N{imputed} \sim \big(\bar{X}_{j},s_{j}^2 \big)$
			\item[\-] \em where\dots\em $s^2_j=\frac{1}{(\sum \{1-M_{ij}\}-1)} \sum (1-M_{ij})(X_{ij} - \bar{X}_{ij})^2$
			\item reduces variability i $X$'s
			\item unbiased if MCAR and distributional assumption correct
			\end{itemize}
		\subsubsection{Conditional mean}
			\begin{itemize}
			\item regress missing variable on other covariates
			\item[\-] $X_{ij}=\hat{\alpha}+\hat{\beta}Z_{i}$
				\begin{itemize}
				\item \textbf{never} include outcome of interest as a predictor for this approach
				\item will correlation between imputed variable and outcome where none exists
				\item imputing mean values does not preserve variability in population
					\begin{itemize}
					\item uses entire population unlike IPW, which only uses observed data and thus preserves the original variability
					\end{itemize}
				\end{itemize}
			\end{itemize}
		\subsubsection{Conditional draw}
			\begin{itemize}
			\item regress missing variable on other covariates, but include a random error term \\ $\epsilon \sim N(0,1)$
			\item steps
				\begin{enumerate}
				\item $X_{ij}^{imputed}=\hat{\alpha}+\hat{\beta}Y_{i}$
				\item $X_{ij}^{imputed} \sim N(\hat{\beta}Y_{i}, \hat{s}^2)$
					\begin{itemize}
					\item final predictions are sampled from mean of original predicted values but have an added random error component
					\end{itemize}
				\end{enumerate}
			\item reduces variablility in imputed $X$'s
				\begin{itemize}
				\item variance is constrained by distribution and modeling assumptions
				\end{itemize}
			\item is unbiased under MAR, but still underestimates variance of paramater estimates
			\end{itemize}		
		
\subsection{Multiple Imputation}
\begin{itemize}
\item the main idea is to ``fill in" missing data multiple times by replacing an unobserved variable by a random draw from the distribution of that variable conditional on the observed data
\item multiple imputation attempts to get an estimate of the $\beta$ parameter for $X$, our covariate of interest, rather than just estimating a value for $X_{missing}$
\item the advantage of multiple imputation over single imputation is it allows one to account for additional uncertainty due to the missingness
\item multiple imputation creates $M$ ``filled in" datasets to analyze and yield estimates of the parameter of interest $\beta$ ($\beta_1, \beta_2, \dots, \beta_M$)
\item the overall estimate of $\beta$ is the average of the $M$ estimates
$$\hat{\beta}=\frac{1}{M}\displaystyle \sum^M_{j=1}\hat\beta_j$$
\item the variance is estimated as
$$\hat{Var}(\hat\beta)=\frac{1}{M}\displaystyle \sum^M_{j=1}\hat\sigma^2_j+\left(1+\frac{1}{M}\right)\left(\frac{1}{M-1}\displaystyle \sum^M_{j=1}\left(\hat\beta_j-\hat\beta\right)^2\right)=A+\left(1+\frac{1}{M}\right)B$$
	\begin{itemize}
	\item $\hat\sigma^2_j$ is the estimated variance of $\hat\beta_j$ in the $j$th imputation
	\item$A$= the variance of each $\hat\beta_j$
	\item$B$= the variance among the $\hat\beta_j$'s, this helps account for the variability in the imputation due to uncertainty from missing data
	\end{itemize}
\item to conduct the hypothesis test that $\beta=0$ the following test statistic is used
$$\frac{\hat\beta}{\sqrt{\hat{Var}(\hat\beta)}}$$
\item this follows a t distribution with degrees of freedom=$(M-1)\left(1+\frac{A}{(1+M^{-1})B}\right)$
\item a key question with multiple imputation is how large $M$ should be
\item it turns out that $M$ does not need to be very large (Table ~\ref{RE_missing})

\begin{table}[h!]
\centering
\begin{tabular}{c | c c c c c}
	\multicolumn{6}{c}{Fraction of Missing Information}\\
	\hline\hline
	$m$ & $10\%$ & $20\%$ & $30\%$ & $50\%$ & $70\%$ \\
	\hline
	3 & 0.9677 & 0.9375 & 0.9091 & 0.8571 & 0.8108\\
	5 & 0.9804 & 0.9615 & 0.9434 & 0.9091 & 0.8772\\
	10 & 0.9901 & 0.9804 & 0.9709 & 0.9524 & 0.9346\\
	20 & 0.9950 & 0.9901 & 0.9852 & 0.9756 & 0.9662\\
	\hline\hline
	\multicolumn{6}{c}{ }\\
\end{tabular}
\caption{Relative efficiency of $M$ under different scenarios of missingness}
\label{RE_missing}
\end{table}

\item SAS Proc MI can be used to generate multiple imputed datasets
	\begin{itemize}
	\item the default $M$ is 5
	\item Proc MI can use a Markov Chain Monte Carlo (MCMC) procedure to impute missingness
		\begin{itemize}
		\item basically MCMC is a computationally intensive approach that breaks up a complicated distribution into simpler, smaller parts that can be sampled from
		\item it is a chain because the algorithm randomly jumps from one point in the sampling distribution to another
		\end{itemize}
	\end{itemize}
\end{itemize}


\section{Maximum Likelihood}
\begin{itemize}
\item can deal with NMAR
\item rely on the fact that $$Pr[M,X_{obs},Y]=\sum_{X_{missing}}Pr[M|X_{missing},X_{obs},Y]Pr[Y|X_{missing},X_{obs}]Pr[X_{missing},X_{obs}]$$
\item we may correctly impute $\{X_{missing},X_{obs}\}$ if we can correctly specify $3$ models for each component of $Pr[M,X_{obs},Y]$
\item also, we need to integrate the function over all possible $X_{missing}$
\end{itemize}

\pagebreak

\begin{sidewaystable}[h!]
	\vspace{40em}
	\caption{Comparison between methods for missing data methods}
	\centering
		\begin{tabular}{ | p{1.5in} | p{1.8in} | p{1.8in} | p{1.8in}  | p{1.8in}  |}
		\hline 
		\textbf{Method} & \textbf{Assumption} & \textbf{Validity} & \textbf{Efficiency} & \textbf{Implementation} \\ & & & & \\ \hline
		\textsl{Complete case} & \textsf{MCAR} & \textsf{unbiased if assumption holds} & \textsf{inefficient} & \textsf{easy}\\  & & & & \\ \hline
		\textsl{Missing indicator} & \textsf{No assumption} & \textsf{always biased} & \textsf{a little more efficient than complete case analysis}  & \textsf{difficult if more than} $1$ \textsf{variable} \\  & & & & \\ \hline
		\textsl{IPW} & \textsf{MAR and no model mis-specification (or MCAR)} & \textsf{unbiased if assumption holds} & \textsf{depends on the accuracy of missing data prediction given observed data} & \textsf{easy to get point estimates, difficult to get variance estimates}\\  & & & & \\ \hline
		
		
		\textsl{Unconditional single mean imputation} & \textsf{distribution of the variable (no model mis-specification)} & \textsf{generally biased (underestimates variance of the variable and weaken the association between the variable and the outcome)} & \textsf{depends the accuracy of missing data prediction given observed data} & \textsf{easy}\\  & & & & \\ \hline
		
		\textsl{Unconditional single draw imputation} & \textsf{distribution of the variable (no model mis-specification)} & \textsf{generally biased (underestimates variance of the variable and weaken the association between the variable and the outcome)} & \textsf{depends the accuracy of missing data prediction given observed data} & \textsf{easy}\\   & & & & \\ \hline
		
		\textsl{Conditional single mean imputation} & & \textbf{\textsf{It creates biased results!!!}} & &\textbf{\textsf{do NOT use this method}} \\  & & & & \\ \hline
		
		\textsl{Conditional single draw imputation} & \textsf{distribution of the variable given observed data (no model mis-specification)} & \textsf{unbiased if MAR} & \textsf{estimates variance of imputed variable better that mean imputation} & \textsf{easy}\\  & & & & \\ \hline
		
		\textsl{Maximum likelihood} & \textsf{distribution of the variable (no model mis-specification)} & \textsf{unbiased if no model mis-specification}  & \textsf{efficient} & \textsf{difficult}\\  & & & & \\ \hline
		\end{tabular}
	\label{tab:missingdatacomparison}
\end{sidewaystable}
\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Inference With Models: Types}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Models for Statistical Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{stratified analysis using regression models}
		\begin{itemize}
		\item examples
			\begin{itemize}
			\item linear regression: $Y=\beta_{0}+\beta_{1} X_{1} +\epsilon$ 
			\item logistic regression: $logit\{E[Y|X]\}=\beta_{0}+\beta_{1} X_{1} $ 
			\item Poisson regression (for rate) : $log\{E[Y]\}=\beta_{0}+\beta_{1} X_{1}+log\{person-time\} $
			\item Cox proportional hazard model: $h_{X}(t)=h_{0}(t) e^{\beta_{1}X} $
			\end{itemize}
		\item exposures and confounders ware both included in the model as covariates
		\item \textbf{must condition on all covariates in the model}		
		\item require exchangeability, positivity, and consistency assumptions

		\item no average causal effect in the population was estimated 
			\begin{itemize}
			\item unless we assume that there is a constant average causal effect across strata
			\end{itemize}
		\end{itemize}

	\begin{table}[h!]
	\centering
	\caption{Interpretation of Regression Coefficients}
	\begin{tabular}{l l p{2.0in}}
	\hline \hline
	\textbf{Form} & \textbf{model} & \textbf{Interpretation} \\
	\hline
	& & \\
	Logistic 	& $\text{log(odds)}=\beta_0 + \beta_1X_1 + X^{\prime}\beta$	& Increase in the \em log odds \em of outcome per unit increase in $X_1$, adjusted for all other variables in the model \\ & & \\
	Cox 		& $\text{log(hazard)}=\beta_0 + \beta_1X_1 + X^{\prime}\beta$	& Increase in the \em log hazard \em of outcome per unit increase in $X_1$, adjusted for all other variables in the model \\ & & \\
	Poisson 	& $\text{log(rate)}=\beta_0 + \beta_1X_1 + X^{\prime}\beta$			& Increase in the \em log rate \em of outcome per unit increase in $X_1$, adjusted for all other variables in the model \\ & & \\
	\hline \hline
	\end{tabular}
	\label{tabreg1}
	\end{table}
	
	
	\pagebreak

	
	\begin{sidewaystable}[h!]
	\vspace{40em}
	\centering
	\caption{Summary of Regression Models in Epidemiology}
	\begin{tabular}{ p{1.5 in} p{2.25 in} p{2.25 in} p{2.25 in}}
	\hline \hline
	\textbf{Model} & \textbf{Assumptions} & \textbf{Strengths} & \textbf{Weaknesses} \\
	\hline 
	& & & \\
	%row------------------------------------
	\multicolumn{2}{l}{\textsl{Unconditional Logistic}} & & \\
	\textsf{relative risk} 			& \textsf{Fixed follow up, constant risk within strata, time-to-event not important, odds a linear fxn of covariates on exp scale, independence}
									& \textsf{In matched case-control, if OR does not vary by matching factor can reduce assumptions about matching factors (collapse them) or remove them entirely; estimate baseline risks if had sampling fractions and external data}
									& \textsf{CASE-CONTROL: can be inefficient in matched case-control. COHORT: not appropriate for open cohorts (staggered entry, loss to follow up) or time trends in risk (variable induction periods)}	
									\\
									& & & \\
	\hline 
	%row------------------------------------								
	\multicolumn{2}{l}{\textsl{Poisson (log-linear)}} & & \\
	\textsf{rate ratio}				& \textsf{Open follow up, constant rate within strata, rate a linear fxn of covariates on exp scale, poisson process: rare outcome, fine strata, independence}
									& \textsf{Model baseline rate; censoring not an issue; good for rare outcomes; can test if baseline rate varies over time; can get effect for time-scale}
									& \textsf{Could mis-specify baseline rate, not as efficient as cox}
									\\
									& & & \\
	\hline 
	%row------------------------------------
	\multicolumn{2}{l}{\textsl{Conditional Logistic}} & & \\
	\textsf{matched relative risk}	& \textsf{Within strata...(see logistic)}
									& \textsf{Efficient for matched case-control studies, analyzes in risk-sets; can still test if OR varies by matching factors}
									& \textsf{No paramater estimates for baseline odds or main effect of matching factors; harder to reduce assumptions about matching factors}
									\\
									& & & \\
	\hline 
	%row------------------------------------								
	\multicolumn{2}{l}{\textsl{Pooled Logistic}}	& & \\
	\textsf{hazard ratio}	& \textsf{COHORT: Outcome is rare over narrowly defined intervals}
									& \textsf{Model baseline rate}	
									& \textsf{Must account for censoring, could mis-specify baseline}
									\\
									& & & \\
	\hline 
	%row------------------------------------
	\multicolumn{2}{l}{\textsl{Cox}} & & \\
	\textsf{hazard ratio}			& \textsf{Hazards are proportional, risk-sets are independent, uninformative censoring}
									& \textsf{Efficienty, does not model baseline rate, common outcomes}
									& \textsf{PH assumption may not hold; no absolute rates; cannot test if baseline rate varies; can't get effect for time-scale}
									\\ 
									& & & \\
	%row------------------------------------
	\hline \hline 
	\end{tabular}
	\end{sidewaystable}
	
	\pagebreak


\section{Linear Regression}

\section{Mantel Hanszel as a Regression Function}
\subsection{M-H Estimator}
\begin{itemize}
\item Mantel-Haenszel (MH) analysis for an open cohort study

\begin{center}
\begin{tabular}{rccl}
	&\multicolumn{2}{c}{\textbf{open cohort}}\\
	& \ exposed \ & unexposed \\
	\cline{2-3} 
	\multicolumn{1}{r|}{cases} & 			\multicolumn{1}{c|}{$a_i$} & \multicolumn{1}{c|}{$b_i$} & $M_1$ \\
	\cline{2-3}
	\multicolumn{1}{r|}{person-time} & \multicolumn{1}{c|}{$N_{1i}$} & \multicolumn{1}{c|}{$N_{0i}$} & $T_i$ \\
	\cline{2-3}
	\\
\end{tabular}
\end{center}

\item recall the summary Mantel-Haenszel rate ratio is equal to:

$$ \hat{IRR}_{MH}=\frac{\displaystyle \sum^I_{i=1}\frac{a_iN_{0i}}{T_i}}{\displaystyle \sum^I_{i=1}\frac{b_iN_{1i}}{T_i}}$$

\item with variance equal to:

$$ \hat{Var}\left(log(\hat{IRR})\right)=\frac{A}{BC} $$

\item where:
	\begin{itemize}
	\item $ A=\displaystyle \sum^I_{i=1}\frac{M_{1i}N_{1i}N_{0i}}{T^2_i}$, the variance of the $IRR_{MH}$
	\item $ B=\displaystyle \sum^I_{i=1}\frac{a_iN_{0i}}{T_i} $, numerator of $IRR_{MH}$
	\item $ C=\displaystyle \sum^I_{i=1}\frac{b_iN_{1i}}{T_i} $, denominator of $IRR_{MH}$
	\end{itemize}
\end{itemize}
	
\subsection{M-H Equation}
\begin{itemize}
\item the multivariate model implicitly assumed by the Mantel-Haenszel analysis is:

\begin{align*}
I(t|A, L_1, L_2, L_3) &= I_{0jkl}(t)\times e^{\beta_1A(t)}\\
I(t) &= I_0(t)\times e^{\beta_1A(t) + \beta_2L_1 + \beta_3L_2 + \beta_4L_3 + \beta_5 L_1L_2 + \beta_6L_1L_3 + \beta_7L_2L_3 + \beta_8L_1L_2L_3}
\end{align*}

\item if interested in a dichotomous outcome, MH can be generalized to logistic regression where an $OR_{MH}$ is estimated, which approximates the $IRR$ under the assumption of a rare outcome
\item baseline varies over time $t$, but other measures of time can be included into $L$ (ex: time of day of blood draw, season of blood draw, time sample stored in freezer, etc)
\end{itemize}

\subsection{Assumptions}
\begin{itemize}
\item the baseline hazard $I_{0jkl}(t)$ \em is \em correctly specified (not really an assumption because it is a saturated model)
\item the model is saturated for possible confounders ($L_1,\ L_2,\ L_3$) and interactions between confounders, making it robust against model misspecification
	\begin{itemize}
	\item avoids potential bias, but not statistically efficient
	\end{itemize}
\item there is no effect modification between treatment $A$ and confounders $L$, that is the effect of $A$ is constant across strata of $L$
\item there is no effect modification between treatment $A$ and time $t$, that is the effect of $A$ is constant over time
\item there is no residual confounding due to $L$, no unmeasured confounding due to $U$, no selection bias, and no information bias
\end{itemize}

\subsection{Parameter Interpretation}
\begin{itemize}
\item $\beta_1$ is the combined log M-H incidence rate ratio comparing those on treatment ($A=1$) to those not on treatment ($A=0$) holding the level of $L_1$, $L_2$, and $L_3$ constant
\item $\beta_2 - \beta_8$ are implied by the M-H model, but not estimated in the stratified analysis
\end{itemize}


\section{Unconditional Logistic Regression}
     \subsection{Likelihood Function}
	  \begin{itemize}
	  \item the likelihood function for the unconditional logistic regression is:
	  \item  $L(\theta|A, x_{1},...,x_{p}) = \prod_{i}\frac{exp(\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p})}{1+ exp(\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p})}$ 
	   \end{itemize}
     \subsection{Regression Equation}
	  \begin{itemize}
	  \item  $Pr(Y=1|x_{1},...,x_{p}) = \frac{exp(\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p})}{1+ exp(\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p})}$, or 
	  \item $Logit[Pr(Y=1|x_{1},...,x_{p})] = \beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p}$ 
	  \end{itemize}
     \subsection{Assumptions}
	  \begin{itemize}
	  \item The mean of the response variable $(Logit[Pr(Y=1)])$ is linearly related to the explanatory variables $(x_{1},\dots,x_{p})$
	  \end{itemize}
     \subsection{Parameter Interpretation}
	 \begin{itemize}
	 \item $\beta$ can be interpreted as log odds ratio comparing X=1 to X=0 (binominal variable) or log odds ratio associated one unit change of X (contiuous variable) controlling for other variables
	 \item $exp(\beta)$ is the odds ratio 
	 \item the effect estimates only has causal interpretaton within the level of of covariates, unless assuming constant effects across level of covariates
	 \end{itemize}
	 \subsection{Relationship between log-linear model and logistic regression}
		\begin{itemize}
		\item log-linear model for number of cases in $j^{th}$ risk set: $log(E[X_{j}])=\beta_{1}+\beta_{2}Z_{1j}+log(t_{j})$
		\item log-linear model for number of cases in $j^{th}$ risk set based on covariate $Z_{1}$: $log(E[Y_{j}])=\gamma_{1}+log(t_{j})$
		\item logistic regression in $j^{th}$ risk set:
		\item[\-]$log(E[X_{j}])-log(E[Y_{j}])=(\beta_{1}-\gamma_{1})+\beta_{2}Z_{1j}$
		\item[\-]$log(E[X_{j}]/E[Y_{j}]) = \beta_{1}^{\prime} +\beta_{2}Z_{1j}$
		\item[\-]$log(E[P_{j}]/(1-E[P_{j})])=\beta_{1}^{\prime}+\beta_{2}Z_{1j}$
		\end{itemize}
 

\section{Poisson} (\textsl{Log-Linear})

	\begin{itemize}
	\item models \em average count \em, adjusted for length of follow up time when an offest is used
		\begin{itemize}
		\item in this situation parameters have interpretation as average log incidence rates
		\item not instantaneous incidence rate\dots e.g. hazard
		\end{itemize}
	\item akin to a person-time analysis
	\end{itemize}

\subsubsection*{A review of Poisson Distribution}
	\begin{itemize}
	\item Let $Y=$\# events, $\mu=$ average \# events, $T$=follow up time, $\lambda=$incidence rate
	\item if $Y\sim$ Poisson $(\mu= \lambda T)$ then it's expected value depends on the length of follow up and a constant incidence rate\dots
		\begin{align*}
		E(Y)=\mu=\lambda T
		\end{align*}
	\item the probability mass function is given as\dots
		\begin{align*}
		Pr[Y=y]&=(\mu^ye^{-\mu})/Y!
		\end{align*}
	\item also $\Delta T=\displaystyle \lim_{\Delta \rightarrow 0} t<k<t+\delta$ 
	\item thus this distribution assumes that (a) within fine intervals the event is rare (b) the total number of events is dependent on (i) a constant incidence rate and (ii) the length of follow up\dots
		\begin{enumerate}
		\item $P(Y=1|\Delta T)\approx\lambda T$
		\item $P(Y\geq2|\Delta T)\approx0$
		\item $P(Y=1|\Delta T)$ is constant over time
		\item events occur independently over time
		\item constant incidence rate within strata (if stratified)
		\end{enumerate}
	\end{itemize}

\subsection{Likelihood Function}
	\begin{itemize}
	\item we use the PMF to construct the likelihood function
		\begin{align*}
		L(\beta_0,\beta_1)&=\prod_{i=1} P(Y_i=y_i|\lambda_i,T_i=t_i,X=x_i)=   \prod_{i=1} \left( \big( \mu_i^{y_i}e^{-\mu_i} \big) \middle/ y_i! \right)  \\
		&where \dots \\
		\mu_i&=\exp\big[\beta_0+\beta_1+log(t_i) \big] \quad and \quad \log(t_i) = \text{offset for subject }i
		\end{align*}
	\item to see that the likelihood assumes a constant incidence rate we can write
		\begin{align*}
		L(\beta_0,\beta_1)&=\prod_{i=1} P(Y_i=y_i|\lambda_i,T_i=t_i,X=x_i)= \prod_{i=1} \left( \big(\lambda_i \times T_i \big)^{y_i}e^{-\big(\lambda_i \times T_i\big)}  \middle/ y_i! \right) \\
		&where\dots \\
		\lambda_i&=\exp(\beta_0+\beta_1X_i)
		\end{align*}
	\item note that the constant incidence rate can vary by covariates in $X$
		\begin{itemize}
		\item thus we can \em encode \em a time-varying incidence rate by explicitly including $f(t)$ as a covariate in $X$
		\end{itemize}
	\item note also that individuals can have different baseline rates that are \em constant \em over follow up
	\item the likelihood allows us to model an expected counts $\mu$ or an incidence rate $\lambda$
	\end{itemize}
		
\subsection{Regression Equation}
	\begin{enumerate}
	\item to model a constant incidence rate that depends only on covariate values\dots
		\begin{align*}
		\lambda_{A,Z}=\exp(\underbrace{\beta_0}_\text{baseline rate}+\beta_1A+\beta_2Z)
		\end{align*}
		
		\begin{itemize}
		\item in terms of a generalized linear model
			\begin{itemize}
			\item[\-] link: \em log \em, distribution: \em poisson \em
			\end{itemize}
		\item an offset term is applied (later\dots)
		\end{itemize}
			
	\item to model a incidence rate that varies over time, we must specify it's function
		\begin{enumerate}
		\item piecewise: 
			\begin{align*}
			\lambda(t)_{A,Z}&=\exp\big[ \underbrace{\beta_0+\beta_2 \sum_{k=1}^K I(G=k)}_\text{baseline rate} +\beta_3A+\beta_4Z \big]\\
			&where\dots \\
			k&=k \text{th time interval }\Longrightarrow\{1,2,\dotsc,K\}
			\end{align*}
		\item log-linear: $$ \lambda(t)_{A,Z}=\exp(\underbrace{\beta_0+\beta_1t}_\text{baseline rate}+\beta_2A+\beta_3Z)$$
		\item fractional polynomial: $$ \lambda(t)_{A,Z}=\exp(\underbrace{\beta_0+\beta_1t+\beta_2t^{-2}+\beta_3t^2}_\text{baseline rate}+\beta_4A+\beta_5Z)$$
		\item non-parametric smoothing: $$ \lambda(t)_{A,Z}=\exp \big[\underbrace{\beta_0+g(\theta, t)}_\text{baseline rate}+\beta_1A+\beta_2Z\big]$$
		\end{enumerate}	
	\end{enumerate}	
	
\subsubsection*{What \& Where is the Offset?}		
	\begin{itemize}
	\item the random variable in a poisson distribution is the count of events
	\item an offset is the log of the total amount of follow up time for subject $i$
	\item we add it to the model to give a rate ratio interpretation to the paraeters even though we're modeling the average count of events
	\item it 'standardizes' the counts according to differential follow up by encoding the unit-specific amount of follow-up time
	\item recall that $\mu=\lambda T \longrightarrow \log(\mu)=\log(\lambda)+\log(T) $
		\begin{align*}
		\log(\lambda_i) &= \beta_0+\beta_1X_i \\
		\log(\mu_i/t_i) &= \beta_0+\beta_1X_i		\\
		\log(\mu_i)-\log(t_i) &= \beta_0+\beta_1X_i		\\
		\log(\mu_i) &= \underbrace{\beta_0+\beta_1X_i}_\text{model} + \underbrace{log(t_i)}_\text{offset}
		\end{align*}
	\end{itemize}

\subsubsection*{Parameter Interpretation}

	\begin{align*}
	h(t)=\exp(\beta_0+\beta_1f(t)+\beta_2X)
	\end{align*}
	
	\begin{itemize}
	\item $\beta_0+\beta_1f(t)$ is the log baseline incidence rate at time $t$ for group $X=x$
	\item $\beta_1$ is the log incidence rate ratio for a 1 unit increase in time, at time $t$ $X$
	\item $\beta_2$ is the log incidence rate ratio for a 1 unit increase in exposure, at time $t$ $X$
	\end{itemize}
	
\subsubsection*{Constant Incidence Rate Assumption}
		\begin{align*}
		log(\mu_i) &= \beta_0+\beta_1X_i + \log(t_i) \\
		log(\mu_i) &= \log(\lambda_i) + \log(t_i) \\
		\mu_i&=\lambda_i \times t_i \\
		E(Y_i) &= \lambda_i \times t_i
		\end{align*}
		
\subsection{Assumptions}
	\begin{itemize}
	\item the \textbf{incidence rate is homogeneous} for every time $t$ within strata formed by covariates
		\begin{itemize}
		\item implies proportional rate (i.e. constant $\beta$ over follow up)
		\item baseline incidence rate varies as a function of covariates
		\end{itemize}
	\item[\-] $\Longrightarrow$ $\lambda_0$ \textbf{varies by time only if $f(t)$ is explicitly specified} in the model
		\begin{itemize}
		\item assumes that you know the true form of the function $f(t)$
		\item can cause bias if it is mis-specified
		\end{itemize}
	\item best for poisson processes
		\begin{itemize}
		\item outcome is rare within strata formed by covariates
		\end{itemize}
	\end{itemize}
		
\subsection{Comparisons}
	\begin{itemize}
	\item unlike cox
		\begin{itemize}
		\item models average incidence rate (i.e. not hazard)
		\item estimates the rate by counting events within strata and dividing by total person-time within strata 
			\begin{itemize}
			\item \em does not use risk sets!
			\item but approaches them if modeling time very finely
			\item is not based on ranks, so the time scale is only accounted for by specifically paramaterizing it in the model
			\end{itemize}
		\item estimates baseline incidence rate
		\end{itemize}
	\item like cox
		\begin{itemize}
		\item uninformative strata are where strata have no events
		\end{itemize}
	\end{itemize}
		
\subsection{Practical Issues}
	\begin{itemize}
	\item \textbf{advantages}
		\begin{itemize}
		\item can obtain absolute rates $\rightarrow$ estimate rate difference
		\item natural extension of non-parametric person-time analysis
		\end{itemize}
	\item \textbf{disadvantages}
		\begin{itemize}
		\item constant incidence rate over time is a strong assumption, unlikely to hold
		\item model is vulnerable to mis-specification of $\lambda(t)_0$ via $f(t)$ in model
			\begin{itemize}
			\item can induce residual confounding by covariates included in the model
			\end{itemize}
		\end{itemize}
	\end{itemize}

\section{Pooled Logistic Regression}
    \subsection{likelihood function}
    \begin{itemize}
    \item  $L(\beta_{0},\dots,\beta_{p}|x_{1},...,x_{p,t_{i-1}}) = \displaystyle \prod_{t=1}^{i-1}\frac{exp(\beta_{0}+\dots+\beta_{p}x_{p,t_{i-1}})}{1+ exp(\beta_{0}+\dots+\beta_{p}x_{p,t_{i-1}})}$ 
	\end{itemize}
    
    \subsection{equivalence between pooled logistic model and Cox model}
	\begin{itemize}
	\item an intuitive way to show the equivalence between pooled logistic model and Cox model
	\begin{enumerate}
	\item $logit[Pr(Y_{t_{i}}=1|Y_{t_{i-1}}=0,X_{t_{i-1}})]=\underbrace{\beta_{0}+\beta_{t_{i}}t_{i}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$ \\ \\
$\Rightarrow$ $log\frac{Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}{1-Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}=\underbrace{\beta_{0}+\beta_{t_{i}}t_{i}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
	\item if the disease is rare in the person-time unit used in the model, the model can be reduced to the following model \\ \\
	$log[Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})]=\underbrace{\beta_{0}+\beta_{t_{i}}t_{i}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$ 
	\item to see the correspondence between pooled logistic model and Cox model
	\item Cox model: $I(t)=I_{0}(t)e^{\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}$ \\ 
	$\Rightarrow$ $log[I(t)]=log[I_{0}(t)]+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
		\begin{enumerate}
		\item $I(t) \Rightarrow Pr(Y_{t_{i}}=1|Y_{t_{i-1}}=0,X_{t_{i-1}})$
		\item $I_{0}(t)\Rightarrow \beta_{0}+\beta_{t_{i}}t_{i}$
		\end{enumerate}
	\item $\beta_{0}+\beta_{t_{i}}t_{i}$ determines the modeling of baseline rate ($I_{0}(t)$)
	\end{enumerate}  
	\end{itemize}  
	  
    \subsection{regression equation}
    \begin{itemize}
	\item $logit[Pr(Y_{t_{i}}=1|X_{t_{i-1}})]=\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
	\item[\-] $log[\frac{Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}{1-Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}]=\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
	\item[\-] $\frac{Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}{1-Pr(Y_{t_{i}}=1|Y_{t_{i-1}=0},X_{t_{i-1}})}=e^{\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}$
	\item[\-] $\frac{I(t_{i})}{1-I(t_{i})}=e^{\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}$
	\item[\-] $I(t_{i})=\frac{e^{\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}}{1+e^{\beta_{0}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}} \approx I_{0}(t)\times \frac{e^{\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}}{1+e^{\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}}}$
	\item the $logit(I(t_{i}))$ is a linear combination of the covariates in the model 
	\item this is a multiplicative model on the log of the odds of the incidence rate scale
	\item $log(\frac{I(t)}{1-I(t)}) \approx log(I(t))$ when the disease is rare in the person-time unit used in the model
	\item we may use pooled logistic model to estimate incidence rate ratio
	\item we may also estimate incidence rate because the $\beta_{0}$ was specified in the model
		\begin{itemize}
		\item models with different baseline rate parameter
		\item[\-] $logit[Pr(Y_{t_{i}}=1|X_{t_{i-1}})]=\underbrace{\beta_{0}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
		\item[\-] $logit[Pr(Y_{t_{i}}=1|X_{t_{i-1}})]=\underbrace{\beta_{0}+\beta_{t_{i}}t_{i}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
		\item[\-] $logit[Pr(Y_{t_{i}}=1|X_{t_{i-1}})]=\underbrace{\beta_{0}+\beta_{t_{2,i}}t_{2,i}+\dots+\beta_{t_{q,i}}t_{q,i}}_{\mbox{baseline rate}}+\beta_{1}X_{1,t_{i-1}}+\dots+\beta_{p}X_{p,t_{i-1}}$
		\end{itemize}
	\end{itemize}
     
    \subsection{assumptions}
    \begin{itemize}
    \item short intervals for the grouping of outcome events
	\item the incidence rate is small (lees than $0.1$) in the interval (usually the measured person-time units)
	\end{itemize}


\section{Conditional Logistic Regression}
     \subsection{likelihood function}
     \begin{itemize}
	\item the likelihood function for the conditional logistic regression for matched data set with k strata and M controls for each case is
		\begin{align*}
		&\prod^{K}_{k=1}\frac{exp(\alpha_{k}+\sum^{I}_{i=1}\beta_{i}x_{k0i})}{\sum^{M}_{j=0}[exp(\alpha_{k}+\sum^{I}_{i=1}\beta_{i}x_{kji})]} \quad \text{which is simplified to\dots} \\
		&\prod^{K}_{k=1}\frac{1}{1+\sum^{M}_{j=1}exp[\sum^{I}_{i=1}\beta_{i}(x_{kji}-x_{k0i})]}
		\end{align*}
	\end{itemize}

	\subsection{regression equation}
     \begin{itemize}
     \item the typical regression equation for a conditional logistic model of a binary outcome Y a covariate vector X and I matched strata is $logitPr[Y=1]=\alpha_{k}+\beta{x}$ where the case/total ratios per stratum will equal
     $$logit(A_{k}(X)/N_{k}(X))=\alpha^{*}_{k}+\beta{x} \quad \text{where}\dots$$ 
     \item[\-] $\alpha^{*}_{k}=\alpha_{k}+ln(\frac{\textit{$f_{k}$}}{\textit{$h_{k}$}})$  and \textit{$f_{k}$} and \textit{$h_{k}$} are the sampling fractions for cases and controls in strata k
     \end{itemize}
     \subsection{assumptions}
     \begin{itemize}
     \item assumptions of logistic regression plus the assumption of fixed 2x2 table margins (\textit{hypergeometric distribution})
     \item data probabilities conditioned on  the observed margins
     \item no effect modification across matched strata
     \end{itemize}
     \subsection{parameter interpretation}
     \begin{itemize}
     \item for the simple regression with outcome Y and exposure A\\ 
     $logitPr[Y=1]=\beta_{0k} + \beta_{1}A$
     \begin{itemize}
     \item the  parameter $\beta_{0k}$ in the conditional logistic model is an intercept term that varies for each matched stratum k
     \item $\beta_{1}=log(OR)$ of the effect of the exposure A on outcome Y, $exp(\beta_{1})=OR$ approximates hazard ratio 
     \item Note that the regression equation does not include parameters for matching factors as the effects of mathcing factors on the outcome Y cannot be computed. The stratum specific effecs of the matching factors are included in the intercept terms. 
	 \item however, we can assess effect modification by matching factors \em can \em be assesed by 
		\begin{itemize}
		\item modeling exposure classified according to matching factors.
		\item including a product term between exposure and the matching factor
		\end{itemize}
     \end{itemize}
     \end{itemize}
     \subsection{practical issues}
     \begin{itemize}
     \item the conditional maximum likelihood (CML) method is restricted to multiplicative-intercpet models for rates and odds
     \item the conditional likelihood function is very similar to the partial likelihood function used in the Cox model
     \item when time is part of the matching factors as for example is the case in nested case control risk set sampling, then the conditional logistic model is algebraically equivalent to a cox-model and can use phreg and estimate hazard ratios that will approximate true HR as the matching ratios approach infinity
     \item conditional logisic models are also used in case-crossover studies (each individual case is a stratum) and also approximate true hazard ratios
\end{itemize}

\section{Cox-Proportional Hazards Regression}
	\subsection{counting process}
	\begin{itemize}
	\item in the Anderson-Gill formulation, the time interval is considered as $(start, end]$ (not to includeleft hand side, include right hand side)
	\item (Anderson-Gill) counting process data structure is suitable for the follow scenarios 
		\begin{enumerate}
		\item time dependent covariate
		\item recurrent event
		\item alternative time scale
		\item time dependent strata 
		\item discontinuous intervals of risk
		\end{enumerate}
	\item all study subjects who have entered the study in the same day or earlier as the current case, and have not died from another cause (competing risk) or been lost to follow-up by the day that the case became a case, would belong to the case's risk set.
	\end{itemize}
	
	\subsection{likelihood function}
	\begin{itemize}
	\item[\-] $I(t)=I_{0}(t)e^{\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}}$	
	\item the major advantage of Cox proportional hazards model is that it does not require estimating the baseline rate $I_{0}(t)$
	\item it is similar to calculating rate ratio in each strata of time and summarized these rate ratios by M-H estimate
	\item Cox model is a semi-parametric model
		\begin{itemize}
		\item we do not specify the parameters of baseline incidence rate $I_{0}(t)$ is non-parametrically modeled) 
		\item we do specify the parameters for the covariates in the model (the effect of $X_{1},X_{2},\dots,X_{p}$ are parametrically model)
		\item the combination of the non-parametric model of $I_{0}(t)$ and the parametric model of $X_{1},X_{2},\dots,X_{p}$ is a semi-parametric model	
		\end{itemize}
	\item the magic of Cox model is that we can estimate parameters of the covariates without estimating the non-parametric baseline rate by using partial likelihood function
	\item the partial likelihood function
	\begin{itemize}
	\item the likelihood function at the distinct failure time $t_{j}$ when individual j fails
	\begin{eqnarray*}
	L_{j}(\beta) &=& Pr(\mbox{individual with covariate value} \ x_{j} \ \mbox{fails at} \ t_{j} | \mbox{1 individual fails from the risk set at} \ t_{j}) \\
			     &=& \frac{Pr(\mbox{individual with covariate value} \ x_{j} \ \mbox{fails} | \mbox{in the risk set} \ R_{t_{j}})}
				    {\sum_{i \in R_{t_{j}}} Pr(\mbox{individual with covariate value} \ x_{i} \ \mbox{fails} | \mbox{in the risk set} \ R_{t_{j}})}\\
			     &=& \frac{\Delta t \times \lambda(t_{j};Z_{j})}{\sum_{i \in R_{t_{j}}}\Delta t \times \lambda(t_{j};Z_{i})}\\
			     &=& \frac{\Delta t \times \lambda(t_{j};Z_{j})}{\Delta t \times \sum_{i \in R_{t_{j}}} \lambda(t_{j};Z_{i})}\\
			     &=& \frac{\lambda(t_{j};Z_{j})}{\sum_{i \in R_{t_{j}}}\lambda(t_{j};Z_{i})}
	\end{eqnarray*}
	\item under the proportional hazards assumption $\lambda(t;Z)=\lambda_{0}e^{\beta Z}$
	\item and the assumption that these individual likelihoods are independent from each other given the risk set
	$$L^{partial}(\beta)=\prod_{j=1}^{K}\frac{\lambda_{0}(t_{j})e^{\beta Z_{j}}}{\sum_{i \in R_{t_{j}}}\lambda_{0}(t_{j})e^{\beta Z_{i}}}=\prod_{j=1}^{K}\frac{e^{\beta Z_{j}}}{\sum_{i \in R_{t_{j}}}e^{\beta Z_{i}}}$$
	\end{itemize}
	\end{itemize}    
	
    \subsection{regression equation}
    \begin{itemize}
	\item $I(t)=I_{0}(t)e^{\beta_{1}X_{1}}\times e^{\beta_{2}X_{2}} \times \dots \times e^{\beta_{p}X_{p}}=I_{0}(t)e^{\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}}$
	\end{itemize}

    \subsection{assumptions}
    \begin{itemize}
	\item in terms of PH model: the hazard ratio is constant over time = the hazard functions (e.g. $I_{0}(t)e^{\beta}$ and $I_{0}(t)$) are parallel to each other 
	\item in terms of M-H method: there is no effect modification by time for the rate ratio 
	\item the proportional hazards assumption can be test by testing the interaction term between time and covariates
	\end{itemize}
		
    \subsection{comparisons}
    \begin{itemize}
	\item advantages of Cox model:
		\begin{enumerate}
		\item do not need to specify and estimate the baseline rate
		\item less vulnerable to model misspecification
		\end{enumerate}
	\item disadvantages of Cox model:
		\begin{enumerate}
		\item cannot estimate baseline rate directly
		\end{enumerate}
	\end{itemize}
	
	\subsection{parameter interpretation}
    \begin{itemize}
    \item $I(t)=$the incidence rate at time t for a subject with values $X_{1}=x_{1}, X_{2}=x_{2}, \dots, X_{p}=x_{p} $
    \item $I_{0}(t)=$the baseline incidence rate at time t \\
    = the incidence rate at time t for a subject with values $X_{1}=0, X_{2}=0, \dots, X_{p}=0$
	\item $\beta_{1}=$ the log rate ratio for a one unit change in $X_{1}$
	\item[\-] e.g. $\beta_{1}=$ the log rate ratio for death comparing male to female
	\item[\-] e.g. $\beta_{2}=$ the log rate ratio for death for one unit (mmHg) increase in blood pressure
	\item the $95\%$ confidence interval of $\beta$ is not symmetric since that it is $e^{\hat\beta\pm 1.96\times se(\hat\beta)}$
	\end{itemize}

\section{Accelerated Failure Time Regression}
     \subsection{likelihood function}
     \subsection{regression equation}
     \subsection{assumptions}
     \subsection{parameter interpretation}
     \subsection{comparisons}
     \subsection{practical issues}
	 
	 
	 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Models for Statistical Inference: Special Topics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model Selection for Case-control Data}
\subsection{Incidence Density Sampling}
\begin{itemize} 
\item unconditional logistic regression is the choice of model for typical density sampling 
\item the OR estimates of the exposure are approximates for the rate ratio of the underlying cohort
\end{itemize}
\subsection{Cumulative Incidence Sampling}
\begin{itemize} 
\item logistic models fit with OR approximating a risk ratio only under the rare disease assumption
\item sometimes additive logistic models are used (linear OR models)
\end{itemize}
\subsection{Matched Case-Control}
\begin{itemize}
\item conditional logistic regression is used with the OR approximating Rate Ratios 
\item in nested case-control studies (risk-set sampling) where participants are also matched on time the conditional logistic model and Cox model are equivalent and cox model can be fit conditioning out the stratum intercepts and directly calculate hazard ratios (OR from conditional logistic regression approximates HR as the sampling ratio approaches infinity)
\end{itemize}
\subsection{Case-crossover}
\begin{itemize}
\item conditional logistic regression with each person representing a stratum
\item effect estimates interpreted as hazard ratio (need exchangeability of case-control periods)
\end{itemize}
\subsection{Case-Cohort}
\begin{itemize}
\item risk set method allows for cox-model (see nested-case control)
\end{itemize}

\section{Covariate Selection}
	\begin{itemize}
	\item Problems that will occur when the model fitted to the data is incorrect
		\begin{itemize}
		\item standard errors will be biased
		\item power to detect an exposure-response relationship will be reduced
		\item the effects of extremes of the exposure distribution will be more biased than estimates of exposure effect near the center of the exposure distribution
		\item transforming the exposure or adding higher-order interaction terms will not always be able to overcome the bias due to model mis-specification.
		\end{itemize} 
	\item Situations when controlling for all possible confounders cause more bias that it prevents
	   \begin{itemize}
	   \item Finite samples with sparse data. Large samples can have sparse data issues as well if the number of confounders is large. 
	   \item Incorrectly specified model, such as adjusted for intermedaites
	   \end{itemize} 
	\item Covariates selection
		\begin{itemize}
		\item All known risk factors should be treated as potential confounders and included in the model.
		\item Inclusion of variables associated with exposure but not outcome will not introduce bias, but will decrease efficiency (thus making it undesirable)
		\item Inclusion of variables associated with outcome but not associated with exposure will increase efficiency, and will not introduce bias. 
		\end{itemize}
	\item Covariates selection algorithm 
	\item \textbf{Deluxe algorithm}
	    \begin{itemize}
	    \item Start with highly saturated model
	    \item Drop risk factors weakly associated with outcome $(p>0.2)$; ie. collapsing 2x2 table on the variable in stratified analysis or deleting the variable with all its higher order interaction terms
		\item Check change of effect estimate, keep the variable if change $>10\%$\\
		$$BIAS =\frac{OR_{reduced}-OR_{saturated}}{OR_{saturated}}*100$$ 
       \end{itemize}
    \item \textbf{Bargain basement algorithm}
       \begin{itemize}
	   \item Start by including the main effects only of all known risk factors.
	   \item Identify suspected risk factors with $p < 0.20$ in crude analysis with the outcome.
	   \item Drop the variables weakly associated with outcome $(P>0.2)$ in the multivariate model.
	   \item Keep the variable by $10\%$ change of effect estimate rule.
	   \item Assumption: Higher-order interactions terms will be unimportant if each association of the potential confounders is multiplicative.
       \end{itemize}
    \item Bias traded off with efficiency in the selection of confounders 
		\begin{itemize}
		\item A model that includes all potential confouders may get least biased results
		\item When the covariates is relatively large to the sample ( greater thtan 1 covariates  for 10 events), the confidence intervals will become so large that we cannot infer anything useful about the parameter of interest
		\item Remove some covarites may increase iefficiency, but may increase bias if we missed important confouders
		\item In either situation, mean squared error may increase because mean squared error is equal to the variance of the estimator plus the square of the bias. Thus, both efficiency and bias are taken into account in mean squared error. 
		\end{itemize}
    \end{itemize}

	\section{Stratified Sampling Fractions}
	
	\subsection{Formulation}
	
	\begin{itemize}
	\item regular log-linear model where x is expected number of cases over follow up $t_j$
		\begin{align*}
		\log E(X_j)=\beta_0+\beta_1z_{1j}+\log t_j
		\end{align*}
	\item now if we let $\gamma_0$ be sampling fraction when $Z=0$ and $\{\gamma_0+\gamma_1\}$ be the sampling fraction for when $Z=1$ then\dots
		\begin{align*}
		\log \bigg[\frac{E(Y_j)}{E(t_j)}\bigg]=\gamma_0+\gamma_1z_1
		\end{align*}
	\item then we can model sampled controls (days) $Y$ as a function of sampling fraction $\gamma$ that depends on covariate $z_j$ and the corresponding amount of follow up time
		\begin{align*}
		\log E(Y_j)=\gamma_0+\gamma_1z_{1j}+\log t_j
		\end{align*}
	\item subracting we get
		\begin{align*}
		\log \bigg[\frac{E(X_j)}{E(Y_j)}\bigg]&=(\beta_0-\gamma_0)+({\beta_1-\gamma_1})z_{1j} \\
                            &= \beta^{\prime}_0+\beta^{\prime}_1z_{1j}
		\end{align*}
	\item because $Y_j$ is number of controls, we have essentially a logit model 
		\begin{itemize}
		\item where $p_j$ is observed fraction of subjects at covariate level $i$ who are cases
		\item if we condition on the total number of cases and controls at the covariate level\dots
			\begin{align*}
			\log \bigg[\frac{E(P_j)}{1-E(P_j)}\bigg] = \beta^{\prime}_0+\beta^{\prime}_1z_{1j}	
			\end{align*}
		\end{itemize}
	\item $\beta_0$ is not baseline odds of outcome, but we can recover it if we know the sampling fractions	
	\item note: if one uses stratified sampling fractions, should always \textbf{include the stratifying factors in the analysis} 
	\item to simulate the entire cohort, can either
		\begin{itemize}
		\item include the sampling fraction as an the offset in a poisson model as shown above
		\item weight the cases and controls by the inverse of their covariate-specific sampling fraction
		\end{itemize}
	\end{itemize}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Models for Causal Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Parametric G-Formula} 
$$\displaystyle E[Y^{a}] = \sum_{l}E[Y|A=a,L=l]Pr[L=l] $$
	\begin{itemize}
	\item estimate $E[Y|A,L]$ with parametric model
	\item null paradox (always get effect)
	\item even if the causal null hypothesis is true, effect estimate will not be null
	\item model misspecification
	\item need model for each time point and thus the risk of model misspecification is high
	\item Strength
		\begin{enumerate}
		\item Appropriately adjusts for time-varying confounding affected by prior exposures although subject to the g-null paradox.
		\item Naturally handles joint interventions and dynamic interventions
		\item Estimates multiple parameters (including risk ratios and risk differences) and yields population estimates.
		\end{enumerate}
	\item Weakness
		\begin{enumerate}
		\item Requires models for covariates as well as outcome
		\item More sensitive to violations of assumptions of model misspecification (A violation in one of the multiple models may reverberate throughout the others)
		\item Not implemented in packaged statistical software
		\end{enumerate}
	\end{itemize}


\section{Marginal Structural Models}
	\begin{itemize}
	\item Conditional versus Marginal Models
		\begin{itemize}
		\item conditional: $E[Y|A,L]=\theta_0 + \theta_1A + \theta_2L$
		\item marginal: $E[Y^a]=\beta_0 + \beta_1a$
		\item marginal \em and \em conditional: $E[Y^a|L]=\beta_0 + \beta_1a + \beta_2L +\beta_3aL$, used to assess effect modification
		\end{itemize}
	\item \textit{marginal}:
		\begin{itemize}
		\item model the marginal distribution of counterfactual random variables
		\item not modelling observed associations
		\end{itemize}
	\item \textit{structural}:
		\begin{itemize}
		\item model the probabilities of counterfactual variables
		\end{itemize}
	\item \textit{saturated}: 
		\begin{itemize}
		\item unless exploring effect modification
		\item can be regarded as a weighted crude analysis of the study population
		\end{itemize}
	\item paramaters are estimated via inverse probability weighted regression
		\begin{align*}
		E[Y^a|a] &= \gamma_0+\gamma_1a \ \text{is estimated by} \dots \\
		  E[Y|A] &= \beta_0+\beta_1A \ \text{with weights} \ \frac{1}{f[A|L]}
		\end{align*}
	\item when ideintifiability conditions hold\dots
		\begin{itemize}
		\item[\-] $\beta_0=\gamma_0$ and $\beta_1=\gamma_1$
		\end{itemize}
	\end{itemize}
		
   \subsection{MSMs for Constant Exposures}
		\subsubsection*{Estimation}
		$$W^{A}=\frac{1}{Pr[A=a|L=l]}$$
		\begin{itemize}
		\item we need to simulate the population in the counterfactual world using IP weight, then use the marginal structural model with the \emph{simulated data} (psuedo-population)
		\item for exposures as \emph{dichotomous} variables, estimate $Pr[A=a|L=l]$ using logistic regression. e.g.\dots
			\begin{align*}
			logit[Pr[A=a|L,M,N]] =\theta_{0} &+\theta_{1}L+\theta_{2}M+\theta_{3}N+\theta_{4}LM \\
			                                 & +\theta_{5}LN+\theta_{6}MN+\theta_{7}LMN && \text{(saturated model)}
			\end{align*}
		\item estimate weights $W^A$ with logistic regression
			\begin{align*}
			logit \ Pr[A=1|L]&=\theta_0+\theta_1L \\
			Pr[A=1|L] &= \bigg( \frac{e^{\theta_0+\theta_1}}{1+e^{\theta_0+\theta_1}} \bigg) \\
			Pr[A=0|L] &= \bigg( 1-\frac{e^{\theta_0+\theta_1}}{1+e^{\theta_0+\theta_1}}  \bigg) \\
			W^A &=\frac{1}{Pr[A=a|L=l]}
			\end{align*}		
		\item model $E[Y|A]$ in study population with weights $W^A$
		\item use robust variance (sandwich estimator) or bootstrap because counterfactual outcomes are correlated within individuals (recall $\epsilon_{Y^a}$ in twin DAG)
		\end{itemize}

	\subsubsection*{Parameter Interpretation}
		\begin{eqnarray*}
		E[Y^a] &=& \beta_0+\beta_1a\\
		E[Y^{a=0}]&=&\beta_0\\	
		E[Y^{a=1}]&=&\beta_0 + \beta_1\\
		E[Y^{a=1}]-E[Y^{a=0}]&=&\beta_1 
		\end{eqnarray*}
		
		\begin{itemize}
		\item $\beta_0$ is the average outcome if the entire population had not been treated
		\item $\beta_0+\beta_1$ is the average outcome if the entire population had been treated
		\item $\beta_1=$ causal effect of treatment
		\item depends on model form and specification
			\begin{itemize}
			\item \textit{linear} $\rightarrow$ causal difference in outcome per unit change in exposure
			\item \textit{logistic} $\rightarrow$ causal odds ratio
			\item \textit{log-linear} link $\rightarrow$ causal incidence rate ratio
			\end{itemize}
		\end{itemize}
	
	\subsubsection*{Stabilized Weights}
		$$SW^A=\frac{Pr[A=a]}{Pr[A=a|L=l]}$$
		
		\begin{itemize}
		\item stabilizes variance of causal effect
		\item by preventing persons with $Pr[A|L] \approx 0$ from dominating the pseudo-population
		\item \textbf{necessary} for continous exposures
		\end{itemize}
	
	\subsubsection*{Subset of Population Treated with $A=a^{\prime}$ }
		\begin{align*}
		SW^{A^{\prime}} = \frac{Pr[A=a^{\prime}|L]}{Pr[A=a|L]}
		\end{align*}
		\begin{itemize}
		\item note: when $a^{\prime}=1$, causal effect is analagous to \em SMR \em
			\begin{itemize}
			\item for treated $\rightarrow SW^{A^{\prime}}=\frac{Pr[A=1|L]}{Pr[A=1|L]=1}\rightarrow 1$
			\item for untreated $\rightarrow SW^{A^{\prime}}=\frac{Pr[A=0|L]}{Pr[A=1|L]}=1\rightarrow P/(1-P)$ 
			\end{itemize}
		\item note: when $a^{\prime}=0$, causal effect is analagous to \em SRR \em
			\begin{itemize}
			\item for treated $\rightarrow SW^{A^{\prime}}=\frac{Pr[A=0|L]}{Pr[A=1|L]}=1\rightarrow (1-P)/P$
			\item for untreated $\rightarrow SW^{A^{\prime}}=\frac{Pr[A=0|L]}{Pr[A=0|L]=}1\rightarrow 1$ 
			\end{itemize}
		\end{itemize}
		
	\subsubsection*{Effect Modification}
			\begin{itemize}
\item we can model effect modification of causal effect estimates from marginal structural models 
$$	 E[Y^{a},M] =\beta_0 + \beta_1a+\beta_{2}M+\beta_{3}aM $$
\begin{eqnarray*}
		 E[Y^{a=0}|M=0] &=&\beta_0 \\
		 E[Y^{a=0}|M=1] &=&\beta_0+\beta_{2} \\
		 E[Y^{a=1}|M=0] &=&\beta_0+\beta_{1} \\
		 E[Y^{a=1}|M=1] &=&\beta_0+\beta_{1}+\beta_{2}+\beta_{3} \\
\end{eqnarray*}
\begin{eqnarray*}
		 E[Y^{a=1}|M=1]-E[Y^{a=0}|M=1] &=&\beta_{1}+\beta_{3} \\
		 E[Y^{a=1}|M=0]-E[Y^{a=0}|M=0] &=&\beta_{1} \\
\end{eqnarray*}
	\end{itemize}

	
	\subsubsection*{Adjustment for Censoring}
	$$SW^C=\frac{Pr[C=0|A=a]}{Pr[C=0|A=a,L=l]}$$\\
	\begin{itemize}
	\item where $C=1$ if the subject was lost to follow-up
	\item censoring is conceptualized as just another treatment
	\item interest is in the causal effect of treatment $A$ had all subjects remained uncensored ($C=0$)
	\item requires no unmeasured confounders of both treatment $A$ and ``treatment" $C$ (censoring)
	\item weighted regression model is restricted to uncensored subjects where the subject specific weight $w_i= SW^A_i \times SW^C_i$
	\item $IPW=standardization$ \emph{non-parametrically}
		\item[\-] these two methods use different part of the joint probability of random variables on a causal DAG represented by Makcov factorization:
			$$f(Y,A,L)=f(Y|A,L)f(A|L)f(L)$$
			$$\mbox{IPW}: \frac{1}{f(A|L)}$$
			$$\mbox{standardization}: f(Y|A,L)f(L)$$
	\item assumptions needed for parametric MSM:
			\begin{enumerate}
			\item identifying conditions (especially \emph{positivity} \dots)
				\begin{itemize}
				\item structural violation of positivity: subjects with certain confounder values cannot possibly be exposed/unexposed ($Pr[A=1|L=l]=0 \ \mbox{or} \ Pr[A=0|L=l]=0 $)
				\item random violation of positivity: sparse data problem ($\hat Pr[A=1|L=l]=0 \ \mbox{or} \ \hat Pr[A=0|L=l]=0 $)
				\item using parametric model to estimate IP weight is essentially \textbf{assuming \emph{random non-positivity}}
				\end{itemize}
			\item assumptions implied by the IPW model (e.g. interaction between $Ls$)
			\item assumptions implied by the marginal structure model (e.g. $cum(\bar{a_{i}})$)
			\end{enumerate}
	\end{itemize}
	
\subsection{MSMs for Time-Varying or Joint Exposures}
	\subsubsection*{Estimation}
		\begin{itemize}
		\item estimate weights $W^A$ with pooled logistic regression
			\begin{align*}
			 logit \ Pr[A=1|\bar{L}_k,A_{\bar{k}-1}]&= g(\bar{L}_k,\bar{A}_{k-1}; \ \theta) \\
			&= \theta_0f(k)+ \theta_i \sum_{k=0}^{k}  L_{k} + \theta_j  \sum_{k=0}^{k-1} A_i \\
			Pr[A_k=1|\bar{L}_k,A_{\bar{k}-1}] &= \bigg( \frac{e^{g(\bar{L}_k,\bar{A}_{k-1}; \ \theta)}}{1+e^{g(\bar{L}_k,\bar{A}_{k-1}; \ \theta)}} \bigg) \\
			Pr[A_k=0|\bar{L}_k,A_{\bar{k}-1}] &= \bigg( 1-\frac{e^{g(\bar{L}_k,\bar{A}_{k-1}; \ \theta)}}{1+e^{g(\bar{L}_k,\bar{A}_{k-1}; \ \theta)}}  \bigg) \\
			\end{align*}		
		$$ W^A=\frac{1}{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i},\bar{L}_k=\bar{l}_{ki}]}$$
			\begin{itemize}
			\item estimation for each unit of person-time
			\item only need to estimate for times $k$ where treatment changes because otherwise $Pr[A=1|\bar{L}_k,A_{\bar{k}-1}]=1$
			\item subtle point: only need to establish identifiability conditions for times where treatment changes, all other times follow suit
			\item often simplify $f\big(\theta_j  \sum_{k=0}^{k-1} \bar{A}_i\big)$ as $f(\bar{A}_k)$ and $h\big(\theta_i \sum_{k=0}^{k}  L_{k}\big)$ as $h(\bar{L}_k)$
				\begin{itemize}
				\item requires subject matter considerations
				\item often necessary for estimation (i.e. positivity violation s \& efficiency)
				\end{itemize}
			\end{itemize}
		\item model $E[Y|f(\bar{A})]$ in study population with weights $W^A$
		\item use robust variance or bootstrap because counterfactual outcomes are correlated within individuals (recall $\epsilon_{Y^a}$ in twin DAG)
		\end{itemize}
		
	\subsubsection*{Parameter Interpretation}
		\begin{itemize}
		\item as before \em depends on model form and specification \em
		
			\begin{align*}
			E[Y^{\bar{a}_k}|\bar{a}_k]=\beta_0+\beta_1f(\bar{a}_k)
			\end{align*}
			
		\item $\beta_0$ is the average counterfactual outcome if entire population untreated over entire follow up
			\begin{itemize}
			\item[\-] $\bar{a}_k=(A_0=0,A_1=0,\dots,A_k=0)$ for all $k$
			\end{itemize}
		\item $\beta_0+\beta_1$ is the average counterfactual outcome if the entire population had been treated with regime $\bar{a}_k$
		\item interpretation of causal contrast depends on how $f(\bar{a}_k)$ is specified
		\end{itemize}
		
	\subsubsection*{Stabilized Weights}
	$$SW^A= \frac{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i}]}{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i},\bar{L}_k=\bar{l}_{ki}]}$$
	
	\begin{itemize}
	\item estimate of $\beta_1$ is unbiased even if the numerator model is misspecified
	\item all others must hold (exposure, censoring, and outcome)
	\item \textit{fine point 1}:
		\begin{itemize}
		\item in \em unstabilized weighting \em the entire population (in its original size) is passed through all treatment regimes, such that the size of the pseudo-population equals the size of the original population times the number of treatment regimes
		\item in \em stabilized weighting \em the entire population is again passed through all treatment regimes, however it is down-weighted by the probability in the numerator. The numerator is usually some function of treatment, so the different pseudo-populations reflect the size of the original treatment arms. 
		\item in the case where $f[A]$ is the numerator, the size of the population 'copy' passing through the treatment regime equals the size of the treatment group in the original population.
		\end{itemize}
	\item \textit{fine point 2}:
		\begin{itemize}
		\item the denominator of IPW weight removes the association between $A$ and whatever is in the conditioning event (i.e. $1/f[A|L]$ removes $L \rightarrow A$)
		\item the numerator of stabilized weight restores the association between $A$ and whatever is in the conditioning event (i.e. $f[A|L_0]/f[A|L_0,L_1]$ removes $L_1 \rightarrow A$ but not $L_0
	 \rightarrow A$)
		\item variables should be in conditioning event of numerator \textbf{only if} they are present in the conditioning event of the denominator
		\end{itemize}
	\end{itemize}
	
	\subsubsection*{Subset of Population: $\bar{A}_k=\bar{a}_k^{\mathbf{\prime}}$}
	
	$$SW^A= \frac{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}^{\mathbf{\prime}}|\bar{A}_{k-1}=\bar{a}_{(k-1)i}^{\mathbf{\prime}},\bar{L}_k=\bar{l}_{ki}]}{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i},\bar{L}_k=\bar{l}_{ki}]}$$
	
	\subsubsection*{Effect Modification (by $V$)}
	
	$$SW^A= \frac{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i},V=v]}{\displaystyle \prod_{k=0}^{K} Pr[A_{k}=a_{ki}|\bar{A}_{k-1}=\bar{a}_{(k-1)i},\bar{L}_k=\bar{l}_{ki}]}$$
	
	\begin{itemize}
	\item $V$ is a subset of $L_{k=0}$ (i.e. \em \underline{pre-treatment} \em baseline covariates)
	\item assess effect modification by including $V$ in marginal model for outcome
	\item efficiency gain if include in numerator of weights (recommended)
	\item \textbf{cannot} assess effect-modification by time-varying covariates (need SNM)
	\end{itemize}
	
	\subsubsection*{Adjustment for Censoring}
$$ SW^C=\frac{\displaystyle \prod_{k=0}^{K+1} Pr[C_k=0|\bar{C}_{k-1}=0,\bar{A}_{k-1}=\bar{a}_{(k-1)_i}]}{\displaystyle \prod_{k=0}^{K+1} Pr[C_k=0|\bar{C}_{k-1}=0,\bar{A}_{k-1}=\bar{a}_{(k-1)i},\bar{L}_k=\bar{l}_{ki}]}$$\\
	\begin{itemize}
	\item where $C_k=1$ if the subject was lost to follow-up
	\item interest is in the causal effect of treatment history $\bar{A}$ had all subjects remained uncensored ($C=0$)
	\item requires no unmeasured confounders of both treatment $\bar A$ and ``treatment" $\bar C$ (censoring)
	\item weighted regression model is restricted to uncensored subjects where the subject specific weight equals $SW^A \times SW^C$
	\end{itemize}
	
	\subsubsection*{Artificial Censoring}
		\begin{itemize}
		\item for \textbf{static} regimes, weights for artificial censoring = treatment weights $W^A$
			\begin{itemize}
			\item \em more on this in chapter: advanced topics in causal inference \em
			\end{itemize}	
			\item similar to 'per-protocol' analysis
		\end{itemize}
	
	
	\subsection{practical issues}
	 \begin{itemize}
	\item In practice, unconditional pooled logistic regression will be used to approximate a Cox proportional hazards model, because the PROC PHREG is unable to deal with the time-dependent weights required for the MSM.
	\item For pooled logistic regression, the baseline hazard (unspecified in Cox model) has to be specified. 
	\item A function of time to model the baseline hazard, either single variable or polynomials, has to be included in pooled logistic models. 
	\item MSMs use weights and create a correlated data structure of treatment and covariates, so GEE with sandwich variance estimator (PROC GENMOD with the option repeated) has to be used to obtain robust confidence intervals.
	\item To obtain both censoring and treatment stabilized weight, four four pooled logistic models are fit. 
	\item  In two of them the outcome will be censor and in the other two the outcome will be treatment
	\item  For each outcome,  the numerator model contains exclusively baseline covariates; and a denomenator model contains baseline and time-dependent covariates.
	\item  By using the predicted values of the models, we can estimate, for each person-day, the probability of being uncensored and the probability of being treated or untreated up to that time.
	\item The final weight SW for each person-day will be the product of the probabilities, for treatment up to time K, for tretment to time K+1
	\end {itemize}
	\subsubsection*{positivity violations}
	\begin{itemize}
	\item nonpositivity can be
	\begin{itemize}
	\item random:result of a non-infite sample and multiple confounders stratification\\ different zero cells in different samples
	\item structural (non random): a certain confounder profile cannot possibly be exposed (structural zero probabilty of being exposed)\\ always zero cells for these confounder values\\example men will always be unexposed in terms of hormone replacement therapy
  \end{itemize}
  \item for structural nonpositivity causal inference has to be restricted to subset of population for which positvity holds
  \item with random nonpositivity we can use parametric models to smooth over zero cells \\with parametric IPW model weights for zero cells estimated using parametric model for $Pr[A|L]$
  \item keep in mind that parametric IPW and standardization are not equivalent\\ different models that are only equivalent if saturated and in parametric situations $IPW\neq standardization$ in high-dimentional data\\ IPW models estimate ]$\textit{f}[A|L]$ while standardization models estimate $\textit{f}[L]$, $\textit{f}[Y|A,L]$
  \item high-dimentional data occur in the cases of
  \begin{itemize}
  \item many categorical variable
  \item continuous variables
  \item time-varying variables
  \item combinations of the above
  \end{itemize}
  \begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{non-positivity.png}
		\caption{Increasing categories of a covariate increases non-positivity bias}
		\label{non-positivity}
\end{figure}
  \item by increasing the number and levels of covariates for which weights can be calculated we better control for confounding bias at the expence of nonpositivity bias (even if nonpositivity is random)
  
  \begin{itemize}
  
  \item might want to consider not controlling for a weak confounder that does not reduce confounding bias by much but increases the probability of non-positivity bias
  \end{itemize}
  \item nonpositvity also increases with increasing time points in time-varying exposure situations but non-positivity bias decreases with the use of appropriate stabilized weights 
  \item when paramatrically esimating weights with satistical software useful hints of nonpositivity are weights with $mean\neq 1$ and an inflated standard deviation
  \item in general the parameters of MSMs can be estimated even with non-positivity using parametric weighs but the parameters are no longer causal
  \end{itemize}
				
	\subsubsection*{strength and weakness}
	\begin{itemize}
	\item strength
	\begin{enumerate}
	\item Easy to understand and easy to fit with standard software that allows for weights
	\item Can also handle dynamic regimes by dynamic MSM
	\end{enumerate}
	\item weakness
	\begin{enumerate}
	\item Pseudo bias and imprecision, the product of probability of treatment became very small in large number of time periods and may inflate the IPTW 	inappropriately (the drawback can be ameliorated by bounded doubly robust estimators, stabilized weights, truncating large influential weights)
	\item MSM cannot be used to estimate causal effects when treatment is confounded, but instrument is available
	\item MSMs do not allow modeling of interaction between treatment and time-varying covariates and cannot directly quantify effect modification
	\end{enumerate}	
	\end{itemize}
 
	 
\section{Structural Nested-Models}
	\subsection{SNMs for Constant Exposures}
		
		\subsubsection{Some things to remember\dots}
			\begin{itemize}
			\item \textbf{conditional exchangeability}:
				\begin{itemize}
				\item conditional exchangeability $Y \amalg A |L$ can be paramaterized in a regression model
				\item $\text{logit}Pr[A=1|L,Y^{a=0}]=\alpha_0+\alpha_1Y^{a=0}+\alpha2_L$
				\item[\-] \quad where $\alpha_1=0$
				\end{itemize}
			\item \textbf{Sharp Causal Null}:
				\begin{itemize}
				\item no causal effect for any individual $i$ 
				\item $Y=Y_i^{a^{\prime}}=Y_i^{a^{\prime \prime}}$ for all $i$ and all $a$
				\end{itemize}
				
			\item \textbf{rank-preservation}: \em not necessary but useful for understanding \em
				\begin{itemize}
				\item each individual's counterfactual outcome, in the presence of treatment, is raised (if causative) or lowered (if preventative) by the amount equal to the causal effect
				\item ignores heterogeneity and random variability in outcome and is biolologically implausible
				\item is \textbf{unnecessary} mathematically but it helps us understand g-estimation	
				\end{itemize}
			\end{itemize}

		\subsubsection{Sharp G-null Test}
			\begin{enumerate}
			\item Paramaterize the assumption of conditional exchangeabilty
				\begin{align*}
				\text{logit}Pr[A=1|L,Y^{a=0}]=\alpha_0+\alpha_1Y^{a=0}+\alpha_2L
				\end{align*}
			\item Assume sharp causal null
				\begin{align*}
				Y_i^{a=0}=Y_i^{a=1}=Y \ \text{for all } a
				\end{align*}
			\item \textbf{g-null text}: if both sharp causal null and conditional exchangeability are true\dots
				\begin{itemize}
				\item $\alpha_1=0$ in the model $\text{logit}Pr[A=1|L,Y^{a=0}]=\alpha_0+\alpha_1Y^{a=0}+\alpha_2L$
				\end{itemize}
			
			\item in fact, $\alpha_1\neq$ then either\dots
				\begin{itemize}
				\item we have conditional exchangeability but sharp causal null is not true
				\item we do not have conditional exchangeability but sharp causal null is true
				\item niether conditional exchangeability or sharp causal null are true
				\end{itemize}
			\item thus only under the assumption of conditional exchangeability can we use g-estimation to reject the sharp causal (i.e. when $\alpha_1=0$)
			\end{enumerate}
		
		\subsubsection{G-estimation of SNMM with Rank Preservation}
			\begin{itemize}
			\item only can be used for
				\begin{itemize}
				\item continuous outcomes
				\item time to event outcomes
				\item dichotomous outcomes only if use log-linear model with rare-disease assumption
				\end{itemize}
			\item in the presence of a point exposure g-estimation and marginal structural models use exactly the same models, albiet in a different way
			\item thus if paramaratized equivalently the approaches make exactly the same modeling assumptions
			\end{itemize}

			\begin{enumerate}
			\item Begining with a mean structural model, specify an individual level structural model
				\begin{align*}
				E[Y^{a}&=\beta_0+\beta_1a \\
				E[Y^{a=1}]&=E[Y^{a=0}]+\beta_1a && \beta_0 = E[Y^{a=0}] \\
				E[Y^{a=0}]&=E[Y^{a=0}]-\psi^{*}a && \beta_1=\psi^{*} \\
				Y^{a=0}&=Y^{a=1}-\psi^{*}a && \text{by rank preservation} \\
				Y^{a=0}&=Y-\psi^{*}A && \text{by consistency} \\
				H(\psi^{*})&=f(Y-\psi^{*}A) && \text{for efficiency} \\
				\end{align*}
			\item[\-]  note that up to this point we have not used conditional exchangeability \\
				also, $f(x)$ can equal $\ln(x)$, $x^2$, $\frac{1}{x}$, etc.
			\item Paramaterize the assumption of conditional exchangeability
				\begin{itemize}
				\item use either of the following\dots
					\begin{align*}
					Pr[A=1|L,Y^{a=0}]&=\alpha_0+\alpha_1Y^{a=0}+\alpha_2L \\
					Pr[A=1|L,Y^{a=0}]&=\alpha_0+\alpha_1H(\psi^{*})+\alpha_2L
					\end{align*}	
				\end{itemize}	
			\item Find the true $\psi^{*}$ using a ''grid search" to test all candidate $\psi$
				\begin{enumerate}
				\item choose evenly space $\psi$ over a range, usually bounded by subject matter knowledge
				\item translate all candidate $\psi$ into $H(\psi)$ using individaul level structural model
				$$H(\psi)=Y-\psi A$$
				\item for each $H(\psi)$, fit the model $$Pr[A=1|H(\psi),L]=\alpha_0+\alpha_1H(\psi)+\alpha_2L$$ 
					\begin{enumerate}
					\item \textbf{Point estimate}: iteratively, narrow the window of candidate $\psi$ until find $\psi^{*}$ such that when $\psi=\psi^{*}\Longrightarrow \alpha_1=0$
					\item \textbf{95\% confidence interval}: using the p-value function, find the values of $H(\psi)$ that yield p>0.05 for the wald test of $H_0$: $\alpha_1=0$
					\item[$\bullet$] translate $\hat{H(\psi^{*})}$ and $H(\psi^{*})_{L,U}\longrightarrow \hat{\psi^{*}}$ and $\psi_{L,U}^{*}$ using structural model
					
					\begin{figure}[H]
					\centering
					\includegraphics[scale=0.3]{gest95ci.png}
					\caption{95\% ci and point estimate}
					\label{gest95ci}
					\end{figure}
							
					\end{enumerate}				
				\end{enumerate}
			\end{enumerate}	
			
		\subsubsection{Censoring}
			\begin{enumerate}
			\item use IPW = $SW^C$ to create a pseudo-population in which no one has been censored
			\item g-estimation in pseudopopulation equates to weighting both the treatment model and structural model by $SW^C$
			\end{enumerate}
	\subsection{SNMMs for Time-Varying Exposures}	 

	\subsubsection {Structural Nested Mean Model}
		\begin{itemize}
		\item consider the general form of the additive structural nested mean model where m are treatment times $m=0...K$\\
		$E[Y_{g={\bar{\alpha}(m-1),\alpha(m),\_{0}(m+1)}}|\bar{L}_{\bar{\alpha}(m-1)}(m)=l(m),\bar{A}(m-1)=\bar{\alpha}(m-1)]\\ 
		=E[Y_{g={\bar{\alpha}(m-1),\_{0}(m)}}|\bar{L}_{\bar{\alpha}(m-1)}(m)=l(m),\bar{A}(m-1)=\bar{\alpha}(m-1)]+\alpha(m)\gamma_{m}[\bar{\alpha}(m-1),\bar{l}(m),\beta^{*}]$\\
		for nondynamic regimes $g={\bar{\alpha}(m-1),\alpha(m),\_{0}(m+1)}$ and $g={\bar{\alpha}(m-1),\_{0}(m)}$, where both have treatment $\bar{\alpha}(m-1)$ through $m-1$, treatment $a(m)$ and 0 at m repsectively, and both have treatment 0 from $m+1$ onward, where $\beta*$ is an unknown parameter vector, \\$\gamma_{m}[\bar{\alpha}(m-1),\bar{l}(m),\beta^{*}]$, is a known function that $\gamma_{m}[\bar{\alpha}(m-1),\bar{l}(m),0]=0$ for $\beta^{*}=0$ under the null hypothesis of no effect treatment
		\item for K=1 the model becomes $E[Y_{g={\bar{\alpha}(0),\alpha(1),0(2)}}|\bar{L}_{\bar{\alpha}(0)}(1)=l(1),\bar{A}(0)=\bar{\alpha}(0)]\\
		 = E[Y_{g={\bar{\alpha}(0),0(1)}}|\bar{L}_{\bar{\alpha}(0)}(1)=l(1),\bar{A}(0)=\bar{\alpha}(0)] + \alpha(1)\gamma_{1}[\bar{\alpha}(0),\bar{l}(1),\beta^{*}]$\\
		and for a saturated model\\ 
		$\gamma_{0}[\bar{\alpha}(-1),\bar{l}(0),\beta^{*}]=\beta^{*}_{0}$, and $\gamma_{1}[\bar{\alpha}(0),\bar{l}(1),\beta^{*}]=\beta^{*}_{1,1}+\beta^{*}_{1,2}l(1)+\beta^{*}_{1,3}a(0)+\beta^{*}_{1,4}a(0)l(1)$\\ so that we get the SNM\\ $Y_{g=(\alpha_{0},\alpha_{1})}=Y_{g=(\alpha_{0},0)}+\beta_{1,1}\alpha_{1}+\beta_{1,2}\alpha_{1}L_{1,g={a_{0}}}+\beta_{1,3}\alpha_{0}\alpha_{1}+\beta_{1,4}\alpha_{0}\alpha_{1}L_{1,g={\alpha_{0}}}$ 
		\item g-stimation for estimating the unknown $\beta^{*}$
			\begin{itemize}
			\item define $$Y_{m}(\beta)=Y - \sum^{K}_{j=m} A(j)\gamma_{j}[\bar{A}(j-1),\bar{L}(j),\beta]$$
			\item calculate $Y_{m}(\beta)$ for range of $\beta$ containing the true $\beta^{*}$ and a CI.
			\item fit a polled logistic model $logitPr[A(m)=1|\bar{L}(m),\bar{A}(m-1),Y_{m}(\beta)]=\alpha^{T}W(m) + \theta Y_{m}(\beta)$, where W(m) is a vector for covariates and treatment history $[\bar{L}(m),\bar{A}(m-1)]$, $\alpha^{T}$ is a transposed row vector of unknown parameters
			\item the model gives an estimate of $\beta^{*}$  by the value of $\beta$ for which test score for $\theta=0$ is 0, ($p-value=1$)
			\item also 95\% CI are given by the values of $\beta$ for which the null hypothesis of $\theta=0$ cannot be rejected ($p-value>0.05$)
			\item this procedure is valid as if $\beta=\beta^{*}$ then $Y_{m}(\beta)=Y_{g={\bar{A}(m-1),0{m}}}$ and also $Y_{g={\bar{A}(m-1),0{m}}}\coprod A(m)$ so for $\beta=\beta^{*}$ $\theta=0$
			\end{itemize}
		\item for a vector $\beta$ then the pooled logistic model will contain a vector parameter $\theta$ corresponding to a vector of an adequate number of covariate functions needed to estimate the number of parameters in the vector $\beta$
		\end{itemize}

	\subsubsection*{Structural Accelrated Failure Time model}
	 
		\begin{itemize}
		\item consider the model $T_{\alpha}=T_{0}exp[-\psi^{*}\alpha]$, where $T_{\alpha}$ and $T_{0}$ are survival times under treatment regime $\alpha$ and no treatment respectively
			\begin{itemize}
			\item $exp[-\psi^{*}]$is the ratio $T_{\alpha=1}/T_{\alpha=0}$ 
			\item it is the expansion or contraction of survival time attributable to tratement 
			\item $\psi^{*}>0$ corresponds to shorter survival time for the treated compared to their survival time had they remained untreated and $\psi^{*}<0$ corresponds to longer survival times when treated. 
			\item no causal effect when $\psi^{*}=0$
			\end{itemize}
		\item g-estimation of the unknown $\psi^{*}$
			\begin{itemize}
			\item transform model to $T_{0}=T_{\alpha}exp[\psi^{*}\alpha]$
				\begin{itemize}
				\item model is also rank preserving
				\end{itemize}
			\item for time varying exposures where the exposure effect on survival time is averaged over duration through time t then the model becomes $T_{\bar{0}}=\int^{T_{\bar{\alpha}}}_{0} exp[\psi^{*}\alpha(t)]dt$ 
			\item using observational data then we have $T_{\bar{0}}=\int^{T}_{0}exp[\psi^{*}A(t)]dt$ 
			\item substitute $T_{0}$ with $H(\psi)$  and $H(\psi)=\int^{T}_{0}exp[\psi^{*}A(t)]dt$where  $T_{0}=H(\psi^{*})$
			\item fit pooled logistc model $logitPr[A(m)=1|\bar{A}(m-1),\bar{L}(m),T>u(m),H(\psi)]= \theta_{0}(m)+\theta_{1}A(m-1)+\theta_{2}L(m)+\theta_{3}H(\psi)$
			\item we know that $H(\psi^{*})\coprod A(m)|\bar{A}(m-1)$ for all m so for $\psi=\psi^{*}$ in the logistic model $\theta_{3}=0$
			\item for estimate of $\psi^{*}$ and 95\% CI using grid search score test for $\theta_{3}=0$
			\end{itemize}
		\end{itemize}

\subsection{strength and weakness}
  \begin{itemize}
	\item strength
	\item Can estimate causal effects when treatment is confounded, and instrument is available
	\item Allow modeling of interaction between treatment and time-varying covariates 
	\item weakness
	\item SNMs cannot be easily used to compare non-dynamic regimes when there is effect modification by time-dependent covariates
	\item Logistic SNMs, unlike log-linear model, cannot be fit by g-estimation
	\item MSM cannot be used to estimate causal effects when treatment is confounded, but instrument is available
	\item SNMs for failure time data should be based on accelerated failure time-like models, that cannot handle censoring properly
	\end{itemize}
	 
	 
\section{Propensity Scores}
\begin{itemize}
	\item \textbf{Definition of propensity score} : Probability of receiving exposure A conditional on the confounders L
	\item PS = Pr [A=1|L]
	\item Rosenbaum and Rubin (1983):
	\item If there is conditional exchangeability given the confounders, then there is conditional exchangeability given the PS
	\item It enables controlling for all measured confounders and provides more accurate estimates than can conventional methods.
		\begin{itemize}
		\item Include all measured confounders in traditional regression model may decrease the statistical efficiency or have sparse data problems
		\item Conventional variable-selection algorithms (eg. 5% change of estimate, or significance tests for collapsibility) may miss important confounders 
		\end{itemize}
	\item \textbf{Assumptions of propensity score methods}
		\begin{itemize}
		\item Causal inference assumption: Conditional exchangeability, positivity, and consistency
		\item Model assumption: No misspecification of PS model, no misspecification of outcome model conditional on PS 
		\end{itemize}
	\item \textbf{Propensity score can be used to control confounding by}
		\begin{itemize}
		\item Stratification
		\item Regression
		\item Matching
		\item Weighting (marginal structural model)
		\end{itemize}
	\item \textbf{Stratification}
		\begin{itemize}
		\item  Create a categorical PS variable using the continuous PS (eg. decile) and stratify subjects by of the decile PS
		\item  Combine the decile PS stratum-specific effect estimate by MH weight or inverse of variance weight
		\item  Limitation: may have sparse data problems, have to make an assumption of homogenous effect across PS categories
		\end{itemize}
	\item \textbf{Regression}
		\begin{itemize}
		\item  Use continuous or categorical PS as a regressor in the regression model
		\item  Limitation: continuous PS have to make a strong linearity assumption, categorical PS may have residual confounding 
		\end{itemize}
	\item \textbf{Matching}
		\begin{itemize}
		\item  Matched each exposed subject to unexposed subject with same propensity score
		\item  Can be 1:N matching
		\item  Most “clean” type of covariate control (each pair is a strata)
		\item  Limitation: less efficiency than regression because throw out data, cannot always find a matched unexposed subject
		\end{itemize}
	\item \textbf{Weighting}
		\begin{itemize}
		\item  Inverse probability of treatment weight is the inverse of PS 
		\item  Exposed group 1/PS ; unexposed group 1/(1-PS)
		\item  Can also get SMR or SRR effect estimate
			\begin{itemize}
			\item SMR weight:  Exposed 1, unexposed PS/(1-PS)
			\item SRR weight:  Unexposed 1, exposed (1-PS)/PS
			\end{itemize}
		\end{itemize}
	\item \textbf{Stratification, regression, and matching are stratification based method and have the limitation of }
		\begin{itemize}
		\item  Have to make assumption of no effect modification across PS-defined strata
		\item  No average causal interpretation among the whole population, only within the levels of PS
		\item  Result in bias if stratified on time-varying exposure
		\end{itemize}
	\item \textbf{Other PS problems}
		\begin{itemize}
		\item  A PS that perfectly predict exposure may suffer variance inflation by selecting nonconfounding but strong exposure predictors
		\item  A PS includes variables predicting outcome but not exposure increases precision. (However, such model is no longer Pr [A|L])
		\item  A hybrid method is to combine exposure and outcome modeling score and used them for controlling confounding and increasing precision.
		\item  Propensity score for continuous or multiple category outcomes is not well established
		\item The use of propensity score in case-control study is not well established
		\end{itemize}
	\end{itemize}

	

\section{Instrumental Variables}





\section{G-Methods Analysis Example}
\begin{itemize}
\item the following is an example of how data from a non-randomized dynamic treatment regime can be analyzed by the g-methods: g-formula algorithm, IPTW (MSM), and g-estimation (SNM)
\begin{table}[h!]
\caption{Example dataset for g-methods computation}
\centering
\begin{tabular}{c c c c c}
	\hline \hline
	$A_0$ & $L_1$ & $A_1$ & $N$ & $Y$ \\
	\hline
	0 & 0 & 0 & 6,000 & 50\\
	0 & 0 & 1 & 2,000 & 70\\
	0 & 1 & 0 & 2,000 & 200\\
	0 & 1 & 1 & 6,000 & 220\\
	1 & 0 & 0 & 3,000 & 230\\
	1 & 0 & 1 & 1,000 & 250\\
	1 & 1 & 0 & 3,000 & 130\\
	1 & 1 & 1 & 9,000 & 110\\
	\hline \hline
\end{tabular}
\label{ex_table}
\end{table}

\item the structure of the data is depicted in Figure ~\ref{g-methods_ex}, where treatment $A_1$ is dynamically determined by past treatment $A_0$ and covariate $L_1$

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{g-methods_ex.png}
	\caption{DAG of a dynamic regime where $A_1$ is determined by $A_0$ and $L_1$}
	\label{g-methods_ex}
\end{figure}

\item in order to appropriately analyze the data, g-methods are necessary to remove arrows into $A_1$ to make the study appear as a sequentially randomized study (Figure ~\ref{g-methods_rm}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{g-methods_rm.png}
	\caption{DAG of a dynamic regime where arrows into $A_1$ have been removed by g-methods}
	\label{g-methods_rm}
\end{figure}

\item the following is an example of how to make valid causal inferences from the data using the g-formula algorithm, IPTW, and g-estimation
\end{itemize}

\subsection{G-Formula}
Example of using of the g-formula algorithm
\begin{table}[H]
\caption{Data for use with the g-formula}
\centering
\begin{tabular}{c c c c c c}
	\hline \hline
	$A_0$ & $L_1$ & $A_1$ & $N$ & $Y$ & $Pr[L_1|A_0]$\\
	\hline
	0 & 0 & 0 & 6,000 & 50 & 0.50\\
	0 & 0 & 1 & 2,000 & 70 & 0.50\\
	0 & 1 & 0 & 2,000 & 200 & 0.50\\
	0 & 1 & 1 & 6,000 & 220 & 0.50\\
	1 & 0 & 0 & 3,000 & 230 & 0.25\\
	1 & 0 & 1 & 1,000 & 250 & 0.25\\
	1 & 1 & 0 & 3,000 & 130 & 0.75\\
	1 & 1 & 1 & 9,000 & 110 & 0.75\\
	\hline \hline
\end{tabular}
\label{ex_table_g}
\end{table}
\begin{itemize}
\item in this example, $L$ is a binary variable so the g-formula can be explicitly written as:
$$E[Y^{g}]=E[Y|A_0=a_0,A_1=a_1,L_1=0]\times Pr[L_1=0|A_0=a_0]$$
$$\ \ \ \ \ \ \ \ \  + \ E[Y|A_0=a_0,A_1=a_1,L_1=1]\times Pr[L_1=1|A_0=a_0] $$
\item the causal outcomes of interest $Y^{g\{a_0,a_1\}}$ are estimated as follows:
	\begin{itemize}
	\item $Y^{g\{0,0\}}=125$\\
	$Y^{g\{0,0\}}=50 \times 0.50 + 200 \times 0.50=125$
	\item $Y^{g\{0,1\}}=145$\\
	$Y^{g\{0,1\}}=70 \times 0.50 + 220 \times 0.50=145$
	\item $Y^{g\{1,0\}}=155$\\
	$Y^{g\{1,0\}}=230 \times 0.25 + 130 \times 0.75=155$
	\item $Y^{g\{1,1\}}=145$\\
	$Y^{g\{1,1\}}=250 \times 0.25 + 110 \times 0.75=145$
	\end{itemize}
\item causal risk differences for the joint effects of treatments $A_0$ and $A_1$ are calculated as follows:
	\begin{itemize}	
	\item $Y^{g\{1,0\}}-Y^{g\{0,0\}}=155-125=30$
	\item $Y^{g\{0,1\}}-Y^{g\{0,0\}}=145-125=20$
	\item $Y^{g\{1,1\}}-Y^{g\{1,0\}}=145-155=-10$
	\item $Y^{g\{1,1\}}-Y^{g\{0,1\}}=145-145=0$
	\item $Y^{g\{1,1\}}-Y^{g\{0,0\}}=145-125=20$
	\end{itemize}
\end{itemize}



\subsection{IPTW (MSM)}
Example of IPTW
\begin{table}[h!]
\caption{Data from a Non-random Dynamic Regime with \em unstabalized \em IPTW}
\centering
\begin{tabular}{c c c c c c c c r}
	\hline \hline
	$A_0$ & $L_1$ & $A_1$ & $N$ & $Y$ & $f\{A_0\}$ & $f\{A_1|A_0,L_1\}$ & $W$ & $Pseudo \ N$\\
	\hline
	0 & 0 & 0 & 6,000 & 50  & 0.50 & 0.75 & $\frac{8}{3}$ & 16,000\\[0.2em]
	0 & 0 & 1 & 2,000 & 70  & 0.50 & 0.25 & 8 & 16,000\\[0.2em]
	0 & 1 & 0 & 2,000 & 200 & 0.50 & 0.25 & 8 & 16,000\\[0.2em]
	0 & 1 & 1 & 6,000 & 220 & 0.50 & 0.75 & $\frac{8}{3}$ & 16,000\\[0.2em]
	1 & 0 & 0 & 3,000 & 230 & 0.50 & 0.75 & $\frac{8}{3}$ & 8,000\\[0.2em]
	1 & 0 & 1 & 1,000 & 250 & 0.50 & 0.25 & $\frac{8}{3}$ & 8,000\\[0.2em]
	1 & 1 & 0 & 3,000 & 130 & 0.50 & 0.25 & 8 & 24,000\\[0.2em]
	1 & 1 & 1 & 9,000 & 110 & 0.50 & 0.75 & $\frac{8}{3}$ & 24,000\\[0.2em]
	\hline \hline
\end{tabular}
\label{IPTW_table}
\end{table}
\begin{itemize}
\item the first step is to calculate \em unstabalized \em IPTW weights to create a pseudo population without confounding (Table ~\ref{IPTW_table})
\item the goal is to make $Y^g\amalg A_1|A_0,L_1$, thus removing any arrows into $A_1$
\item data can then be collapsed over strata of $L_1$ to create Table ~\ref{IPTW_collapsed}

\begin{table}[h!]
\caption{Data collapsed over strata of $L$}
\centering
\begin{tabular}{c c c c c}
	\hline \hline
	$A_0$ & $A_1$ & $Pseudo \ N$ & $E[Y|A_0,A_1]$ & Parameter\\
	\hline
	0 & 0 & 32,000 & 125 & $\gamma_0$\\
	0 & 1 & 32,000 & 145 & $\gamma_0+\gamma_2$\\
	1 & 0 & 32,000 & 155 & $\gamma_0+\gamma_1$\\
	1 & 1 & 32,000 & 145 & $\gamma_0+\gamma_1+\gamma_2+\gamma_3$\\
	\hline \hline
\end{tabular}
\label{IPTW_collapsed}
\end{table}

\item the Marginal Structural Model (MSM) we want to fit is:
$$E[Y^{g=\{a_0,a_1\}}]=\eta_0+\eta_1a_0+ \eta_2a_1+\eta_3a_0a_1$$
\item the associational model we fit is:
$$ E[Y|a_0,a_1]=\gamma_0+\gamma_1a_0+ \gamma_2a_1+\gamma_3a_0a_1 $$
\item however, if we fit an appropriately IPTW weighted regression model in the actual population, the result is an associational model fitted to a pseudo population where there is no confounding
\item thus association in the pseudo population \em is \em causation in the actual population, $\eta_i=\gamma_i$
\item the model parameters are estimated as:
	\begin{itemize}
	\item $\gamma_0=\eta_0=125$\\
	$\gamma_0=125$
	\item $\gamma_1=\eta_1=20$\\
	$\gamma_0+\gamma_2=125+\gamma_2=145$
	\item $\gamma_2=\eta_2=30$\\
	$\gamma_0+\gamma_1=125+\gamma_1=155$
	\item $\gamma_3=\eta_3=-30$\\
	$\gamma_0+\gamma_1+\gamma_2+\gamma_3= 125+20+30+\gamma_3=145$
	\end{itemize}
\item the causal outcomes of interest $Y^{g=\{a_0,a_1\}}$ are estimated as follows:
	\begin{itemize}
	\item $E[Y^{g=\{0,0\}}]=125$\\
	$E[Y^{g=\{0,0\}}]=\eta_0=125$
	\item $E[Y^{g=\{0,1\}}]=145$\\
	$E[Y^{g=\{0,1\}}]=\eta_0+\eta_2=145$
	\item $E[Y^{g=\{1,0\}}]=155$\\
	$E[Y^{g=\{1,0\}}]=\eta_0+\eta_1=155$
	\item $E[Y^{g=\{1,1\}}]=145$\\
	$E[Y^{g=\{1,1\}}]=\eta_0+\eta_1+\eta_2+\eta_3=145$
	\end{itemize}
\item MSMs cannot assess effect modification by non-baseline covariates
\end{itemize}

\subsection{G-Estimation (SNM)}
Example of g-estimation
\begin{table}[h!]
\caption{Data from a Non-random Dynamic Regime for G-estimation}
\centering
\begin{tabular}{c c c c c c c}
	\hline \hline
	$A_0$ & $L_1$ & $A_1$ & $N$ & $Y$ & $Y^{g=\{A_0,0\}}$ & $Y^{g=\{0,0\}}$\\
	\hline
	0 & 0 & 0 & 6,000 & 50  & 50 & 50\\
	0 & 0 & 1 & 2,000 & 70  & 70-$\beta^*_{1,1}$ & 70-$\beta^*_{1,1}$\\
	\hdashline
	0 & 1 & 0 & 2,000 & 200 & 200 & 200\\
	0 & 1 & 1 & 6,000 & 220 & 220-$\beta^*_{1,1}$-$\beta^*_{1,2}$ & 220-$\beta^*_{1,1}$-$\beta^*_{1,2}$\\
	\hdashline
	1 & 0 & 0 & 3,000 & 230 & 230 & 230-$\beta^*_{0}$\\
	1 & 0 & 1 & 1,000 & 250 & 250-$\beta^*_{1,1}$-$\beta^*_{1,3}$ & 250-$\beta^*_{0}$-$\beta^*_{1,1}$-$\beta^*_{1,3}$\\
	\hdashline
	1 & 1 & 0 & 3,000 & 130 & 130 & 130-$\beta^*_{0}$\\
	1 & 1 & 1 & 9,000 & 110 & 110-$\beta^*_{1,1}$-$\beta^*_{1,2}$-$\beta^*_{1,3}$-$\beta^*_{1,4}$ & 110-$\beta^*_{0}$-$\beta^*_{1,1}$-$\beta^*_{1,2}$-$\beta^*_{1,3}$-$\beta^*_{1,4}$\\
	\hline \hline
\end{tabular}
\label{SNM_table}
\end{table}
\begin{itemize}
\item Structural Nested Models (SNMs) can be used for any static or dynamic regime $g$
\item SNMs need one equation for each treatment time
	\begin{itemize}
	\item time 0: $Y^{g=\{a_0,0\}}=Y^{g=\{0,0\}}+\beta^*_0 a_0$
		\begin{itemize}
		\item where:\\
		$\beta^*_0=Y^{g=\{1,0\}}-Y^{g=\{0,0\}}$ is the direct effect of treatment $a_0$ on outcome $Y$ when $a_1$ is withheld
		\end{itemize}
	\item time 1: $Y^{g=\{a_0,a_1\}}=Y^{g=\{a_0,0\}}+\beta^*_{1,1} a_1+\beta^*_{1,2} a_1 L^{g=\{a_0\}}_1+\beta^*_{1,3}a_0a_1+\beta^*_{1,4}a_1a_0L^{g=\{a_0\}}_1$
		\begin{itemize}
		\item where $\beta^*_1$ parameterizes the effect of $a_1$ on $Y$ within the 4 possible levels of past treatment  and covariate history:\\\\
		$\beta^*_{1,1}$ is the effect of $a_1$ on $Y$ when $a_0$ is withheld among the subset of individuals with $L^{g=\{0\}}_1=0$\\\\
		$\beta^*_{1,1}+\beta^*_{1,2}$ is the effect of $a_1$ on $Y$ when $a_0$ is withheld among the subset of individuals with $L^{g=\{0\}}_1=1$\\\\
		$\beta^*_{1,1}+\beta^*_{1,3}$ is the effect of $a_1$ on $Y$ when $a_0$ is taken among the subset of individuals with $L^{g=\{0\}}_1=0$\\\\
		$\beta^*_{1,1}+\beta^*_{1,3}+\beta^*_{1,4}$ is the effect of $a_1$ on $Y$ when $a_0$ is withheld among the subset of individuals with $L^{g=\{0\}}_1=1$\\
		\end{itemize}
	\end{itemize}
\item SNMs are locally rank preserving models
\item the assumption of conditional exchangeability ($Y^{g=\{0,0\}}\amalg A_0$ and $Y^{g=\{a_0,0\}}\amalg A_1|A_0,L_1$) is needed to estimate the SNM parameters
	\begin{itemize}
	\item under $Y^{g=\{a_0,0\}}\amalg A_1|A_0,L_1$,  the counterfactual outcome $Y^{g\{A_0,0\}}$ is equal for $A_1=0$ and $A_1=1$ in the same strata of $A_0$ and $L_1$ (denoted by sections in between dashed lines in Table ~\ref{SNM_table})
	\item thus, the vector of parameters $\beta^*_1$ can be estimated using the SNM at time 1:
		\begin{itemize}
		\item $\beta^*_{1,1}=20$:\\
	$50=70-\beta^*_{1,1}$ (Table ~\ref{SNM_table}, rows 1 and 2)
		\item $\beta^*_{1,2}=0$:\\
	$200=220-\beta^*_{1,1}-\beta^*_{1,2}$ (Table ~\ref{SNM_table}, rows 3 and 4)
		\item $\beta^*_{1,3}=0$:\\
	$230=250-\beta^*_{1,1}-\beta^*_{1,3}$ (Table ~\ref{SNM_table}, rows 5 and 6)
		\item $\beta^*_{1,4}=-40$:\\
	$130=110-\beta^*_{1,1}-\beta^*_{1,2}-\beta^*_{1,3}-\beta^*_{1,4}$ (Table ~\ref{SNM_table}, rows 5 and 6)
		\end{itemize}
	\item under $Y^{g=\{0,0\}}\amalg A_0$, the mean of $Y^{g=\{0,0\}}$ is the same for $A_0=0$ as for $A_0=1$
		\begin{itemize}
		\item $\beta^*_{1,1}$, $\beta^*_{1,2}$, $\beta^*_{1,3}$, and $\beta^*_{1,4}$ can be plugged in to calculate $Y^{g=\{0,0\}}$ 
		\end{itemize}
	\item thus, $\beta^*_0$ can be estimated using the SNM at time 0:
		\begin{itemize}
		\item $\beta^*_0=30$:\\
		$ 50 \times \frac{6,000+2,000}{16,000} + 200 \times \frac{2,000+6,000}{16,000}=125$ (Table ~\ref{SNM_table}, rows 1-4)\\
		$ 230 \times \frac{3,000+1,000}{16,000} + 130 \times \frac{3,000+9,000}{16,000}=155-\beta^*_0$ (Table ~\ref{SNM_table}, rows 5-8)\\
		$125=155-\beta^*_0$\\
		\end{itemize}
	\item finally, $E[Y^g]$ can be estimated using the locally rank-preserving SNMs
		\begin{itemize}
		\item $E[Y^{g=\{0,0\}}]=125$\\
		estimated from SNM at time 0 (above)
		\item $E[Y^{g=\{1,0\}}]=155$\\
		$E[Y^{g=\{1,0\}}]=E[Y^{g=\{0,0\}}]+\beta^*_0 =125+30=155$, estimated from SNM at time 0
		\item $E[Y^{g=\{0,1\}}]=145$\\
		$E[Y^{g=\{0,1\}}]= E[Y^{g=\{0,0\}}]+\beta^*_{1,1}a_1+\beta^*_{1,2}a_1L^{g=\{0\}}_1=125+20+0=145$, estimated from SNM at time 1
		\item $E[Y^{g=\{1,1\}}]=145$\\
		$E[Y^{g=\{1,1\}}]= E[Y^{g=\{1,0\}}]+\beta^*_{1,1}a_1+\beta^*_{1,2}a_1L^{g=\{1\}}_1+\beta^*_{1,3}a_0a_1+\beta^*_{1,4}a_1a_0L^{g=\{1\}}_1$\\
		$E[Y^{g=\{1,1\}}]=155+20+0+(0-40)\times \frac{3}{4}=145$, estimated from SNM at time 1
		\item where $L^{g=\{A_0=1\}}_1=\frac{3}{4}$, because association is causation for the effect of $A_0$ on $L_1$, hence the $Pr[L_1=1|A_0=1]=\frac{3,000+9,000}{3,000+1,000+3,000+9,000+}=\frac{12,000}{16,000}=\frac{3}{4}$
		\end{itemize}
	\end{itemize}
\item these estimates of $E[Y^g]$ are the same as those from the g-formula and the IPTW MSM
\item SNM allow for the additional investigation of effect modification of treatment regime $g$ by time-varying covariates
\end{itemize}
\end{document}